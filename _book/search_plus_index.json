{"./":{"url":"./","title":"Introduction","keywords":"","body":"Introduction ucore实验指导书 ucore labs 1-8 源码 实验总体流程 认真上操作系统的理论课程； 阅读ucore实验指导书，并参考其内容完成联系和实验报告; 在实验环境中完成实验并提交到自己的github上； 遇到问题，首先查询手册等其他资料，先自行解决； 如若不能解决，可在飞书群里提问，可以互相讨论，有助教老师答疑。 学习目标与对应手段 掌握OS基本概念：通过上课与学习教材，能理解OS原理与概念；阅读指导书并分析源码，能理解lab_codes_answer的labs运行结果。 掌握OS设计实现：在1的基础上，能够通过编程完成lab_codes的8个lab实验中的基本练习和实验报告。 掌握OS核心功能：在2的基础上，能够通过编程完成lab_codes的8个lab实验中的challenge练习。 掌握OS科学研究：联系老师，加入实验室，开始科研吧。 友情提示 课程铺垫——计算机组成原理、C语言、数据结构 工具掌握——命令行 shell、软件管理 apt-get/aptitude、版本管理 git/github、代码阅读 understand/VSCode、代码比较 diff/meld、开发编译调试 gcc/gdb/make、硬件模拟器 qemu、md文档编写 Typora 实验报告要求 独立完成； 用Markdown语言编写； 报告内容包括但不仅限于：实验目的、实验内容、实验步骤、实验结果、遇到的问题与解决方法； 报告编写完需按时发送给助教并上传到自己的github仓库里。 维护者 kelee@mail.nankai.edu.cn 如若对本指导书有任何疑问，请联系维护者！ Reference ucore step by step Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:57 "},"lab0/":{"url":"lab0/","title":"LAB0：ready~go!","keywords":"","body":"LAB0：ready~go! 学习与操作系统实验相关的前导知识，其主要包括实验环境、实验步骤、实验工具、uCore历史、RISC-V简介。 配置实验所需要的环境与软件。 [!NOTE|style:flat] 本次实验不需要撰写实验报告，但是需要自己设置好GitHub账号与自己的git连接。并且把自己的初始化后的仓库地址发送给助教老师。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/shi-yan-mu-de.html":{"url":"lab0/shi-yan-mu-de.html","title":"实验目的","keywords":"","body":"实验目的 了解操作系统开发实验环境 学会使用Ubuntu操作系统 熟悉命令行方式的编译、调试工程 掌握基于硬件模拟器的调试技术 学会使用基本的开发工具 掌握RISCV-32汇编语言 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/shi-yan-nei-rong.html":{"url":"lab0/shi-yan-nei-rong.html","title":"实验内容","keywords":"","body":"实验内容 阅读实验指导书内容以及涉及的官方文档或者教程。 对于实验指导书未涉及的内容能自行动手查询资料学习。 安装Ubuntu操作系统。 安装推荐的开发工具，若对某些工具情有独钟亦可。 安装依赖包、硬件模拟器、调试工具后尝试进行联合调试。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/qian-dao-zhi-shi/":{"url":"lab0/qian-dao-zhi-shi/","title":"前导知识","keywords":"","body":"前导知识 该模块是LAB0的第一个模块，主要就是阅读学习相关的知识，对OS实验有一个基本的了解，看一下我们接下来一个学期到底要做什么，要实现什么样的一个东西，而这个东西又是基于什么基础慢慢搭建起来的。要搭建房子需要各种工具，工欲善其事必先利其器，所以要学会使用主要的开发调试工具，能让我们实验过程变得非常愉快。 最后由于我们是基于RISC-V指令集来完成实验，当然要掌握这门指令集的所有东西啦，幸运的是，你选择的是最简单的指令集，有没有开心一点呢？ [!NOTE|style:flat] 千万不要嫌接下来的前导知识杂没有用哦，答应我，一定要认认真真的看哈~ Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/qian-dao-zhi-shi/le-jie-ucore.html":{"url":"lab0/qian-dao-zhi-shi/le-jie-ucore.html","title":"了解uCore","keywords":"","body":"了解uCore 2006年, MIT的Frans Kaashoek等人参考PDP-11上的UNIX Version 6写了一个可在x86指令集架构上运行的操作系统xv6（基于MIT License)。 2010年, 清华大学操作系统教学团队参考MIT的教学操作系统xv6, 开发了在x86指令集架构上运行的操作系统ucore, 多年来作为操作系统课程的实验框架使用, 已经成为了大家口中的\"祖传实验\". ucore麻雀虽小，五脏俱全。在不超过5k的代码量中包含虚拟内存管理、进程管理、处理器调度、同步互斥、进程间通信、文件系统等主要内核功能，充分体现了“小而全”的指导思想。 ucore的运行环境可以是真实的计算机（包括小型智能设备）。一开始ucore是运行在x86指令集架构上的，到了如今，x86指令集架构的问题渐渐开始暴露出来。 虽然在PC平台上占据绝对主流，但出于兼容性考虑x86架构仍然保留了许多的历史包袱，用于教学的时候总有些累赘。另一方面，为了更好的和目前5G、物联网技术的发展衔接，将ucore移植到RISC-V架构势在必行。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/qian-dao-zhi-shi/le-jie-riscv.html":{"url":"lab0/qian-dao-zhi-shi/le-jie-riscv.html","title":"了解RISC-V","keywords":"","body":"了解RISC-V 发明 RISC发明者是美国加州大学伯克利分校教师David Patterson，RISC-V（拼做risk-five）是第五代精简指令集，也是由David Patterson指导的项目。2010年伯克利大学并行计算实验室(Par Lab) 的1位教授和2个研究生想要做一个项目，需要选一种计算机架构来做。当时面临的的是选择X86、ARM，还是其他指令集，不管选择哪个都或多或少有些问题，比如授权费价格高昂，不能开源，不能扩展更改等等。所以他们在2010年5月开始规划自己做一个新的、开源的指令集，就是RISC-V。 接着时间到2015年，这个指令集在学术界已经开始出名了，这时为了更好的推动这个指令集在技术和商业上的发展，3位创始人大佬做了下面两件事情。 技术方向，成立RISC-V基金会，维护指令集架构的完整性和非碎片化。 商业方向，成立SiFive公司，推动RISC-V的商业化。 基金会会员 目前加入RISC-V基金会的中国企业和机构有：阿里巴巴、华为、中科院计算所、华米科技、智芯科技、浪潮等。（具体会员名单可以在这里查看） 特点 设计哲学-简单就是美 无病一身轻——架构的篇幅。目前的“RISC-V架构文档”分为“指令集文档”（riscv-spec-v2.2.pdf）和“特权架构文档”（riscv-privileged-v1.10.pdf）。“指令集文档”的篇幅为145页，而“特权架构文档”的篇幅也仅为91页。熟悉体系结构的工程师仅需一至两天便可将其通读，虽然“RISC-V的手册”还在不断地丰富，但是相比“x86的架构文档”与“ARM的架构文档”，RISC-V的篇幅可以说是极其短小精悍。 [!NOTE|style:flat] 一定要仔细阅读《RISC-V手册》哦！！ 能屈能伸——模块化的指令集。RISC-V架构相比其他成熟的商业架构的最大一个不同还在于它是一个模块化的架构。因此，RISC-V架构不仅短小精悍，而且其不同的部分还能以模块化的方式组织在一起，从而试图通过一套统一的架构满足各种不同的应用。这种模块化是x86与ARM架构所不具备的。以ARM的架构为例，ARM的架构分为A、R和M三个系列，分别针对于Application（应用操作系统）、Real-Time（实时）和Embedded（嵌入式）三个领域，彼此之间并不兼容。模块化的RISC-V架构能够使得用户能够灵活选择不同的模块组合，以满足不同的应用场景，可以说是“老少咸宜” 浓缩的都是精华——指令的数量。短小精悍的架构以及模块化的哲学，使得RISC-V架构的指令数目非常的简洁。基本的RISC-V指令数目仅有40多条，加上其他的模块化扩展指令总共几十条指令。 指令集简介 模块化的指令子集。RISC-V的指令集使用模块化的方式进行组织，每一个模块使用一个英文字母来表示。RISC-V最基本也是唯一强制要求实现的指令集部分是由I字母表示的基本整数指令子集，使用该整数指令子集，便能够实现完整的软件编译器。其他的指令子集部分均为可选的模块，具有代表性的模块包括M/A/F/D/C。 规整的指令编码。RISC-V的指令集编码非常的规整，指令所需的通用寄存器的索引（Index）都被放在固定的位置。因此指令译码器（Instruction Decoder）可以非常便捷的译码出寄存器索引然后读取通用寄存器组（Register File，Regfile）。 优雅的压缩指令子集。基本的RISC-V基本整数指令子集（字母I表示 ）规定的指令长度均为等长的32位，这种等长指令定义使得仅支持整数指令子集的基本RISC-V CPU非常容易设计。但是等长的32位编码指令也会造成代码体积（Code Size）相对较大的问题。为了满足某些对于代码体积要求较高的场景（譬如嵌入式领域），RISC-V定义了一种可选的压缩（Compressed）指令子集，由字母C表示，也可以由RVC表示。RISC-V具有后发优势，从一开始便规划了压缩指令，预留了足够的编码空间，16位长指令与普通的32位长指令可以无缝自由地交织在一起，处理器也没有定义额外的状态。 特权模式。RISC-V架构定义了三种工作模式，又称特权模式（Privileged Mode）：Machine Mode：机器模式，简称M Mode。Supervisor Mode：监督模式，简称S Mode。User Mode：用户模式，简称U Mode。RISC-V架构定义M Mode为必选模式，另外两种为可选模式。通过不同的模式组合可以实现不同的系统。 自定制指令扩展。除了上述阐述的模块化指令子集的可扩展、可选择，RISC-V架构还有一个非常重要的特性，那就是支持第三方的扩展。用户可以扩展自己的指令子集，RISC-V预留了大量的指令编码空间用于用户的自定义扩展，同时，还定义了四条Custom指令可供用户直接使用，每条Custom指令都有几个比特位的子编码空间预留，因此，用户可以直接使用四条Custom指令扩展出几十条自定义的指令。 其他特点 可配置的通用寄存器组、简洁的存储器访问指令、高效的分支跳转指令、简洁的子程序调用、无条件码执行、无分支延迟槽、简洁的运算指令。 [!TIP|style:flat|label:友情链接] RISC-V基金会 中国开放指令生态（RISC-V）联盟 赛昉科技 终于有人把RISC-V讲明白了 什么是RISC-V Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/qian-dao-zhi-shi/le-jie-os-shi-yan.html":{"url":"lab0/qian-dao-zhi-shi/le-jie-os-shi-yan.html","title":"了解OS实验","keywords":"","body":"了解OS实验 写一个操作系统难吗？别被现在上百万行的Linux和Windows操作系统吓倒。当年Thompson趁他老婆带着小孩度假留他一人在家时，写了UNIX；当年Linus还是一个21岁大学生时完成了Linux雏形。站在这些巨人的肩膀上，我们能否也尝试一下做“巨人”的滋味呢？ 实验内容 那我们准备如何一步一步来实现ucore呢？根据一个操作系统的设计实现过程，我们可以有如下的实验步骤： 启动操作系统的bootloader:OpenSBI。了解操作系统启动前的状态和要做的准备工作，了解运行操作系统的硬件支持，操作系统如何加载到内存中，理解两类中断--“外设中断”，“陷阱中断”等； 物理内存管理子系统。用于理解RISC-V分段/分页模式，了解操作系统如何管理物理内存； 虚拟内存管理子系统。通过页表机制和换入换出（swap）机制，以及中断-“故障中断”、缺页故障处理等，实现基于页的内存替换算法； 内核线程子系统。用于了解如何创建相对与用户进程更加简单的内核态线程，如果对内核线程进行动态管理等； 用户进程管理子系统。用于了解用户态进程创建、执行、切换和结束的动态管理过程，了解在用户态通过系统调用得到内核态的内核服务的过程； 处理器调度子系统。用于理解操作系统的调度过程和调度算法； 同步互斥与进程间通信子系统。了解进程间如何进行信息交换和共享，并了解同步互斥的具体实现以及对系统性能的影响，研究死锁产生的原因，以及如何避免死锁； 文件系统。了解文件系统的具体实现，与进程管理等的关系，了解缓存对操作系统IO访问的性能改进，了解虚拟文件系统（VFS）、buffer cache和disk driver之间的关系。 其中每个开发步骤都是建立在上一个步骤之上的，就像搭积木，从一个一个小木块，最终搭出来一个小房子。在搭房子的过程中，完成从理解操作系统原理到实践操作系统设计与实现的探索过程。这个房子最终的建筑架构和建设进度如下图所示： 开发OS实验的步骤 本次OS实验大致可以通过如下过程就可以完成使用： 学习相关理论知识 建立LAB实验环境，采用VMware虚拟机的最简单方式完成 阅读本LAB实验指导书，了解实验要求 下载源码 进入各个OS实验工程目录 例如：cd labcodes/lab1 根据实验要求阅读并修改代码 编译源码 例如执行：make 如果不通过则返回步骤3 如果实现不正确（即看到步骤6的输出存在不是OK的情况）则返回3 如果实现基本正确（即看到6的输出都是OK）则push到自己github仓库 编写实验报告，发送到助教邮箱，并push到github仓库 [!NOTE|style:flat] 可以通过make qemu让OS实验工程在qemu上运行；可以通过make debug或make debug-nox命令实现通过gdb远程调试 OS实验工程。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/qian-dao-zhi-shi/le-jie-shi-yan-huan-jing.html":{"url":"lab0/qian-dao-zhi-shi/le-jie-shi-yan-huan-jing.html","title":"了解实验环境","keywords":"","body":"了解实验环境 在实验中，我们使用的系统环境是Ubuntu18.04。在实验过程中，我们需要了解基于命令行方式的编译、调试、运行操作系统的实验方法。为此，需要了解基本的Linux命令行使用。 [!TIP|style:flat] 当然现在只是了解一下，具体的操作还是要在安装好虚机以后再来哦~ 命令模式的基本结构和概念 Ubuntu是图形界面友好和易操作的Linux发行版，但有时只需执行几条简单的指令就可以完成繁琐的鼠标点击才能完成的操作。Linux的命令行操作模式功能可以实现你需要的所有操作。简单的说，命令行就是基于字符命令的用户界面，也被称为文本操作模式。绝大多数情况下， 用户通过输入一行或多行命令直接与计算机互动，来实现对计算机的操作。 如何进入命令模式 假设使用默认的图形界面为GNOME的任意版本Ubuntu Linux。点击鼠标右键->终端，就可以启动名为terminal程序，从而可以在此软件界面中进行命令行操作。 打开terminal程序后你首先可能会注意到类似下面的界面： kelee@ubuntu:~$ 你所看到的这些被称为命令终端提示符，它表示计算机已就绪，正在等待着用户输入操作指令。以我的屏幕画面为例，“kelee\"是当前所登录的用户名，“ubuntu”是这台计算机的主机名，“~”表示当前目录。此时输入任何指令按回车之后该指令将会提交到计算机运行，比如你可以输入命令：ls 再按下回车： ls [ENTER] 注意：[ENTER]是指输入完ls后按下回车键，而不是叫你输入这个单词，ls这个命令将会列出你当前所在目录里的所有文件和子目录列表。 下面介绍bash shell程序的基本使用方法，它是ubuntu缺省的外壳程序。 常用指令 查询文件列表：(ls) kelee@ubuntu:~$ ls Desktop Downloads Music Public Templates Documents examples.desktop Pictures riscv Videos ls命令默认状态下将按首字母升序列出你当前文件夹下面的所有内容，但这样直接运行所得到的信息也是比较少的，通常它可以结合以下这些参数运行以查询更多的信息： ls / # 将列出根目录'/'下的文件清单.如果给定一个参数，则命令行会把该参数当作命令行的工作目录。换句话说，命令行不再以当前目录为工作目录。 ls -l # 将给你列出一个更详细的文件清单. ls -a # 将列出包括隐藏文件(以.开头的文件)在内的所有文 件. ]ls -h # 将以KB/MB/GB的形式给出文件大小,而不是以纯粹的Bytes. 查询当前所在目录：(pwd) kelee@ubuntu:~$ pwd /home/kelee 进入其他目录：(cd) kelee@ubuntu:~$ cd riscv kelee@ubuntu:~/riscv$ pwd /home/kelee/riscv kelee@ubuntu:~/riscv$ 上面例子中，当前目录原来是/home/kelee,执行cd riscv之后再运行pwd可以发现，当前目录已经改为/riscv了。 在屏幕上输出字符： (echo) kelee@ubuntu:~$ echo \"Hello World\" Hello World 这是一个很有用的命令，它可以在屏幕上输入你指定的参数(\"\"号中的内容)，当然这里举的这个例子中它没有多大的实际意义，但随着你对LINUX指令的不断深入，就会发现它的价值所在。 显示文件内容：cat kelee@ubuntu:~$ cat tempfile.txt Roses are red. Violets are blue, and you have the bird-flue! 也可以使用less或more来显示比较大的文本文件内容。 复制文件： cp kelee@ubuntu:~$ cp tempfile.txt tempfile_copy.txt kelee@ubuntu:~$ cat tempfile_copy.txt Roses are red. Violets are blue, and you have the bird-flue! 移动文件：mv kelee@ubuntu:~$ ls Desktop Downloads Music Public tempfile_copy.txt Templates Documents examples.desktop Pictures riscv tempfile.txt Videos kelee@ubuntu:~$ mv tempfile_copy.txt tempfile_mv.txt kelee@ubuntu:~$ ls Desktop Downloads Music Public tempfile_mv.txt Templates Documents examples.desktop Pictures riscv tempfile.txt Videos 注意：在命令操作时系统基本上不会给你什么提示，当然，绝大多数的命令可以通过加上一个参数-v来要求系统给出执行命令的反馈信息； kelee@ubuntu:~$ mv -v tempfile_mv.txt tempfile_mv_v.txt renamed 'tempfile_mv.txt' -> 'tempfile_mv_v.txt' 建立一个空文本文件：touch kelee@ubuntu:~$ ls Desktop Downloads Music Public tempfile_mv_v.txt Templates Documents examples.desktop Pictures riscv tempfile.txt Videos kelee@ubuntu:~$ touch file1.txt kelee@ubuntu:~$ ls Desktop examples.desktop Pictures tempfile_mv_v.txt Videos Documents file1.txt Public tempfile.txt Downloads Music riscv Templates 建立一个目录：mkdir kelee@ubuntu:~$ ls Desktop examples.desktop Pictures tempfile_mv_v.txt Videos Documents file1.txt Public tempfile.txt Downloads Music riscv Templates kelee@ubuntu:~$ mkdir test_dir kelee@ubuntu:~$ ls Desktop examples.desktop Pictures tempfile_mv_v.txt test_dir Documents file1.txt Public tempfile.txt Videos Downloads Music riscv Templates 删除文件/目录：rm kelee@ubuntu:~$ ls Desktop examples.desktop Pictures tempfile_mv_v.txt test_dir Documents file1.txt Public tempfile.txt Videos Downloads Music riscv Templates kelee@ubuntu:~$ rm tempfile_mv_v.txt kelee@ubuntu:~$ ls -p Desktop/ examples.desktop Pictures/ tempfile.txt Videos/ Documents/ file1.txt Public/ Templates/ Downloads/ Music/ riscv/ test_dir/ kelee@ubuntu:~$ rm test_dir rm: cannot remove 'test_dir': Is a directory kelee@ubuntu:~$ rm -R test_dir kelee@ubuntu:~$ ls Desktop Downloads file1.txt Pictures riscv Templates Documents examples.desktop Music Public tempfile.txt Videos 在上面的操作：首先我们通过ls命令查询可知当前目下有两个文件和一个文件夹； [1] 你可以用参数 -p来让系统显示某一项的类型，比如是文件/文件夹/快捷链接等等； [2] 接下来我们用rm -i尝试删除文件，-i参数是让系统在执行删除操作前输出一条确认提示；i(interactive)也就是交互性的意思； [3] 当我们尝试用上面的命令去删除一个文件夹时会得到错误的提示，因为删除文件夹必须使用-R(recursive,循环）参数 特别提示：在使用命令操作时，系统假设你很明确自己在做什么，它不会给你太多的提示，比如你执行rm -Rf /，它将会删除你硬盘上所有的东西，并且不会给你任何提示，所以，尽量在使用命令时加上-i的参数，以让系统在执行前进行一次确认，防止你干一些蠢事。如 果你觉得每次都要输入-i太麻烦，你可以执行以下的命令，让－i成为默认参数： alias rm='rm -i' 查询当前进程：ps kelee@ubuntu:~$ ps PID TTY TIME CMD 3356 pts/0 00:00:00 bash 3659 pts/0 00:00:00 ps 这条命令会例出你所启动的所有进程； ps -a #可以例出系统当前运行的所有进程，包括由其他用户启动的进程； ps auxww #是一条相当人性化的命令，它会例出除一些很特殊进程以外的所有进程，并会以一个高可读的形式显示结果，每一个进程都会有较为详细的解释； 基本命令的介绍就到此为止，你可以访问网络得到更加详细的Linux命令介绍。 控制流程 输入/输出 input用来读取你通过键盘（或其他标准输入设备）输入的信息，output用于在屏幕（或其他标准输出设备）上输出你指定的输出内容.另外还有一些标准的出错提示也是通过这个命令来实现的。通常在遇到操作错误时，系统会自动调用这个命令来输出标准错误提示； 我们能重定向命令中产生的输入和输出流的位置。 重定向 如果你想把命令产生的输出流指向一个文件而不是（默认的）终端，你可以使用如下的语句： kelee@ubuntu:~$ ls >file2.txt kelee@ubuntu:~$ cat file2.txt Desktop Documents Downloads examples.desktop file1.txt file2.txt Music Pictures Public riscv tempfile.txt Templates Videos 以上例子将创建文件file2.txt如果file2.txt不存在的话。注意：如果file2.txt已经存在，那么上面的命令将复盖文件的内容。如果你想将内容添加到已存在的文件内容的最后，那你可以用下面这个语句： command >> filename 示例: kelee@ubuntu:~$ ls >>file2.txt kelee@ubuntu:~$ cat file2.txt Desktop Documents Downloads examples.desktop file1.txt file2.txt Music Pictures Public riscv tempfile.txt Templates Videos Desktop Documents Downloads examples.desktop file1.txt file2.txt Music Pictures Public riscv tempfile.txt Templates Videos 在这个例子中，你会发现原有的文件中添加了新的内容。接下来我们会见到另一种重定向方式：我们将把一个文件的内容作为将要执行的命令的输入。以下是这个语句： command 示例: kelee@ubuntu:~$ sort 管道 Linux的强大之处在于它能把几个简单的命令联合成为复杂的功能，通过键盘上的管道符号'|' 完成。现在，我们来排序上面的\"grep\"命令： kelee@ubuntu:~$ grep -i 'D' result.txt kelee@ubuntu:~$ cat result.txt Desktop Desktop Documents Documents Downloads Downloads examples.desktop examples.desktop Videos Videos 搜索 file2.txt 中的d字母，将输出分类并写入分类文件到 result.txt 。 有时候用ls列出很多命令的时候很不方便 这时“｜”就充分利用到了 ls -l | less 慢慢看吧. 后台进程 CLI 不是系统的串行接口。您可以在执行其他命令时给出系统命令。要启动一个进程到后台，追加一个“&”到命令后面。 sleep 60 & ls 睡眠命令在后台运行，您依然可以与计算机交互。除了不同步启动命令以外，最好把 '&' 理解成 ';'。 如果您有一个命令将占用很多时间，您想把它放入后台运行，也很简单。只要在命令运行时按下ctrl-z，它就会停止。然后键入 bg使其转入后台。fg 命令可使其转回前台。 sleep 60 # 这表示敲入Ctrl+Z键 bg fg 最后，您可以使用 ctrl-c 来杀死一个前台进程。 环境变量 特殊变量。PATH, PS1, ... 不显示中文 可通过执行如下命令避免显示乱码中文。在一个shell中，执行： export LANG=”” 这样在这个shell中，output信息缺省时英文。 获得软件包 命令行获取软件包 Ubuntu 下可以使用 apt-get 命令，apt-get 是一条 Linux 命令行命令，适用于 deb 包管理式的操作系统，主要用于自动从互联网软件库中搜索、安装、升级以及卸载软件或者操作系统。一般需要 root 执行权限，所以一般跟随 sudo 命令，如： sudo apt-get install gcc [ENTER] 常见的以及常用的 apt 命令有： apt-get install 下载 以及所依赖的软件包，同时进行软件包的安装或者升级。 apt-get remove 移除 以及所有依赖的软件包。 apt-cache search 搜索满足 的软件包。 apt-cache show/showpkg 显示软件包 的完整描述。 例如： kelee@ubuntu:~$ apt-cache search aptitude aptitude - terminal-based package manager aptitude-common - architecture independent files for the aptitude package manager aptitude-doc-en - English manual for aptitude, a terminal-based package manager libcwidget-dev - high-level terminal interface library for C++ (development files) apt-cacher - Caching proxy server for Debian/Ubuntu software repositories apticron - Simple tool to mail about pending package updates - cron version apticron-systemd - Simple tool to mail about pending package updates - systemd version aptitude-doc-cs - Czech manual for aptitude, a terminal-based package manager aptitude-doc-es - Spanish manual for aptitude, a terminal-based package manager aptitude-doc-fi - Finnish manual for aptitude, a terminal-based package manager aptitude-doc-fr - French manual for aptitude, a terminal-based package manager aptitude-doc-it - Italian manual for aptitude, a terminal-based package manager aptitude-doc-ja - Japanese manual for aptitude, a terminal-based package manager aptitude-doc-nl - Dutch manual for aptitude, a terminal-based package manager aptitude-doc-ru - Russian manual for aptitude, a terminal-based package manager aptitude-robot - Automate package choice management cron-apt - automatic update of packages using apt-get cupt - flexible package manager -- console interface gbrainy - brain teaser game and trainer to have fun and to keep your brain trained pkgsync - automated package list synchronization wajig - unified package management front-end for Debian kelee@ubuntu:~$ 配置升级源 Ubuntu的软件包获取依赖升级源，通过Software&Updates->Ubuntu Software->Download from:->Other:->China->mirrors.aliyun.com->Choose Server 查找帮助文件 Ubuntu 下提供 man 命令以完成帮助手册得查询。man 是 manual 的缩写，通过 man 命令可以对 Linux 下常用命令、安装软件、以及C语言常用函数等进行查询，获得相关帮助。 例如： kelee@ubuntu:~$ man printf PRINTF(1) User Commands PRINTF(1) NAME printf - format and print data SYNOPSIS printf FORMAT [ARGUMENT]... printf OPTION DESCRIPTION Print ARGUMENT(s) according to FORMAT, or execute according to OPTION: --help display this help and exit --version output version information and exit FORMAT controls the output as in C printf. Interpreted sequences are: \\\" double quote \\\\ backslash Manual page printf(1) line 1 (press h for help or q to quit) 通常可能会用到的帮助文件例如： gcc-doc cpp-doc glibc-doc 上述帮助文件可以通过 apt-get 命令或者软件包管理器获得。获得以后可以通过 man 命令进行命令或者参数查询。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/qian-dao-zhi-shi/le-jie-kai-fa-tiao-shi-ji-ben-gong-ju.html":{"url":"lab0/qian-dao-zhi-shi/le-jie-kai-fa-tiao-shi-ji-ben-gong-ju.html","title":"了解开发调试基本工具","keywords":"","body":"了解开发调试基本工具 编辑器 Understand 在OS实验网站上有Understand Windows版的资源。该软件是一个阅读代码的很好工具，可以可视化的看到各个函数之间的调用关系，可以很好的找到函数、变量的定义，具体的使用方法以及介绍可以参考该教程。但是就编辑代码来说，不建议使用Understand。 VScode VScode是很好的项目管理、代码编译器工具，集成了git，并且可以安装各类插件支持各种语言，习惯使用visual studio的同学使用起来会非常习惯，具体的下载安装使用方法，可以参考该教程，值得说明的是，我们在编译的时候需要其他工具联合编译，因此可以仅仅把VScode当成没有感情的写代码工具，不由它来编译运行，编译运行交给终端。 [!NOTE|style:flat] VScode 快捷键的使用在Windows和Ubuntu上有些不同哦~ 写完了代码别忘了格式化代码鸭~看起来好舒服的！ 编译器:GCC 在Ubuntu Linux中的C语言编程主要基于GNU C的语法，通过gcc来编译并生成最终执行文件。GNU汇编（assembler）采用的是AT&T汇编格式，Microsoft 汇编采用Intel格式。 编译简单的 C 程序 C 语言经典的入门例子是 Hello World，下面是一示例代码： #include int main(void) { printf(\"Hello, world!\\n\"); return 0; } 我们假定该代码存为文件‘hello.c’。要用 gcc 编译该文件，使用下面的命令： $ gcc -Wall hello.c -o hello 该命令将文件‘hello.c’中的代码编译为机器码并存储在可执行文件 ‘hello’中。机器码的文件名是通过 -o 选项指定的。该选项通常作为命令行中的最后一个参数。如果被省略，输出文件默认为 ‘a.out’。 注意到如果当前目录中与可执行文件重名的文件已经存在，它将被复盖。 选项 -Wall 开启编译器几乎所有常用的警告──强烈建议你始终使用该选项。编译器有很多其他的警告选项，但 -Wall 是最常用的。默认情况下GCC 不会产生任何警告信息。当编写 C 或 C++ 程序时编译器警告非常有助于检测程序存在的问题。 本例中，编译器使用了 -Wall 选项而没产生任何警告，因为示例程序是完全合法的。 要运行该程序，输入可执行文件的路径如下： $ ./hello Hello, world! 这将可执行文件载入内存，并使 CPU 开始执行其包含的指令。 路径 ./ 指代当前目录，因此 ./hello 载入并执行当前目录下的可执行文件 ‘hello’。 AT&T汇编基本语法 Ucore中用到的是AT&T格式的汇编，与Intel格式的汇编有一些不同。二者语法上主要有以下几个不同： * 寄存器命名原则 AT&T: %eax Intel: eax * 源/目的操作数顺序 AT&T: movl %eax, %ebx Intel: mov ebx, eax * 常数/立即数的格式　 AT&T: movl $_value, %ebx Intel: mov eax, _value 把value的地址放入eax寄存器 AT&T: movl $0xd00d, %ebx Intel: mov ebx, 0xd00d * 操作数长度标识 AT&T: movw %ax, %bx Intel: mov bx, ax * 寻址方式 AT&T: immed32(basepointer, indexpointer, indexscale) Intel: [basepointer + indexpointer × indexscale + imm32) 如果操作系统工作于保护模式下，用的是32位线性地址，所以在计算地址时不用考虑segment:offset的问题。上式中的地址应为： imm32 + basepointer + indexpointer × indexscale 下面是一些例子： * 直接寻址 AT&T: foo Intel: [foo] boo是一个全局变量。注意加上$是表示地址引用，不加是表示值引用。对于局部变量，可以通过堆栈指针引用。 * 寄存器间接寻址 AT&T: (%eax) Intel: [eax] * 变址寻址 AT&T: _variable(%eax) Intel: [eax + _variable] AT&T: _array( ,%eax, 4) Intel: [eax × 4 + _array] AT&T: _array(%ebx, %eax,8) Intel: [ebx + eax × 8 + _array] GCC基本内联汇编 GCC 提供了两内内联汇编语句（inline asm statements）：基本内联汇编语句（basic inline asm statement)和扩展内联汇编语句（extended inline asm statement）。GCC基本内联汇编很简单，一般是按照下面的格式： asm(\"statements\"); 例如： asm(\"nop\"); asm(\"cli\"); \"asm\" 和 \"asm\" 的含义是完全一样的。如果有多行汇编，则每一行都要加上 \"\\n\\t\"。其中的 “\\n” 是换行符，\"\\t” 是 tab 符，在每条命令的 结束加这两个符号，是为了让 gcc 把内联汇编代码翻译成一般的汇编代码时能够保证换行和留有一定的空格。对于基本asm语句，GCC编译出来的汇编代码就是双引号里的内容。例如： asm( \"pushl %eax\\n\\t\" \"movl $0,%eax\\n\\t\" \"popl %eax\" ); 实际上gcc在处理汇编时，是要把asm(...)的内容\"打印\"到汇编文件中，所以格式控制字符是必要的。再例如： asm(\"movl %eax, %ebx\"); asm(\"xorl %ebx, %edx\"); asm(\"movl $0, _boo); 在上面的例子中，由于我们在内联汇编中改变了 edx 和 ebx 的值，但是由于 gcc 的特殊的处理方法，即先形成汇编文件，再交给 GAS 去汇编，所以 GAS 并不知道我们已经改变了 edx和 ebx 的值，如果程序的上下文需要 edx 或 ebx 作其他内存单元或变量的暂存，就会产生没有预料的多次赋值，引起严重的后果。对于变量 _boo也存在一样的问题。为了解决这个问题，就要用到扩展 GCC 内联汇编语法。 GCC扩展内联汇编 使用GCC扩展内联汇编的例子如下： #define read_cr0() ({ \\ unsigned int __dummy; \\ __asm__( \\ \"movl %%cr0,%0\\n\\t\" \\ :\"=r\" (__dummy)); \\ __dummy; \\ }) 它代表什么含义呢？这需要从其基本格式讲起。GCC扩展内联汇编的基本格式是： asm [volatile] ( Assembler Template : Output Operands [ : Input Operands [ : Clobbers ] ]) 其中，asm 表示汇编代码的开始，其后可以跟 volatile（这是可选项），其含义是避免 “asm” 指令被删除、移动或组合，在执行代码时，如果不希望汇编语句被 gcc 优化而改变位置，就需要在 asm 符号后添加 volatile 关键词：asm volatile(...)；或者更详细地说明为：asm volatile(...)；然后就是小括弧，括弧中的内容是具体的内联汇编指令代码。 \"\" 为汇编指令部分，例如，\"movl %%cr0,%0\\n\\t\"。数字前加前缀 “％“，如％1，％2等表示使用寄存器的样板操作数。可以使用的操作数总数取决于具体CPU中通用寄存器的数 量，如Intel可以有8个。指令中有几个操作数，就说明有几个变量需要与寄存器结合，由gcc在编译时根据后面输出部分和输入部分的约束条件进行相应的处理。由于这些样板操作数的前缀使用了”％“，因此，在用到具体的寄存器时就在前面加两个“％”，如%%cr0。输出部分（output operand list），用以规定对输出变量（目标操作数）如何与寄存器结合的约束（constraint）,输出部分可以有多个约束，互相以逗号分开。每个约束以“＝”开头，接着用一个字母来表示操作数的类型，然后是关于变量结合的约束。例如，上例中： :\"=r\" (__dummy) “＝r”表示相应的目标操作数（指令部分的%0）可以使用任何一个通用寄存器，并且变量__dummy 存放在这个寄存器中，但如果是： :“＝m”(__dummy) “＝m”就表示相应的目标操作数是存放在内存单元__dummy中。表示约束条件的字母很多，下表给出几个主要的约束字母及其含义： 字母 含义 m, v, o 内存单元 R 任何通用寄存器 Q 寄存器eax, ebx, ecx,edx之一 I, h 直接操作数 E, F 浮点数 G 任意 a, b, c, d 寄存器eax/ax/al, ebx/bx/bl, ecx/cx/cl或edx/dx/dl S, D 寄存器esi或edi I 常数（0～31） 输入部分（input operand list）：输入部分与输出部分相似，但没有“＝”。如果输入部分一个操作数所要求使用的寄存器，与前面输出部分某个约束所要求的是同一个寄存器，那就把对应操作数的编号（如“1”，“2”等）放在约束条件中。在后面的例子中，可看到这种情况。修改部分（clobber list,也称 乱码列表）:这部分常常以“memory”为约束条件，以表示操作完成后内存中的内容已有改变，如果原来某个寄存器的内容来自内存，那么现在内存中这个单元的内容已经改变。乱码列表通知编译器，有些寄存器或内存因内联汇编块造成乱码，可隐式地破坏了条件寄存器的某些位（字段）。 注意，指令部分为必选项，而输入部分、输出部分及修改部分为可选项，当输入部分存在，而输出部分不存在时，冒号“：”要保留，当“memory”存在时，三个冒号都要保留，例如 #define __cli() __asm__ __volatile__(\"cli\": : :\"memory\") 下面是一个例子： int count=1; int value=1; int buf[10]; void main() { asm( \"cld \\n\\t\" \"rep \\n\\t\" \"stosl\" : : \"c\" (count), \"a\" (value) , \"D\" (buf) ); } 得到的主要汇编代码为： movl count,%ecx movl value,%eax movl buf,%edi #APP cld rep stosl #NO_APP cld,rep,stos这几条语句的功能是向buf中写上count个value值。冒号后的语句指明输入，输出和被改变的寄存器。通过冒号以后的语句，编译器就知道你的指令需要和改变哪些寄存器，从而可以优化寄存器的分配。其中符号\"c\"(count)指示要把count的值放入ecx寄存器。类似的还有： a eax b ebx c ecx d edx S esi D edi I 常数值，(0 - 31) q,r 动态分配的寄存器 g eax,ebx,ecx,edx或内存变量 A 把eax和edx合成一个64位的寄存器(use long longs) 也可以让gcc自己选择合适的寄存器。如下面的例子： asm(\"leal (%1,%1,4),%0\" : \"=r\" (x) : \"0\" (x) ); 这段代码到的主要汇编代码为： movl x,%eax #APP leal (%eax,%eax,4),%eax #NO_APP movl %eax,x 几点说明： [1] 使用q指示编译器从eax, ebx, ecx, edx分配寄存器。 使用r指示编译器从eax, ebx, ecx, edx, esi, edi分配寄存器。 [2] 不必把编译器分配的寄存器放入改变的寄存器列表，因为寄存器已经记住了它们。 [3] \"=\"是标示输出寄存器，必须这样用。 [4] 数字%n的用法：数字表示的寄存器是按照出现和从左到右的顺序映射到用\"r\"或\"q\"请求的寄存器．如果要重用\"r\"或\"q\"请求的寄存器的话，就可以使用它们。 [5] 如果强制使用固定的寄存器的话，如不用%1，而用ebx，则： asm(\"leal (%%ebx,%%ebx,4),%0\" : \"=r\" (x) : \"0\" (x) ); [!NOTE|style:flat] 注意要使用两个%,因为一个%的语法已经被%n用掉了。 代码维护 make和Makefile 简介 GNU make(简称make)是一种代码维护工具，在大中型项目中，它将根据程序各个模块的更新情况，自动的维护和生成目标代码。 make命令执行时，需要一个 makefile （或Makefile）文件，以告诉make命令需要怎么样的去编译和链接程序。首先，我们用一个示例来说明makefile的书写规则。以便给大家一个感兴认识。这个示例来源于gnu的make使用手册，在这个示例中，我们的工程有8个c文件，和3个头文件，我们要写一个makefile来告诉make命令如何编译和链接这几个文件。我们的规则是： 如果这个工程没有编译过，那么我们的所有c文件都要编译并被链接。 如果这个工程的某几个c文件被修改，那么我们只编译被修改的c文件，并链接目标程序。 如果这个工程的头文件被改变了，那么我们需要编译引用了这几个头文件的c文件，并链接目标程序。 只要我们的makefile写得够好，所有的这一切，我们只用一个make命令就可以完成，make命令会自动智能地根据当前的文件修改的情况来确定哪些文件需要重编译，从而自己编译所需要的文件和链接目标程序。 makefile的规则 在讲述这个makefile之前，还是让我们先来粗略地看一看makefile的规则。 target ... : prerequisites ... command ... ... target也就是一个目标文件，可以是object file，也可以是执行文件。还可以是一个标签（label）。prerequisites就是，要生成那个target所需要的文件或是目标。command也就是make需要执行的命令（任意的shell命令）。 这是一个文件的依赖关系，也就是说，target这一个或多个的目标文件依赖于prerequisites中的文件，其生成规则定义在 command中。如果prerequisites中有一个以上的文件比target文件要新，那么command所定义的命令就会被执行。这就是makefile的规则。也就是makefile中最核心的内容。 可以查看GNU手册，或者查看这份中文教程。 Git Git是一个开源的分布式版本控制系统，可以有效、高速地处理从很小到非常大的项目版本管理。Git 是 Linus Torvalds 为了帮助管理 Linux 内核开发而开发的一个开放源码的版本控制软件。Git 与Github不一样哦，Git是工具，而GitHub是可以用Git进行管理的远程仓库。 代码层次 你目录中的文件是第一层 缓存区，每次add之后，当前目录中要追踪的文件会作为一个版本会存放在缓存区。注意不是所有的文件。一般一个文件生成之后，会标记为“未追踪”，但是否对其做版本管理还是要选择的。例如一些编译文件就没有必要追踪。对需要做版本管理的问件，用add添加，不需要的用clean删除。 本地仓库，每次commit之后，缓存区最新的版本就会存放在本地仓库。这里要提及一个HEAD的概念。HEAD是当前的版本指向，每次更新或者回退都会修改HEAD的指向，但对仓库中每一个版本并不会删除。所以即使回退到过去还是有机会回到现在的版本的。 远程仓库，每次push之后，会将本地仓库中HEAD所指向的版本存放到远程仓库 常用命令 命令 功能 git init 在本地的当前目录里初始化git仓库 git status 查看当前仓库的状态 git add -A 增加目录中所有的文件到缓存区 git add file 增加相应文件到缓存区 git commit -m \"信息\" 将缓存区中更改提交到本地仓库 git log 查看当前版本之前的提交记录 git reflog 查看HEAD的变更记录，包括回退 git branch -b branch_name 建立一个新的分支 git diff 查看当前文件与缓存区文件的差异 git checkout -- file 取消更改，将缓存区的文件提取覆盖当前文件 git reset --hard 版本号 回退到相应版本号，同样也可以回退到未来的版本号 git clean -xf 删除当前目录中所有未追踪的文件 git config --global core.quotepath false 处理中文文件名 与Github链接 首先我们认为你已经有一个github的账户。 然后我们要建立SSH链接。这是一种通讯的加密协议。我先在我的笔记本上计算一对公钥和私钥，将公钥存储在github中，这样本地就可以通过SSH与github展开加密通讯。 建立方法，输入命令 ssh-keygen -t rsa -C \"your_email@youremail.com\" //双引号里面是你的常用邮箱 输入之后要输入口令，可以不用输入直接按“enter”一路确认就可以了。然后在账户的根目录（/或者/home/你的账户名，具体取决于你执行上述命令时所采用的账户）查找隐藏目录.ssh/id_rsa.pub文件，将当中内容添加到github中。 这样你就可以通过SSH链接到github中了。但是github作为一个远程仓库，你可以链接这个仓库，并保持同步。但是你不能把本地仓库直接上传到github中去。所以你应该先在github中建立一个对应的仓库，然后再在本地建立一个仓库，将两者进行链接，再去写入文件执行版本管理。所用到的命令有 git remote add origin git@github.com:/.git git pull origin master //因为github建立仓库时会有readme.md文件，先要拷贝一份 git push -u origin master //将本地仓库链接到master分支上，你当然可以链接到其他分支 git push//上传你的本地仓库 还有一种方法不用分两地建库再去链接。你可以只在github上建库，然后clone到本地目录中。 git clone git@github.com:/.git VScode中使用 因为VScode是一个集成工具可以直接在VScode中使用Git，用VScode打开已经配置好的仓库，VScode就可以自动读取里面的内容，然后当进行修改后可以通过VScode直接commit与push。具体的操作可以参考该教程。 调试器:GDB 功能 gdb 是功能强大的调试程序，可完成如下的调试任务： 设置断点 监视程序变量的值 程序的单步(step in/step over)执行 显示/修改变量的值 显示/修改寄存器 查看程序的堆栈情况 远程调试 调试线程 在可以使用 gdb 调试程序之前，必须使用 -g 或 –ggdb编译选项编译源文件。运行 gdb 调试程序时通常使用如下的命令： gdb progname 在 gdb 提示符处键入help，将列出命令的分类，主要的分类有： aliases：命令别名 breakpoints：断点定义； data：数据查看； files：指定并查看文件； internals：维护命令； running：程序执行； stack：调用栈查看； status：状态查看； tracepoints：跟踪程序执行。 键入 help 后跟命令的分类名，可获得该类命令的详细清单。 常用命令 命令 功能 break FILENAME:NUM 在特定源文件特定行上设置断点 clear FILENAME:NUM 删除设置在特定源文件特定行上的断点 run 运行调试程序 step 单步执行调试程序，不会直接执行函数 next 单步执行调试程序，会直接执行函数 backtrace 显示所有的调用栈帧。该命令可用来显示函数的调用顺序 where continue 继续执行正在调试的程序 display EXPR 每次程序停止后显示表达式的值,表达式由程序定义的变量组成 file FILENAME 装载指定的可执行文件进行调试 help CMDNAME 显示指定调试命令的帮助信息 info break 显示当前断点列表，包括到达断点处的次数等 info files 显示被调试文件的详细信息 info func 显示被调试程序的所有函数名称 info prog 显示被调试程序的执行状态 info local 显示被调试程序当前函数中的局部变量信息 info var 显示被调试程序的所有全局和静态变量名称 kill 终止正在被调试的程序 list 显示被调试程序的源代码 quit 退出 gdb 窗口相关命令 用gdb查看源代码可以用list命令，但是这个不够灵活。可以使用\"layout src\"命令，或者按Ctrl-X再按A，就会出现一个窗口可以查看源代码。也可以用使用-tui参数，这样进入gdb里面后就能直接打开代码查看窗口。其他代码窗口相关命令： 命令 功能 info win 显示窗口的大小 layout next 切换到下一个布局模式 layout prev 切换到上一个布局模式 layout src 只显示源代码 layout asm 只显示汇编代码 layout split 显示源代码和汇编代码 layout regs 增加寄存器内容显示 focus cmd/src/asm/regs/next/prev 切换当前窗口 refresh 刷新所有窗口 tui reg next 显示下一组寄存器 tui reg system 显示系统寄存器 update 更新源代码窗口和当前执行点 winheight name +/- line 调整name窗口的高度 tabset nchar 设置tab为nchar个字符 示例 下面以一个有错误的例子程序来介绍gdb的使用： /*bugging.c*/ #include #include static char buff [256]; static char* string; int main () { printf (\"Please input a string: \"); gets (string); printf (\"\\nYour string is: %s\\n\", string); } 这个程序是接受用户的输入，然后将用户的输入打印出来。该程序使用了一个未经过初始化的字符串地址 string，因此，编译并运行之后，将出现 \"Segment Fault\"错误： $ gcc -o bugging -g bugging.c $ ./bugging Please input a string: asdf Segmentation fault (core dumped) 为了查找该程序中出现的问题，我们利用 gdb，并按如下的步骤进行： [1] 运行 “gdb bugging” ，加载 bugging 可执行文件； $gdb bugging [2] 执行装入的 bugging 命令； (gdb) run [3] 使用 where 命令查看程序出错的地方； (gdb) where [4] 利用 list 命令查看调用 gets 函数附近的代码； (gdb) list [5] 在 gdb 中，我们在第 11 行处设置断点，看看是否是在第11行出错； (gdb) break 11 [6] 程序重新运行到第 11 行处停止，这时程序正常，然后执行单步命令next； (gdb) next [7] 程序确实出错，能够导致 gets 函数出错的因素就是变量 string。重新执行测试程，用 print 命令查看 string 的值； (gdb) run (gdb) print string (gdb) $1=0x0 [8] 问题在于string指向的是一个无效指针，修改程序，在10行和11行之间增加一条语句 “string=buff; ”，重新编译程序，然后继续运行，将看到正确的程序运行结果。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/qian-dao-zhi-shi/le-jie-ying-jian-mo-ni-qi.html":{"url":"lab0/qian-dao-zhi-shi/le-jie-ying-jian-mo-ni-qi.html","title":"了解硬件模拟器","keywords":"","body":"了解硬件模拟器 简介 我们有了操作系统的代码，那要在哪里去运行呢。我们当然可以像计算机组成原理一样去烧制一块RISC-v架构的开发板，然后去debug。虽然这样可以，但duck不必，使用模拟器会使我们的实验更加方便。模拟器就是在计算机上通过软件模拟一个RISC-v架构的硬件平台，从而能够运行RISC-v的目标代码。 模拟器有很多，但我们为了方便，选择的是QEMU模拟器，的优点在于，内置了一套OpenSBI固件的实现，可以简化我们的代码。 常用命令 help 查看 qemu 帮助，显示所有支持的命令。 q、quit、exit 退出 qemu。 stop 停止 qemu。 c、cont、continue 连续执行。 x /fmt addr xp /fmt addr 显示内存内容，其中 'x' 为虚地址，'xp' 为实地址。 参数 /fmt i 表示反汇编，缺省参数为前一次参数。 p、print 计算表达式值并显示，例如 $reg 表示寄存器结果。 memsave addr size file pmemsave addr size file 将内存保存到文件，memsave 为虚地址，pmemsave 为实地址。 breakpoint 相关： 设置、查看以及删除 breakpoint，pc执行到 breakpoint，qemu 停止。（暂时没有此功能） watchpoint 相关： 设置、查看以及删除 watchpoint, 当 watchpoint 地址内容被修改，停止。（暂时没有此功能） s、step 单步一条指令，能够跳过断点执行。 r、registers 显示全部寄存器内容。 info 相关操作 查询 qemu 支持的关于系统状态信息的操作。 其他具体的命令格式以及说明，参见 qemu help 命令帮助。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/pei-zhi-huan-jing/":{"url":"lab0/pei-zhi-huan-jing/","title":"配置环境","keywords":"","body":"配置环境 看了这么多理论知识，终于可以动手开始实验啦。相信经过大一的学习，很多同学都会明白环境的重要性。在这一模块我们需要实现以下东西。 安装好Ubuntu虚拟系统，当然也可以是双系统，但不建议哈。太麻烦了。大家伙对Ubuntu可能不太熟悉，所以指导了大家安装了一些小工具，能够让大家用起来舒服一些。 安装开发工具。这里大家就要小心啦，涉及到编译、运行的东西一定咬谨慎哦。 安装硬件模拟器。我们的系统是需要运行在RISC-v架构的计算机上的，但是可以用软件模拟，为什么要用硬件呢，成本还低。所以要安装好硬件模拟器噢~ 我们写出来的代码不一定立马是对的，所以需要调试工具与硬件模拟器一起来联合调试。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/pei-zhi-huan-jing/an-zhuang-xu-ni-huan-jing.html":{"url":"lab0/pei-zhi-huan-jing/an-zhuang-xu-ni-huan-jing.html","title":"安装虚拟环境","keywords":"","body":"安装虚拟环境 安装Ubuntu 首先下载并安装VMware15 客户端（VMware只支持Windows与Linux，MAC需要自己上网查找虚拟机安装教程）。 到清华镜像站下载Ubuntu 18.04.3 的镜像文件。 具体的安装步骤可参考这个教程。 [!NOTE|style:flat] 不建议大家使用最新版的Ubuntu系统哦，因为可能会有一些玄学问题。 记得根据【了解实验环境】里面的配置升级源哦。 安装小工具 aptitude sudo apt get install aptitude 这个工具是安装软件的一个工具，可以自己解决包依赖问题，之后安装可以直接使用sudo aptitude install $APP($APP 为要安装的软件名字) gnome-tweaks sudo aptitude install gnome-tweaks 有的同学可能会感觉Ubuntu的字体太小，可以安装gnome-tweaks，来调整哦。具体的使用方法，自己研究哈。 搜狗输入法 默认的Ubuntu调中文还是比较麻烦，可以到搜狗输入法官网下载。 v2rayL 在某些时候，我们不得已需要科学上网的时候，可以使用一些工具，Windows的工具很多，但Linux的工具很少，可以前往这里下载一个工具。具体的安装步骤自行领会。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/pei-zhi-huan-jing/an-zhuang-kai-fa-gong-ju.html":{"url":"lab0/pei-zhi-huan-jing/an-zhuang-kai-fa-gong-ju.html","title":"安装开发工具","keywords":"","body":"安装开发工具 设置环境变量 方便起见，可以先在终端里设置一个叫做RISCV的环境变量(在bash命令里可以通过$RISCV使用)，作为你安装所有和riscv有关的软件的路径。在/etc/profile里面写一行export RISCV=/your/path/to/riscv之类的东西就行。后面安装的各个项目最好也放在上面的的路径里面。当然需要去创建这个文件夹。 最小的软件开发环境需要：能够编译程序，能够运行程序。开发操作系统这样的系统软件也不例外。 安装VScode 进入其官网进行下载安装即可，与Windows类似。 安装git sudo aptitude install git git下载完后记得初始化噢~并且与自己的github连接起来。 安装编译器 我们使用的计算机都是基于x86架构的。如何把程序编译到riscv64架构的汇编？这需要我们使用“目标语言为riscv64机器码的编译器”，在我们的电脑上进行交叉编译。 放心，这里不需要你自己写编译器。我们使用现有的riscv-gcc编译器即可。从https://github.com/riscv/riscv-gcc clone下来，然后在x86架构上编译riscv-gcc编译器为可执行的x86程序，就可以运行它，来把你的程序源代码编译成riscv架构的可执行文件了。这有点像绕口令，但只要有一点编译原理的基础就可以理解。不过，这个riscv-gcc仓库很大，而且自己编译工具链总是一件麻烦的事。 其实，没必要那么麻烦，我们大可以使用别人已经编译好的编译器的可执行文件，也就是所谓的预编译（prebuilt）工具链，下载下来，放在你喜欢的地方（比如之前定义的$RISCV），配好路径（把编译器的位置加到系统的PATH环境变量里），就能在终端使用了。我们推荐使用sifive公司提供的预编译工具链，下载“GNU Embedded Toolchain ”。然后解压到之前的riscv文件夹下，把里面的bin文件夹加入到环境变量。修改完记得运行source /etc/profile噢。 配置好后，在终端输入riscv64-unknown-elf-gcc -v查看安装的gcc版本, 如果输出一大堆东西且最后一行有gcc version 某个数字.某个数字.某个数字，说明gcc配置成功，否则需要检查一下哪里做错了，比如环境变量PATH配置是否正确。一般需要把一个形如..../bin的目录加到PATH里。 [!NOTE|style:flat] 可能有人会说，到底该怎么去做嘛，烦人，都没有写完。没有写完的，剩一点点的需要自己去查资料，自己实现哦~ Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/pei-zhi-huan-jing/an-zhuang-ying-jian-mo-ni-qi.html":{"url":"lab0/pei-zhi-huan-jing/an-zhuang-ying-jian-mo-ni-qi.html","title":"安装硬件模拟器","keywords":"","body":"安装硬件模拟器 下载安装QEMU 我们也像安装编译器一样，去sifive的官网下载Ubuntu系统的riscv-qemu，里面的很多设置都已经设置好了的。在下载完，然后解压到之前的riscv文件夹下，把里面的bin文件夹加入到环境变量。修改完记得重启虚机噢。（这次熟练多了吧~）。 使用OpenSBI 新版 Qemu 中内置了 OpenSBI 固件（firmware），它主要负责在操作系统运行前的硬件初始化和加载操作系统的功能。我们使用以下命令尝试运行一下： qemu-system-riscv64 --machine virt --nographic --bios default 如果成功的话，就可以看到。 OpenSBI v0.5 (Oct 9 2019 12:03:04) ____ _____ ____ _____ / __ \\ / ____| _ \\_ _| | | | |_ __ ___ _ __ | (___ | |_) || | | | | | '_ \\ / _ \\ '_ \\ \\___ \\| _ 可以看到我们已经在 qemu-system-riscv64 模拟的 virt machine 硬件上将 OpenSBI 这个固件 跑起来了。Qemu 可以使用 Ctrl+a 再按下 x 退出（注意要松开Ctrl再单独按x）。 如果无法正常使用 Qemu，可以尝试下面这个命令。 $ sudo sysctl vm.overcommit_memory=1 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0/pei-zhi-huan-jing/an-zhuang-tiao-shi-gong-ju.html":{"url":"lab0/pei-zhi-huan-jing/an-zhuang-tiao-shi-gong-ju.html","title":"安装调试工具","keywords":"","body":"安装调试工具 编译 其实说安装有点不准确，因为之前已经把GDB已经悄悄的装进了环境变量里。其实就是在我们安装编译器的时候我们只需要打开lab0的文件夹，然后打开终端，输入make，就可以进行编译了 $ make + cc kern/init/entry.S + cc kern/init/init.c + cc kern/libs/stdio.c + cc kern/driver/console.c + cc libs/string.c + cc libs/printfmt.c + cc libs/readline.c + cc libs/sbi.c + ld bin/kernel riscv64-unknown-elf-objcopy bin/kernel --strip-all -O binary bin/ucore.img 可以看到多了几个文件夹，我们之后再来介绍这些。 修改脚本 工具链（之前下载的编译器里面的工具）里提供了相应的gdb工具，要使用gdb，就需要对脚本进行修改。在源代码的文件夹中，找到lab0中的Makefile文件然后找到CC := $(GCCPREFIX)gcc在后面添加-g。 使用GDB进行调试 因为gdb和qemu是两个应用不能直接交流，比较常用的方法是以tcp进行通讯，也就是让qemu在localhost::1234端口上等待。 在lab0文件夹下打开终端，运行 $ qemu-system-riscv64 -S -s -hda ./bin/ucore.img WARNING: Image format was not specified for './bin/ucore.img' and probing guessed raw. Automatically detecting the format is dangerous for raw images, write operations on block 0 will be restricted. Specify the 'raw' format explicitly to remove the restrictions. VNC server running on 127.0.0.1:5900xxxxxxxxxx qemu-system-riscv64 -S -s -hda ./bin/ucore.img$ qemu-system-riscv64 -S -s -hda ./bin/ucore.img WARNING: Image format was not specified for './bin/ucore.img' and probing guessed raw. Automatically detecting the format is dangerous for raw images, write operations on block 0 will be restricted. Specify the 'raw' format explicitly to remove the restrictions.VNC server running on 127.0.0.1:5900 然后在该文件夹下重新打开一个终端，运行 $ riscv64-unknown-elf-gdb ./bin/kernel GNU gdb (SiFive GDB 8.3.0-2020.04.0) 8.3 Copyright (C) 2019 Free Software Foundation, Inc. License GPLv3+: GNU GPL version 3 or later This is free software: you are free to change and redistribute it. There is NO WARRANTY, to the extent permitted by law. Type \"show copying\" and \"show warranty\" for details. This GDB was configured as \"--host=x86_64-linux-gnu --target=riscv64-unknown-elf\". Type \"show configuration\" for configuration details. For bug reporting instructions, please see: . Find the GDB manual and other documentation resources online at: . For help, type \"help\". Type \"apropos word\" to search for commands related to \"word\"... Reading symbols from ./bin/kernel... (gdb) 接着连接qemu： (gdb) target remote :1234 Remote debugging using :1234 0x0000000000001000 in ?? () 连接成功输入si就可以进行运行下一条指令， (gdb) si 0x0000000000001004 in ?? () 这样就代表可以正常运行了噢，具体调试步骤与方法，看一下前面哈。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0.5/":{"url":"lab0.5/","title":"LAB0.5：最小可执行内核","keywords":"","body":"LAB0.5：最小可执行内核 相对于上百万行的现代操作系统(linux, windows), 几千行的ucore是一只\"麻雀\"。但这只麻雀依然是一只胖麻雀，我们一眼看不过来几千行的代码。所以，我们要再做简化，先用好刀法，片掉麻雀的血肉, 搞出一个\"麻雀骨架\"，看得通透，再像组装哪吒一样，把血肉安回去，变成一个活生生的麻雀。 lab0.5是lab1的预备，我们构建一个最小的可执行内核（”麻雀骨架“），它能够进行格式化的输出，然后进入死循环。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0.5/shi-yan-mu-de.html":{"url":"lab0.5/shi-yan-mu-de.html","title":"实验目的","keywords":"","body":"实验目的 逐步掌握以下过程： 源码是如何被编译成可执行文件的。 编译成可执行文件后，计算机如何加载操作系统。 加载以后，该从哪里去运行操作系统。 操作系统的输出信息是怎么输出的呢。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0.5/shi-yan-nei-rong.html":{"url":"lab0.5/shi-yan-nei-rong.html","title":"实验内容","keywords":"","body":"实验内容 跟着实验指导书的步伐，阅读框架代码。 结合框架代码，深刻理解RISC-v。 内核的内存布局和入口点设置 通过sbi封装好输入输出函数 借助bootloader:OpenSBI初始化OS，完成练习。 按要求撰写实验报告。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0.5/lian-xi.html":{"url":"lab0.5/lian-xi.html","title":"练习","keywords":"","body":"练习 练习1：理解通过make生成执行文件的过程 列出本实验各练习中对应的OS原理的知识点，并说明本实验中的实现部分如何对应和体现了原理中的基本概念和关键知识点。 在此练习中，大家需要通过静态分析代码来了解： lab0.5/Makefile，解释操作系统镜像文件ucore.img是如何一步一步生成的？ (需要比较详细地解释Makefile中每一条相关命令和命令参数的含义，以及说明命令导致的结果) 阅读分析lab0.5/tools/kernel.ld 链接脚本，给出其每行含义。 一个被系统认为是符合规范的硬盘主引导扇区的特征是什么？ [!TIP|style:flat] 查看linkscript、装载位置(base address)和对其地址(align)） 练习2： 分析OpenSBI加载bin格式的OS的过程 OpenSBI如何读取硬盘扇区的？ OpenSBI是如何加载bin格式的OS？ Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0.5/nei-cun-bu-ju.html":{"url":"lab0.5/nei-cun-bu-ju.html","title":"内存布局","keywords":"","body":"内存布局 计算机组成 首先我们回顾计算机的组成: CPU, 存储设备（粗略地说，包括断电后遗失的内存，和断电后不遗失的硬盘），输入输出设备，总线。 现在我们手里的东西有：QEMU会帮助我们模拟一块riscv64的CPU，一块物理内存，还会借助你的电脑的键盘和显示屏来模拟命令行的输入和输出。虽然QEMU不会真正模拟一堆线缆，但是总线的通信功能也在QEMU内部实现了。 还差什么呢？硬盘。 OpenSBI 我们需要硬盘上的程序和数据。比如崭新的windows电脑里C盘已经被占据的二三十GB空间，除去预装的应用软件，还有一部分是windows操作系统的内核。在插上电源开机之后，就需要运行操作系统的内核，然后由操作系统来管理计算机。 问题在于，操作系统作为一个程序，必须加载到内存里才能执行。而“把操作系统加载到内存里”这件事情，不是操作系统自己能做到的，就好像你不能拽着头发把自己拽离地面。 因此我们可以想象，在操作系统执行之前，必然有一个其他程序执行，他作为“先锋队”，完成“把操作系统加载到内存“这个工作，然后他功成身退，把CPU的控制权交给操作系统。 这个“其他程序”，我们一般称之为bootloader. 很好理解：他负责boot(开机)，还负责load(加载OS到内存里)，所以叫bootloader. 在QEMU模拟的riscv计算机里，我们使用QEMU自带的bootloader: OpenSBI固件。 [!TIP|style:flat|label:知识点] 在计算机中，固件(firmware)是一种特定的计算机软件，它为设备的特定硬件提供低级控制，也可以进一步加载其他软件。固件可以为设备更复杂的软件（如操作系统）提供标准化的操作环境。对于不太复杂的设备，固件可以直接充当设备的完整操作系统，执行所有控制、监视和数据操作功能。 在基于 x86 的计算机系统中, BIOS 或 UEFI 是固件；在基于 riscv 的计算机系统中，OpenSBI 是固件。OpenSBI运行在M态（M-mode），因为固件需要直接访问硬件。 RISCV有四种特权级（privilege level）。 Level Encoding 全称 简称 0 00 User/Application U 1 01 Supervisor S 2 10 Reserved(目前未使用，保留) 3 11 Machine M 粗略的分类： U-mode是用户程序、应用程序的特权级，S-mode是操作系统内核的特权级，M-mode是固件的特权级。 详细内容请自行查阅RISC-v手册。 elf与bin 我们可以想象这样的过程：操作系统的二进制可执行文件被OpenSBI加载到内存中，然后OpenSBI会把CPU的\"当前指令指针\"(pc, program counter)跳转到内存里的一个位置，开始执行内存中那个位置的指令。 OpenSBI怎样知道把操作系统加载到内存的什么位置？总不能随便选个位置。也许你会觉得可以把操作系统的代码总是加载到固定的位置，比如总是加载到内存地址最高的地方。 问题在于，之后OpenSBI还要把CPU的program counter跳转到一个位置,开始操作系统的执行。如果加载操作系统到内存里的时候随便加载，那么OpenSBI怎么知道把program counter跳转到哪里去呢？难道操作系统的二进制可执行文件需要提供“program counter跳转到哪里\"这样的信息？ 实际上，操作系统的二进制可执行文件，会指定它自己应该被加载到内存的哪个地址。而OpenSBI会很听话地把二进制可执行文件放到她想去的位置上。但是关于program counter的跳转，OpenSBI是独断专行的，总是会把program counter跳到0x80200000这个内存地址开始执行, 所以故事(版本1)其实是这样的： OpenSBI: 操作系统， 你到0x8020000等着program counter跳过来执行！ 操作系统：好的！请把我加载到xxxxxx这个位置，这样program counter跳过来的时候就不会出问题了。 实际上，二进制程序加载到内存中是一件很精细的工作。一个二进制程序包括很多section, 如text(程序代码)，bss(需要初始化为零的数据)，rodata(只读数据)。二进制程序的每个section都可以指定一个希望被加载到的内存地址。 故事可以是这样的吗？（版本2） OpenSBI: 操作系统， 你到0x8020000等着program counter跳过来执行！ 操作系统：好的！请把我的text section加载到A位置，data section加载到B位置，rodata section加载到C位置......这样program counter跳过来的时候就不会出问题了！ OpenSBI: 你说啥？ 两个版本的故事是因为，我们有两种不同的可执行文件格式：elf(e是executable的意思， l是linkable的意思，f是format的意思)和bin(binary)。 elf文件(wikipedia: elf)比较复杂，包含一个文件头(ELF header), 包含冗余的调试信息，指定程序每个section的内存布局，需要解析program header才能知道各段(section)的信息。如果我们已经有一个完整的操作系统来解析elf文件，那么elf文件可以直接执行。但是对于OpenSBI来说，elf格式还是太复杂了，把操作系统内核的elf文件交给OpenSBI就会发生版本2的悲惨故事。 bin文件就比较简单了，简单地在文件头之后解释自己应该被加载到什么起始位置。OpenSBI可以理解得很清楚，这就是版本1的故事。 我们举一个例子解释elf和bin文件的区别：初始化为零的一个大数组，在elf文件里是bss数据段的一部分，只需要记住这个数组的起点和终点就可以了，等到加载到内存里的时候分配那一段内存。但是在bin文件里，那个数组有多大，有多少个字节的0，bin文件就要对应有多少个零。所以如果一个程序里声明了一个大全局数组（默认初始化为0），那么可能编译出来的elf文件只有几KB, 而生成bin文件之后却有几MB, 这是很正常的。实际上，可以认为bin文件会把elf文件指定的每段的内存布局都映射到一块线性的数据里，这块线性的数据（或者说程序）加载到内存里就符合elf文件之前指定的布局。 那么我们的任务就明确了：得到内存布局合适的elf文件，然后把它转化成bin文件（这一步通过objcopy实现），然后加载到QEMU里运行（QEMU自带的OpenSBI会干这个活）。下面我们来看如何设置elf文件的内存布局。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0.5/lian-jie-jiao-ben.html":{"url":"lab0.5/lian-jie-jiao-ben.html","title":"链接脚本","keywords":"","body":"链接脚本 gnu工具链中，包含一个链接器ld 如果你很好奇，可以看linker script的详细语法 链接器的作用是把输入文件(往往是 .o文件)链接成输出文件(往往是elf文件)。一般来说，输入文件和输出文件都有很多section, 链接脚本(linker script)的作用，就是描述怎样把输入文件的section映射到输出文件的section, 同时规定这些section的内存布局。 如果你不提供链接脚本，ld会使用默认的一个链接脚本，这个默认的链接脚本适合链接出一个能在现有操作系统下运行的应用程序，但是并不适合链接一个操作系统内核。你可以通过ld --verbose来查看默认的链接脚本。 [!NOTE|style:flat] 要打开代码看噢，这里没有哒！ 我们在链接脚本里把程序的入口点定义为kern_entry, 那么我们的程序里需要有一个名称为kern_entry的符号。我们在kern/init/entry.S编写了一段汇编代码, 作为整个内核的入口点。 #include #include .section .text,\"ax\",%progbits .globl kern_entry kern_entry: la sp, bootstacktop tail kern_init #调用kern_init, 这是我们要用C语言编写的一个函数, tail是riscv伪指令，作用相当于调用函数（跳转） .section .data # .align 2^12 .align PGSHIFT .global bootstack bootstack: .space KSTACKSIZE .global bootstacktop bootstacktop: 里面有很多符号和指令，定义的符号，应该知道去哪找吧（偷偷告诉你吧，在头文件呀！）。关于指令呢，就需要自己去查阅手册咯。如果看不懂汇编代码结构的，自己查资料噢~不要怀着疑惑往下做。会越来越懵。 诶诶，看到这，下一步该干嘛，该去找哪个呢，想一想噢。 哎鸭！调用了唯一的一个函数，当然是去找函数啦。这个函数在干嘛呢？不好奇么。那就往下看。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0.5/zhen-zheng-de-ru-kou-dian.html":{"url":"lab0.5/zhen-zheng-de-ru-kou-dian.html","title":"真正的入口点","keywords":"","body":"真正的入口点 我们在kern/init/init.c编写函数kern_init, 作为“真正的”内核入口点。为了让我们能看到一些效果，我们希望它能在命令行进行格式化输出。 如果我们在linux下运行一个C程序，需要格式化输出，那么大一学生都知道我们应该#include。于是我们在kern/init/init.c也这么写一句。且慢！linux下，当我们调用C语言标准库的函数时，实际上依赖于glibc提供的运行时环境，也就是一定程度上依赖于操作系统提供的支持。可是我们并没有把glibc移植到ucore里！ 怎么办呢？只能自己动手，丰衣足食。QEMU里的OpenSBI固件提供了输入一个字符和输出一个字符的接口，我们一会把这个接口一层层封装起来，提供stdio.h里的格式化输出函数cprintf()来使用。这里格式化输出函数的名字不使用原先的printf()，强调这是我们在ucore里重新实现的函数。 接下来就去看看，我们是怎么从OpenSBI的接口一层层封装到格式化输入输出函数的。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0.5/cong-sbi-dao-stdio.html":{"url":"lab0.5/cong-sbi-dao-stdio.html","title":"从SBI到stdio","keywords":"","body":"从SBI到stdio OpenSBI作为运行在M态的软件（或者说固件）, 提供了一些接口供我们编写内核的时候使用。 我们可以通过ecall指令(environment call)调用OpenSBI。通过寄存器传递给OpenSBI一个”调用编号“，如果编号在 0-8 之间，则由OpenSBI进行处理，否则交由我们自己的中断处理程序处理（暂未实现）。有时OpenSBI调用需要像函数调用一样传递参数，这里传递参数的方式也和函数调用一样，按照riscv的函数调用约定(calling convention)把参数放到寄存器里。可以阅读SBI的详细文档。 [!TIP|style:flat|label:知识点] ecall(environment call)，当我们在 S 态执行这条指令时，会触发一个 ecall-from-s-mode-exception，从而进入 M 模式中的中断处理流程（如设置定时器等）；当我们在 U 态执行这条指令时，会触发一个 ecall-from-u-mode-exception，从而进入 S 模式中的中断处理流程（常用来进行系统调用）。 关于这个，大三的时候会被好好折磨的噢【坏笑】。 C语言并不能直接调用ecall, 需要通过内联汇编来实现。 // libs/sbi.c #include #include //SBI编号和函数的对应 uint64_t SBI_SET_TIMER = 0; uint64_t SBI_CONSOLE_PUTCHAR = 1; uint64_t SBI_CONSOLE_GETCHAR = 2; uint64_t SBI_CLEAR_IPI = 3; uint64_t SBI_SEND_IPI = 4; uint64_t SBI_REMOTE_FENCE_I = 5; uint64_t SBI_REMOTE_SFENCE_VMA = 6; uint64_t SBI_REMOTE_SFENCE_VMA_ASID = 7; uint64_t SBI_SHUTDOWN = 8; //sbi_call函数是我们关注的核心 uint64_t sbi_call(uint64_t sbi_type, uint64_t arg0, uint64_t arg1, uint64_t arg2) { uint64_t ret_val; __asm__ volatile ( \"mv x17, %[sbi_type]\\n\" \"mv x10, %[arg0]\\n\" \"mv x11, %[arg1]\\n\" \"mv x12, %[arg2]\\n\" //mv操作把参数的数值放到寄存器里 \"ecall\\n\" //参数放好之后，通过ecall, 交给OpenSBI来执行 \"mv %[ret_val], x10\" //OpenSBI按照riscv的calling convention,把返回值放到x10寄存器里 //我们还需要自己通过内联汇编把返回值拿到我们的变量里 : [ret_val] \"=r\" (ret_val) : [sbi_type] \"r\" (sbi_type), [arg0] \"r\" (arg0), [arg1] \"r\" (arg1), [arg2] \"r\" (arg2) : \"memory\" ); return ret_val; } void sbi_console_putchar(unsigned char ch) { sbi_call(SBI_CONSOLE_PUTCHAR, ch, 0, 0); //注意这里ch隐式类型转换为int64_t } void sbi_set_timer(unsigned long long stime_value) { sbi_call(SBI_SET_TIMER, stime_value, 0, 0); } [!TIP|style:flat|label:知识点] 函数调用与calling convention 我们知道，编译器将高级语言源代码翻译成汇编代码。对于汇编语言而言，在最简单的编程模型中，所能够利用的只有指令集中提供的指令、各通用寄存器、 CPU 的状态、内存资源。那么，在高级语言中，我们进行一次函数调用，编译器要做哪些工作利用汇编语言来实现这一功能呢？ 显然并不是仅用一条指令跳转到被调用函数开头地址就行了。我们还需要考虑： 如何传递参数？ 如何传递返回值？ 如何保证函数返回后能从我们期望的位置继续执行？ 等更多事项。通常编译器按照某种规范去翻译所有的函数调用，这种规范被称为 calling convention 。值得一提的是，为了实现函数调用，我们需要预先分配一块内存作为 调用栈 ，后面会看到调用栈在函数调用过程中极其重要。你也可以理解为什么第一章刚开始我们就要分配栈了。 可以参考riscv calling convention 现在可以输出一个字符了，有了第一个，就会有第二个第三个……第无数个。 这样我们就可以通过sbi_console_putchar()来输出一个字符。接下来我们要做的事情就像月饼包装，把它封了一层又一层。 console.c只是简单地封装一下 // kern/driver/console.c#include #include void cons_putc(int c) { sbi_console_putchar((unsigned char)c); } stdio.c里面实现了一些函数，注意我们已经实现了ucore版本的puts函数: cputs() // kern/libs/stdio.c #include #include #include /* HIGH level console I/O */ /* * * cputch - writes a single character @c to stdout, and it will * increace the value of counter pointed by @cnt. * */ static void cputch(int c, int *cnt) { cons_putc(c); (*cnt)++; } /* cputchar - writes a single character to stdout */ void cputchar(int c) { cons_putc(c); } int cputs(const char *str) { int cnt = 0; char c; while ((c = *str++) != '\\0') { cputch(c, &cnt); } cputch('\\n', &cnt); return cnt; } 我们还在libs/printfmt.c实现了一些复杂的格式化输入输出函数。最后得到的cprintf()函数仍在kern/libs/stdio.c定义，功能和C标准库的printf()基本相同。 可能你注意到我们用到一个头文件defs.h, 我们在里面定义了一些有用的宏和类型 // libs/defs.h #ifndef __LIBS_DEFS_H__ #define __LIBS_DEFS_H__ ... /* Represents true-or-false values */ typedef int bool; /* Explicitly-sized versions of integer types */ typedef char int8_t; typedef unsigned char uint8_t; typedef short int16_t; typedef unsigned short uint16_t; typedef int int32_t; typedef unsigned int uint32_t; typedef long long int64_t; typedef unsigned long long uint64_t; ... /* * * Rounding operations (efficient when n is a power of 2) * Round down to the nearest multiple of n * */ #define ROUNDDOWN(a, n) ({ \\ size_t __a = (size_t)(a); \\ (typeof(a))(__a - __a % (n)); \\ }) ... #endif printfmt.c还依赖一个头文件riscv.h,这个头文件主要定义了若干和riscv架构相关的宏，尤其是将一些内联汇编的代码封装成宏，使得我们更方便地使用内联汇编来读写寄存器。当然这里我们还没有用到它的强大功能。 // libs/riscv.h ... #define read_csr(reg) ({ unsigned long __tmp; \\ asm volatile (\"csrr %0, \" #reg : \"=r\"(__tmp)); \\ __tmp; }) //通过内联汇编包装了 csrr 指令为 read_csr() 宏 #define write_csr(reg, val) ({ \\ if (__builtin_constant_p(val) && (unsigned long)(val) 到现在，我们已经看过了一个最小化的内核的各个部分，虽然一些部分没有逐行细读，但我们也知道它在做什么。 是不是感觉好麻烦啊！输出一个字符都那么麻烦。那是肯定的噢，可以稍微喘下气，脑子里回忆一下，我们是怎么一层一层剥开，又是如何一层一层包装的。好玩吧！ 但一直到现在我们还没进行过编译。下面就把它编译一下跑起来。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0.5/bian-yi-yun-hang.html":{"url":"lab0.5/bian-yi-yun-hang.html","title":"编译运行","keywords":"","body":"编译运行 我们需要：编译所有的源代码，把目标文件链接起来，生成elf文件，生成bin硬盘镜像，用qemu跑起来 这一系列复杂的命令，我们不想每次用到的时候都敲一遍，所以我们使用魔改的祖传Makefile。 我们的Makefile还依赖tools/function.mk 在源代码的根目录下make qemu, 我们就把ucore跑起来了。 它输出一行(THU.CST) os is loading, 然后进入死循环。 关于Makefile的语法, 如果不熟悉, 可以回看LAB0的前导知识。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab0.5/xiang-mu-zu-cheng-yu-zhi-hang-liu.html":{"url":"lab0.5/xiang-mu-zu-cheng-yu-zhi-hang-liu.html","title":"项目组成与执行流","keywords":"","body":"项目组成与执行流 lab0的项目组成: ── Makefile ├── kern │ ├── debug │ │ ├── assert.h │ │ ├── kdebug.c │ │ ├── kdebug.h │ │ ├── kmonitor.c │ │ ├── kmonitor.h │ │ ├── panic.c │ │ └── stab.h │ ├── driver │ │ ├── clock.c │ │ ├── clock.h │ │ ├── console.c │ │ ├── console.h │ │ ├── intr.c │ │ ├── intr.h │ │ ├── kbdreg.h │ │ ├── picirq.c │ │ └── picirq.h │ ├── init │ │ ├── entry.S │ │ └── init.c │ ├── libs │ │ ├── readline.c │ │ └── stdio.c │ ├── mm │ │ ├── memlayout.h │ │ ├── mmu.h │ │ ├── pmm.c │ │ └── pmm.h │ └── trap │ ├── trap.c │ ├── trap.h │ └── trapentry.S ├── libs │ ├── defs.h │ ├── elf.h │ ├── error.h │ ├── printfmt.c │ ├── riscv.h │ ├── sbi.c │ ├── sbi.h │ ├── stdarg.h │ ├── stdio.h │ ├── string.c │ └── string.h └── tools ├── function.mk ├── kernel.ld 内核启动 kern/init/entry.S: OpenSBI启动之后将要跳转到的一段汇编代码。在这里进行内核栈的分配，然后转入C语言编写的内核初始化函数。 kern/init/init.c： C语言编写的内核入口点。主要包含kern_init()函数，从kern/entry.S跳转过来完成其他初始化工作。 设备驱动 kern/driver/console.c(h): 在QEMU上模拟的时候，唯一的“设备”是虚拟的控制台，通过OpenSBI接口使用。简单封装了OpenSBI的字符读写接口，向上提供给输入输出库。 库文件 libs/riscv.h: 以宏的方式，定义了riscv指令集的寄存器和指令。如果在C语言里使用riscv指令，需要通过内联汇编和寄存器的编号。这个头文件把寄存器编号和内联汇编都封装成宏，使得我们可以用类似函数的方式在C语言里执行一句riscv指令。 libs/sbi.c(h): 封装OpenSBI接口为函数。如果想在C语言里使用OpenSBI提供的接口，需要使用内联汇编。这个头文件把OpenSBI的内联汇编调用封装为函数。 libs/defs.h: 定义了一些常用的类型和宏。 例如bool 类型（C语言不自带，这里typedef int bool)。 libs/string.c(h): 一些对字符数组进行操作的函数，如memset(),memcpy()等，类似C语言的string.h。 kern/libs/stdio.c, libs/readline.c, libs/printfmt.c: 实现了一套标准输入输出，功能类似于C语言的printf() 和getchar()。需要内核为输入输出函数提供两个桩函数（stub): 输出一个字符的函数，输入一个字符的函数。在这里，是cons_getc()和cons_putc()。 kern/errors.h: 定义了一些内核错误类型的宏。 编译、链接脚本 tools/kernel.ld: ucore的链接脚本(link script), 告诉链接器如何将目标文件的section组合为可执行文件。 tools/function.mk: 定义Makefile中使用的一些函数 Makefile: GNU make编译脚本 执行流 最小可执行内核的执行流为: 加电 -> OpenSBI启动 -> 跳转到 0x80200000 (kern/init/entry.S）->进入kern_init()函数（kern/init/init.c) ->调用cprintf()输出一行信息->结束 cprintf()函数的执行流为: 接受一个格式化字符串和若干个需要输出的变量作为参数 -> 解析格式化的字符串，把需要输出的各种变量转化为一串字符 -> 调用console.c提供的字符输出接口依次输出所有字符（实际上console.c又封装了sbi.c向上提供的OpenSBI接口) Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab1/":{"url":"lab1/","title":"LAB1：中断机制","keywords":"","body":"LAB1：中断机制 中断（interrupt）机制，就是不管CPU现在手里在干啥活，收到“中断”的时候，都先放下来去处理其他事情，处理完其他事情可能再回来干手头的活。 例如，CPU要向磁盘发一个读取数据的请求，由于磁盘速度相对CPU较慢，在“发出请求”到“收到磁盘数据\"之间会经过很多时间周期，如果CPU干等着磁盘干活就相当于CPU在磨洋工。因此我们可以让CPU发出读数据的请求后立刻开始干另一件事情。但是，等一段时间之后，磁盘的数据取到了，而CPU在干其他的事情，我们怎么办才能让CPU知道之前发出的磁盘请求已经完成了呢？我们可以让磁盘给CPU一个“中断”，让CPU放下手里的事情来接受磁盘的数据。 再比如，为了保证CPU正在执行的程序不会永远运行下去，我们需要定时检查一下它是否已经运行“超时”。想象有一个程序由于bug进入了死循环，如果CPU一直运行这个程序，那么其他的所有程序都会因为等待CPU资源而无法运行，造成严重的资源浪费。但是检查是否超时，需要CPU执行一段代码，也就是让CPU暂停当前执行的程序。我们不能假设当前执行的程序会主动地定时让出CPU，那么就需要CPU定时“打断”当前程序的执行，去进行一些处理，这通过时钟中断来实现。 从这些描述我们可以看出，中断机制需要软件硬件一起来支持。硬件进行中断和异常的发现，然后交给软件来进行处理。回忆一下组成原理课程中学到的各个控制寄存器以及他们的用途（下一小节会进行简单回顾），这些寄存器构成了重要的硬件/软件接口。由此，我们也可以得到在一般OS中进行中断处理支持的方法： 编写相应的中断处理代码 在启动中正确设置控制寄存器 CPU捕获异常 控制转交给相应中断处理代码进行处理 返回正在运行的程序 由于中断处理需要进行较高权限的操作，中断处理程序一般处于内核态，或者说，处于“比被打断的程序更高的特权级”。注意，在RISCV里，中断(interrupt)和异常(exception)统称为\"trap\"。 这次实验就一起来看一下ucore是如何支持中断处理的。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab1/shi-yan-mu-de.html":{"url":"lab1/shi-yan-mu-de.html","title":"实验目的","keywords":"","body":"实验目的 了解CPU的中断机制 了解RISC-v架构是如何支持CPU中断的 掌握与软件相关的中断处理 掌握时钟中断管理 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab1/shi-yan-nei-rong.html":{"url":"lab1/shi-yan-nei-rong.html","title":"实验内容","keywords":"","body":"实验内容 跟着实验指导书理解lab1框架代码。 阅读RISC-V手册有关中断部分。 完成练习。 撰写并提交实验报告。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab1/lian-xi.html":{"url":"lab1/lian-xi.html","title":"练习","keywords":"","body":"练习 练习1：描述处理中断异常的流程 像LAB0.5里的执行流一样描述ucore是如何处理中断异常的。从异常的产生开始。 练习2：对于任何中断，都需要保存所有寄存器吗？为什么？ 练习3：触发、捕获、处理异常 编程：在任意位置触发一条非法指令异常（如：mret），在 kern/trap/trap.c的异常处理函数中捕获，并对其进行处理，简单输出异常类型和指令即可。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab1/riscv-zhong-duan-xiang-guan.html":{"url":"lab1/riscv-zhong-duan-xiang-guan.html","title":"RISC-V中断相关","keywords":"","body":"RISC-V中断相关 寄存器 除了32个通用寄存器之外，RISCV架构还有大量的 控制状态寄存器 Control and Status Registers(CSRs)。其中有几个重要的寄存器和中断机制有关。 有些时候，禁止CPU产生中断很有用。（就像你在做重要的事情，如操作系统lab的时候，并不想被打断）。所以，sstatus寄存器(Supervisor Status Register)里面有一个二进制位SIE(supervisor interrupt enable，在RISCV标准里是2^1 对应的二进制位)，数值为0的时候，如果当程序在S态运行，将禁用全部中断。（对于在U态运行的程序，SIE这个二进制位的数值没有任何意义），sstatus还有一个二进制位UIE(user interrupt enable)可以在置零的时候禁止用户态程序产生中断。 在中断产生后，应该有个中断处理程序来处理中断。CPU怎么知道中断处理程序在哪？实际上，RISCV架构有个CSR叫做stvec(Supervisor Trap Vector Base Address Register)，即所谓的”中断向量表基址”。中断向量表的作用就是把不同种类的中断映射到对应的中断处理程序。如果只有一个中断处理程序，那么可以让stvec直接指向那个中断处理程序的地址。 对于RISCV架构，stvec会把最低位的两个二进制位用来编码一个“模式”，如果是“00”就说明更高的SXLEN-2个二进制位存储的是唯一的中断处理程序的地址(SXLEN是stval寄存器的位数)，如果是“01”说明更高的SXLEN-2个二进制位存储的是中断向量表基址，通过不同的异常原因来索引中断向量表。但是怎样用62个二进制位编码一个64位的地址？RISCV架构要求这个地址是四字节对齐的，总是在较高的62位后补两个0。 [!NOTE|style:flat] 手册P110 机器和监管者自陷向量（trap-vector）基地址寄存器（ mtvec和 stvec) CSR。他们是位宽为 XLEN的读 /写寄存器，用于保存自陷向量的配置，包括向量基址（ BASE）和向量模式 （MODE）。 BASE域中的值必须按 4字节对齐。 MODE = 0表示所有异常都把 PC设置为 BASE。 MODE = 1会在一部中断时将 PC设置为 (𝑩𝑨𝑺𝑬+(𝟒×𝒄𝒂𝒖𝒔𝒆))。 当我们触发中断进入 S 态进行处理时，以下寄存器会被硬件自动设置，将一些信息提供给中断处理程序： sepc(supervisor exception program counter)，它会记录触发中断的那条指令的地址； scause，它会记录中断发生的原因，还会记录该中断是不是一个外部中断； stval，它会记录一些中断处理所需要的辅助信息，比如指令获取(instruction fetch)、访存、缺页异常，它会把发生问题的目标地址或者出错的指令记录下来，这样我们在中断处理程序中就知道处理目标了。 特权指令 RISCV支持以下和中断相关的特权指令： ecall(environment call)，当我们在 S 态执行这条指令时，会触发一个 ecall-from-s-mode-exception，从而进入 M 模式中的中断处理流程（如设置定时器等）；当我们在 U 态执行这条指令时，会触发一个 ecall-from-u-mode-exception，从而进入 S 模式中的中断处理流程（常用来进行系统调用）。 sret，用于 S 态中断返回到 U 态，实际作用为pc←sepc，回顾sepc定义，返回到通过中断进入 S 态之前的地址。 ebreak(environment break)，执行这条指令会触发一个断点中断从而进入中断处理流程。 mret，用于 M 态中断返回到 S 态或 U 态，实际作用为pc←mepc，回顾sepc定义，返回到通过中断进入 M 态之前的地址。（一般不用涉及） [!TIP|style:flat] 关于上面提及的内容，要去手册里找相关内容，然后看明白！ Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab1/shang-xia-wen-chu-li.html":{"url":"lab1/shang-xia-wen-chu-li.html","title":"上下文处理","keywords":"","body":"上下文处理 我们已经知道,在发生中断的时候, CPU会跳到stvec.我们准备采用Direct模式,也就是只有一个中断处理程序, stvec直接跳到中断处理程序的入口点,那么需要我们对stvec寄存器做初始化. 上下文 中断的处理需要“放下当前的事情但之后还能回来接着之前往下做”，对于CPU来说，实际上只需要把原先的寄存器保存下来，做完其他事情把寄存器恢复回来就可以了。这些寄存器也被叫做CPU的context(上下文，情境)。 我们要用汇编实现上下文切换(context switch)机制，这包含两步： 保存CPU的寄存器（上下文）到内存中（栈上） 从内存中（栈上）恢复CPU的寄存器 [!NOTE|style:flat] 通用寄存器的介绍见中文手册42页。 为了方便我们组织上下文的数据（几十个寄存器），我们定义一个结构体。 // kern/trap/trap.h #ifndef __KERN_TRAP_TRAP_H__ #define __KERN_TRAP_TRAP_H__ #include struct pushregs { uintptr_t zero; // Hard-wired zero uintptr_t ra; // Return address uintptr_t sp; // Stack pointer uintptr_t gp; // Global pointer uintptr_t tp; // Thread pointer uintptr_t t0; // Temporary uintptr_t t1; // Temporary uintptr_t t2; // Temporary uintptr_t s0; // Saved register/frame pointer uintptr_t s1; // Saved register uintptr_t a0; // Function argument/return value uintptr_t a1; // Function argument/return value uintptr_t a2; // Function argument uintptr_t a3; // Function argument uintptr_t a4; // Function argument uintptr_t a5; // Function argument uintptr_t a6; // Function argument uintptr_t a7; // Function argument uintptr_t s2; // Saved register uintptr_t s3; // Saved register uintptr_t s4; // Saved register uintptr_t s5; // Saved register uintptr_t s6; // Saved register uintptr_t s7; // Saved register uintptr_t s8; // Saved register uintptr_t s9; // Saved register uintptr_t s10; // Saved register uintptr_t s11; // Saved register uintptr_t t3; // Temporary uintptr_t t4; // Temporary uintptr_t t5; // Temporary uintptr_t t6; // Temporary }; struct trapframe { struct pushregs gpr; uintptr_t status; //sstatus uintptr_t epc; //sepc uintptr_t badvaddr; //sbadvaddr uintptr_t cause; //scause }; void trap(struct trapframe *tf); C语言里面的结构体，是若干个变量在内存里直线排列。也就是说，一个trapFrame结构体占据36个uintptr_t的空间（在64位RISCV架构里我们定义uintptr_t为64位无符号整数），里面依次排列通用寄存器x0到x31,然后依次排列4个和中断相关的CSR, 我们希望中断处理程序能够利用这几个CSR的数值。 保存上下文 我们在理论课上也学到了保存上下文是用汇编语言实现的。首先我们定义一个汇编宏 SAVE_ALL, 用来保存所有寄存器到栈顶（实际上把一个trapFrame结构体放到了栈顶）。 # kern/trap/trapentry.S #include .macro SAVE_ALL #定义汇编宏 csrw sscratch, sp #保存原先的栈顶指针到sscratch addi sp, sp, -36 * REGBYTES #REGBYTES是riscv.h定义的常量，表示一个寄存器占据几个字节 #让栈顶指针向低地址空间延伸 36个寄存器的空间，可以放下一个trapFrame结构体。 #除了32个通用寄存器，我们还要保存4个和中断有关的CSR #依次保存32个通用寄存器。但栈顶指针需要特殊处理。 #因为我们想在trapFrame里保存分配36个REGBYTES之前的sp #也就是保存之前写到sscratch里的sp的值 STORE x0, 0*REGBYTES(sp) STORE x1, 1*REGBYTES(sp) STORE x3, 3*REGBYTES(sp) STORE x4, 4*REGBYTES(sp) STORE x5, 5*REGBYTES(sp) STORE x6, 6*REGBYTES(sp) STORE x7, 7*REGBYTES(sp) STORE x8, 8*REGBYTES(sp) STORE x9, 9*REGBYTES(sp) STORE x10, 10*REGBYTES(sp) STORE x11, 11*REGBYTES(sp) STORE x12, 12*REGBYTES(sp) STORE x13, 13*REGBYTES(sp) STORE x14, 14*REGBYTES(sp) STORE x15, 15*REGBYTES(sp) STORE x16, 16*REGBYTES(sp) STORE x17, 17*REGBYTES(sp) STORE x18, 18*REGBYTES(sp) STORE x19, 19*REGBYTES(sp) STORE x20, 20*REGBYTES(sp) STORE x21, 21*REGBYTES(sp) STORE x22, 22*REGBYTES(sp) STORE x23, 23*REGBYTES(sp) STORE x24, 24*REGBYTES(sp) STORE x25, 25*REGBYTES(sp) STORE x26, 26*REGBYTES(sp) STORE x27, 27*REGBYTES(sp) STORE x28, 28*REGBYTES(sp) STORE x29, 29*REGBYTES(sp) STORE x30, 30*REGBYTES(sp) STORE x31, 31*REGBYTES(sp) # RISCV不能直接从CSR写到内存, 需要csrr把CSR读取到通用寄存器，再从通用寄存器STORE到内存 csrrw s0, sscratch, x0 csrr s1, sstatus csrr s2, sepc csrr s3, sbadaddr csrr s4, scause STORE s0, 2*REGBYTES(sp) STORE s1, 32*REGBYTES(sp) STORE s2, 33*REGBYTES(sp) STORE s3, 34*REGBYTES(sp) STORE s4, 35*REGBYTES(sp) .endm #汇编宏定义结束 恢复上下文 然后是恢复上下文的汇编宏，恢复的顺序和当时保存的顺序反过来，先加载两个CSR, 再加载通用寄存器。 # kern/trap/trapentry.S .macro RESTORE_ALL LOAD s1, 32*REGBYTES(sp) LOAD s2, 33*REGBYTES(sp) # 注意之前保存的几个CSR并不都需要恢复 csrw sstatus, s1 csrw sepc, s2 # 恢复sp之外的通用寄存器，这时候还需要根据sp来确定其他寄存器数值保存的位置 LOAD x1, 1*REGBYTES(sp) LOAD x3, 3*REGBYTES(sp) LOAD x4, 4*REGBYTES(sp) LOAD x5, 5*REGBYTES(sp) LOAD x6, 6*REGBYTES(sp) LOAD x7, 7*REGBYTES(sp) LOAD x8, 8*REGBYTES(sp) LOAD x9, 9*REGBYTES(sp) LOAD x10, 10*REGBYTES(sp) LOAD x11, 11*REGBYTES(sp) LOAD x12, 12*REGBYTES(sp) LOAD x13, 13*REGBYTES(sp) LOAD x14, 14*REGBYTES(sp) LOAD x15, 15*REGBYTES(sp) LOAD x16, 16*REGBYTES(sp) LOAD x17, 17*REGBYTES(sp) LOAD x18, 18*REGBYTES(sp) LOAD x19, 19*REGBYTES(sp) LOAD x20, 20*REGBYTES(sp) LOAD x21, 21*REGBYTES(sp) LOAD x22, 22*REGBYTES(sp) LOAD x23, 23*REGBYTES(sp) LOAD x24, 24*REGBYTES(sp) LOAD x25, 25*REGBYTES(sp) LOAD x26, 26*REGBYTES(sp) LOAD x27, 27*REGBYTES(sp) LOAD x28, 28*REGBYTES(sp) LOAD x29, 29*REGBYTES(sp) LOAD x30, 30*REGBYTES(sp) LOAD x31, 31*REGBYTES(sp) # 最后恢复sp LOAD x2, 2*REGBYTES(sp) .endm 中断入口 真正的入口点就是去调用这两个宏定义 .globl __alltraps .align(2) #中断入口点 __alltraps必须四字节对齐 __alltraps: SAVE_ALL #保存上下文 move a0, sp #传递参数。 #按照RISCV calling convention, a0寄存器传递参数给接下来调用的函数trap。 #trap是trap.c里面的一个C语言函数，也就是我们的中断处理程序 jal trap #trap函数指向完之后，会回到这里向下继续执行__trapret里面的内容，RESTORE_ALL,sret .globl __trapret __trapret: RESTORE_ALL # return from supervisor call sret 我们可以看到，trapentry.S这个中断入口点的作用是保存和恢复上下文，并把上下文包装成结构体送到trap函数那里去。下面我们就去看看trap函数里面做些什么。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab1/zhong-duan-chu-li-cheng-xu.html":{"url":"lab1/zhong-duan-chu-li-cheng-xu.html","title":"中断处理程序","keywords":"","body":"中断处理程序 scause 当处理自陷时， cause CSR中被写入一个指示导致 自陷的事件的代码。如果自陷由中断引起，则置上中断位。“异常代码”字段包含指示最后一个异常的代 码。具体的中断/异常映射关系，见中文手册100页。 初始化 中断处理需要初始化，所以我们在init.c里调用一些初始化的函数 // kern/init/init.c #include int kern_init(void) { extern char edata[], end[]; memset(edata, 0, end - edata); cons_init(); // init the console const char *message = \"(THU.CST) os is loading ...\\n\"; cprintf(\"%s\\n\\n\", message); print_kerninfo(); // grade_backtrace(); //trap.h的函数，初始化中断 idt_init(); // init interrupt descriptor table //clock.h的函数，初始化时钟中断 clock_init(); //intr.h的函数，使能中断 intr_enable(); // LAB1: CAHLLENGE 1 If you try to do it, uncomment lab1_switch_test() // user/kernel mode switch test // lab1_switch_test(); /* do nothing */ while (1) ; } // kern/trap/trap.c void idt_init(void) { extern void __alltraps(void); //约定：若中断前处于S态，sscratch为0 //若中断前处于U态，sscratch存储内核栈地址 //那么之后就可以通过sscratch的数值判断是内核态产生的中断还是用户态产生的中断 //我们现在是内核态所以给sscratch置零 write_csr(sscratch, 0); //我们保证__alltraps的地址是四字节对齐的，将__alltraps这个符号的地址直接写到stvec寄存器 write_csr(stvec, &__alltraps); } //kern/driver/intr.c #include #include /* intr_enable - enable irq interrupt, 设置sstatus的Supervisor中断使能位 */ void intr_enable(void) { set_csr(sstatus, SSTATUS_SIE); } /* intr_disable - disable irq interrupt */ void intr_disable(void) { clear_csr(sstatus, SSTATUS_SIE); } 处理 trap.c的中断处理函数trap, 实际上把中断处理,异常处理的工作分发给了interrupt_handler()，exception_handler(), 这些函数再根据中断或异常的不同类型来处理。 // kern/trap/trap.c /* trap_dispatch - dispatch based on what type of trap occurred */ static inline void trap_dispatch(struct trapframe *tf) { //scause的最高位是1，说明trap是由中断引起的 if ((intptr_t)tf->cause interrupt_handler()和exception_handler()的实现还比较简单，只是简单地根据scause的数值更仔细地分了下类，做了一些输出就直接返回了。switch里的各种case, 如IRQ_U_SOFT,CAUSE_USER_ECALL,是riscv ISA 标准里规定的。我们在riscv.h里定义了这些常量。我们接下来主要关注时钟中断的处理。 在这里我们对时钟中断进行了一个简单的处理，即每次触发时钟中断的时候，我们会给一个计数器加一，并且设定好下一次时钟中断。当计数器加到100的时候，我们会输出一个100ticks表示我们触发了100次时钟中断。通过在模拟器中观察输出我们即刻看到是否正确触发了时钟中断，从而验证我们实现的异常处理机制。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab1/shi-zhong-zhong-duan.html":{"url":"lab1/shi-zhong-zhong-duan.html","title":"时钟中断","keywords":"","body":"时钟中断 时钟中断需要CPU硬件的支持。CPU以\"时钟周期\"为工作的基本时间单位，对逻辑门的时序电路进行同步。 我们的“时钟中断”实际上就是”每隔若干个时钟周期执行一次的程序“。 ”若干个时钟周期“是多少个？太短了肯定不行。如果时钟中断处理程序需要100个时钟周期执行，而你每50个时钟周期就触发一个时钟中断，那么间隔时间连一个完整的时钟中断程序都跑不完。如果你200个时钟周期就触发一个时钟中断，那么CPU的时间将有一半消耗在时钟中断，开销太大。一般而言，可以设置时钟中断间隔设置为CPU频率的1%，也就是每秒钟触发100次时钟中断，避免开销过大。 我们用到的RISCV对时钟中断的硬件支持包括： OpenSBI提供的sbi_set_timer()接口，可以传入一个时刻，让它在那个时刻触发一次时钟中断 rdtime伪指令，读取一个叫做time的CSR的数值，表示CPU启动之后经过的真实时间。在不同硬件平台，时钟频率可能不同。在QEMU上，这个时钟的频率是10MHz, 每过1s, rdtime返回的结果增大10000000 [!NOTE|style:flat] 在RISCV32和RISCV64架构中，time寄存器都是64位的。 rdcycle伪指令可以读取经过的时钟周期数目，对应一个寄存器cycle 注意，我们需要“每隔若干时间就发生一次时钟中断”，但是OpenSBI提供的接口一次只能设置一个时钟中断事件。我们采用的方式是：一开始只设置一个时钟中断，之后每次发生时钟中断的时候，设置下一次的时钟中断。 在clock.c里面初始化时钟并封装一些接口 //libs/sbi.c //当time寄存器(rdtime的返回值)为stime_value的时候触发一个时钟中断 void sbi_set_timer(unsigned long long stime_value) { sbi_call(SBI_SET_TIMER, stime_value, 0, 0); } // kern/driver/clock.c #include #include #include #include #include //volatile告诉编译器这个变量可能在其他地方被瞎改一通，所以编译器不要对这个变量瞎优化 volatile size_t ticks; //对64位和32位架构，读取time的方法是不同的 //32位架构下，需要把64位的time寄存器读到两个32位整数里，然后拼起来形成一个64位整数 //64位架构简单的一句rdtime就可以了 //__riscv_xlen是gcc定义的一个宏，可以用来区分是32位还是64位。 static inline uint64_t get_time(void) {//返回当前时间 #if __riscv_xlen == 64 uint64_t n; __asm__ __volatile__(\"rdtime %0\" : \"=r\"(n)); return n; #else uint32_t lo, hi, tmp; __asm__ __volatile__( \"1:\\n\" \"rdtimeh %0\\n\" \"rdtime %1\\n\" \"rdtimeh %2\\n\" \"bne %0, %2, 1b\" : \"=&r\"(hi), \"=&r\"(lo), \"=&r\"(tmp)); return ((uint64_t)hi 回来看trap.c里面时钟中断处理的代码, 还是很简单的：每秒100次时钟中断，触发每次时钟中断后设置10ms后触发下一次时钟中断，每触发100次时钟中断（1秒钟）输出一行信息到控制台。 // kern/trap/trap.c #include #define TICK_NUM 100 static void print_ticks() { cprintf(\"%d ticks\\n\", TICK_NUM); #ifdef DEBUG_GRADE cprintf(\"End of Test.\\n\"); panic(\"EOT: kernel seems ok.\"); #endif } void interrupt_handler(struct trapframe *tf) { intptr_t cause = (tf->cause > 1; switch (cause) { /* blabla 其他case*/ case IRQ_S_TIMER: clock_set_next_event();//发生这次时钟中断的时候，我们要设置下一次时钟中断 if (++ticks % TICK_NUM == 0) { print_ticks(); } break; /* blabla 其他case*/ } 现在执行make qemu, 应该能看到打印一行行的100 ticks。 [!TIP|style:flat] 时钟是属于外部设备了。之所以给大家呈现着一块，是为了能够更好的理解，操作系统与外设如何进行交互。中断来临如何处理。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab1/xiang-mu-zu-cheng-yu-zhi-hang-liu.html":{"url":"lab1/xiang-mu-zu-cheng-yu-zhi-hang-liu.html","title":"项目组成与执行流","keywords":"","body":"项目组成与执行流 项目组成 lab1 ├── Makefile ├── kern │ ├── debug │ │ ├── assert.h │ │ ├── kdebug.c │ │ ├── kdebug.h │ │ ├── kmonitor.c │ │ ├── kmonitor.h │ │ ├── panic.c │ │ └── stab.h │ ├── driver │ │ ├── clock.c │ │ ├── clock.h │ │ ├── console.c │ │ ├── console.h │ │ ├── intr.c │ │ └── intr.h │ ├── init │ │ ├── entry.S │ │ └── init.c │ ├── libs │ │ └── stdio.c │ ├── mm │ │ ├── memlayout.h │ │ ├── mmu.h │ │ ├── pmm.c │ │ └── pmm.h │ └── trap │ ├── trap.c │ ├── trap.h │ └── trapentry.S ├── lab1.md ├── libs │ ├── defs.h │ ├── error.h │ ├── printfmt.c │ ├── readline.c │ ├── riscv.h │ ├── sbi.c │ ├── sbi.h │ ├── stdarg.h │ ├── stdio.h │ ├── string.c │ └── string.h ├── readme.md └── tools ├── function.mk ├── gdbinit ├── grade.sh ├── kernel.ld ├── sign.c └── vector.c 9 directories, 43 files 硬件驱动层 kern/driver/clock.c(h): 通过OpenSBI的接口, 可以读取当前时间(rdtime), 设置时钟事件(sbi_set_timer)，是时钟中断必需的硬件支持。 kern/driver/intr.c(h): 中断也需要CPU的硬件支持，这里提供了设置中断使能位的接口（其实只封装了一句riscv指令）。 初始化 kern/init/init.c: 需要调用中断机制的初始化函数。 中断处理 kern/trap/trapentry.S: 我们把中断入口点设置为这段汇编代码。这段汇编代码把寄存器的数据挪来挪去，进行上下文切换。 kern/trap/trap.c(h): 分发不同类型的中断给不同的handler, 完成上下文切换之后对中断的具体处理，例如外设中断要处理外设发来的信息，时钟中断要触发特定的事件。中断处理初始化的函数也在这里，主要是把中断向量表(stvec)设置成所有中断都要跳到trapentry.S进行处理。 执行流 没有啦！需要自己总结，撰写实验报告噢！ Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab2/":{"url":"lab2/","title":"LAB2：物理内存管理","keywords":"","body":"LAB2：物理内存管理 如果我们只有物理内存空间，那么我们也可以写程序，但是所有的程序，包括内核，包括用户程序，都在同一个地址空间里，用户程序访问的0x80200000和内核访问的0x80200000是同一个地址。这样好不好？如果只有一个程序在运行，那也无所谓。但很多程序使用同一个内存空间，就会有问题：怎样防止程序之间互相干扰，甚至互相搞破坏？比较粗暴的方式就是，我让用户程序访问的0x80200000和内核访问的0x80200000不是一个地址。但是我们只有一块内存，为了创造两个不同的地址空间，我们可以引入一个”翻译“机制：程序使用的地址需要经过一步”翻译“才能变成真正的内存的物理地址。这个”翻译“过程，我们用一个”词典“实现---给出翻译之前的地址，可以在词典里查找翻译后的地址。每个程序都有唯一的一本”词典“，而它能使用的内存也就只有他的”词典“所包含的。 ”词典“是否对能使用的每个字节都进行翻译？我们可以想象，存储每个字节翻译的结果至少需要一个字节，那么使用1MB的内存将至少需要构造1MB的”词典“，这效率太低了。观察到，一个程序使用内存的数量级通常远大于字节，至少以KB为单位（所以上古时代的人说的是”640K对每个人都够了“而不是”640B对每个人都够了\"）。那么我们可以考虑，把连续的很多字节合在一起翻译，让他们翻译前后的数值之差相同，这就是“页”。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab2/shi-yan-mu-de.html":{"url":"lab2/shi-yan-mu-de.html","title":"实验目的","keywords":"","body":"实验目的 掌握内存管理相关的概念 掌握内存地址的转换机制 掌握页表的建立和使用方法 掌握物理内存的管理方法 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab2/shi-yan-nei-rong.html":{"url":"lab2/shi-yan-nei-rong.html","title":"实验内容","keywords":"","body":"实验内容 阅读手册有关内存管理部分内容。 了解如何发现系统中的物理内存。 学习如何使用页表机制进行物理内存与虚拟内存管理。 自己动手实现页面分配算法。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab2/lian-xi.html":{"url":"lab2/lian-xi.html","title":"练习","keywords":"","body":"练习 练习1：如何获取物理内存范围 如果 OS 无法提前知道当前硬件的可用物理内存范围，请问你有何办法让 OS 获取可用物理内存范围？ 练习2：实现Best Fit页面分配算法 实现Best Fit页面分配算法，算法的时空复杂度不做要求，能通过测试即可。参考kern/mm/default_pmm.c对First Fit算法的实现。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab2/di-zhi-yu-ye-biao.html":{"url":"lab2/di-zhi-yu-ye-biao.html","title":"地址与页表","keywords":"","body":"地址与页表 物理地址与虚拟地址 RV64支持多种分页方案，但我们只介绍最受欢迎的一种， Sv39。每个页的大小是4KB，也就是4096个字节。页表就是那个“字典”，里面有程序使用的虚拟页号到实际内存的物理页号的对应关系，但并不是所有的虚拟页都有对应的物理页。虚拟页可能的数目远大于物理页的数目，而且一个程序在运行时，一般不会拥有所有物理页的使用权，而只是将部分物理页在它的页表里进行映射。 在 Sv39 中，定义物理地址(Physical Address)有 56位，而虚拟地址(Virtual Address) 有 39位。实际使用的时候，一个虚拟地址要占用 64位，只有低 39位有效，规定 63−39 位的值必须等于第 38 位的值（类似有符号整数），否则会认为该虚拟地址不合法，在访问时会产生异常。 [!TIP|style:flat|label:未被使用的地址位] 由于Sv39的虚拟地址比RISC-v64整数寄存器要短，可能你想知道剩下的35位是什么。 Sv39要求地址位 63-39是第 38位的副本。因此有效的虚拟地址是0x0000 0000 0000 0000 -0x0000 003f ffff ffff和 0xffff ffc0 0000 0000-0xffff ffff ffff ffff。这两个区间之间间隔的大 小是两个区间长度大小的225倍，看上去似乎浪费了64位寄存器可以表达范围的99.999997%。为什么不充分地利用这额外的 25位空间呢？答案是，随着程序的增长，它们可能会需要大于 512 GiB的虚址空间。而架构师希望再不破坏向后兼容性的前提下增加地 址空间。如果我们允许程序在高 25位中存储额外的数据，那么以后就不可能把这些位回收 从而存储更大的地址。像这样允许在未使用的地址位中存储数据的严重错误，在计算机的历 史中已经重复出现了多次。 不论是物理地址还是虚拟地址，我们都可以认为，最后12位表示的是页内偏移，也就是这个地址在它所在页帧的什么位置（同一个位置的物理地址和虚拟地址的页内偏移相同）。除了最后12位，前面的部分表示的是物理页号或者虚拟页号。 页表项 很容易理解，我们需要给词典的每个词条约定一个固定的格式（包括每个词条的大小，含义），查起来才方便。 我们的”词典“（页表）存储在内存里，由若干个格式固定的”词条“也就是页表项（PTE, Page Table Entry）组成。 一个页表项是用来描述一个虚拟页号如何映射到物理页号的。如果一个虚拟页号通过某种手段找到了一个页表项，并通过读取上面的物理页号完成映射，我们称这个虚拟页号通过该页表项完成映射。 Sv39的一个页表项占据8字节，结构是这样的： 63-54 53-28 27-19 18-10 9-8 7 6 5 4 3 2 1 0 Reserved PPN[2] PPN[1] PPN[0] RSW D A G U X W R V 10 26 9 9 2 1 1 1 1 1 1 1 1 我们可以看到 Sv39 里面的一个页表项大小为64位 8 字节。其中第 53−10 共 44 位为一个物理页号，表示这个虚拟页号映射到的物理页号。后面的第 9−0位则描述映射的状态信息。 RSW两位留给 S Mode 的应用程序，我们可以用来进行拓展。 D，即 Dirty ，如果 D=1表示自从上次 D被清零后，有虚拟地址通过这个页表项进行写入。 A，即 Accessed，如果 A=1表示自从上次 A 被清零后，有虚拟地址通过这个页表项进行读、或者写、或者取指。 G，即Global，如果G=1表示这个页表项是”全局\"的，也就是所有的地址空间（所有的页表）都包含这一项 U(user)为 111 表示用户态 (U Mode)的程序 可以通过该页表项进行映射。在用户态运行时也只能够通过 U=1的页表项进行虚实地址映射。 注意，S Mode 不一定可以通过 U=1的页表项进行映射。我们需要将 S Mode 的状态寄存器 sstatus 上的 SUM 位手动设置为 111 才可以做到这一点（通常情况不会把它置1）。否则通过 U=1的页表项进行映射也会报出异常。另外，不论sstatus的SUM位如何取值，S Mode都不允许执行 U=1的页面里包含的指令，这是出于安全的考虑。 R,W,X为许可位，分别表示是否可读 (Readable)，可写 (Writable)，可执行 (Executable)。 V表示这个页表项是否合法。如果为 000 表示不合法，此时页表项其他位的值都会被忽略。 以 W这一位为例，如果 W=0 表示不可写，那么如果一条 store 的指令，它通过这个页表项完成了虚拟页号到物理页号的映射，找到了物理地址。但是仍然会报出异常，是因为这个页表项规定如果物理地址是通过它映射得到的，那么不准写入！R,X 也是同样的道理。 根据 R,W,X取值的不同，我们可以分成下面几种类型： X W R Meaning 0 0 0 指向下一级页表的指针 0 0 1 这一页只读 0 1 0 保留(reserved for future use) 0 1 1 这一页可读可写（不可执行） 1 0 0 这一页可读可执行（不可写） 1 0 1 这一页可读可执行 1 1 0 保留(reserved for future use) 1 1 1 这一页可读可写可执行 ”指向下一级页表的指针 “ 暗示我们有多级页表。下面就来看看多级页表是怎么回事。 多级页表 主要矛盾在于：相比于可用的物理内存空间，我们的虚拟地址空间太大，不可能为每个虚拟内存页都分配一个页表项。在Sv39中，虚拟地址有39位，后12位是页内偏移，还有27位可以编码不同的虚拟页号。如果开个大数组Pagetable[], 给2^27个虚拟页号都分配8字节的页表项，pagetable[vpn]是虚拟页号为vpn的虚拟页的页表项，那就是整整1 GiB的内存。这里面很多虚拟地址我们没有用到，会有大片大片的页表项的V 标志位为0（不合法）。我们不想为那么多非法页表项浪费宝贵的内存空间。 因此，我们可以对页表进行“分级”，变成一个树状结构。也就是把很多页表项组合成一个”大页“，如果这些页表项都非法（没有对应的物理页），那么只需要用一个非法的页表项来覆盖这个大页，而不需要分别建立一大堆非法页表项。很多个大页(megapage)还可以组合起来变成大大页(gigapage!)，继而可以有大大大页(terapage!).....但肯定不是分层越多越好，层数越多开销越大。 Sv39权衡各方面效率，使用三级页表。有4KiB=40964字节的页，大小为2MiB=2^21字节的大页，和大小为1 GiB的大大页。 原先的一个39位虚拟地址，被我们看成27位的页号和12位的页内偏移。 现在我们把它看成9位的“大大页页号”，9位的“大页页号”（也是大大页内的页内偏移），9位的“页号”（大页的页内偏移），还有12位的页内偏移。这是一个递归的过程，中间的每一级页表映射是类似的。 也就是说，整个Sv39的虚拟内存空间里，有512（2的9次方）个大大页，每个大大页里有512个大页，每个大页里有512个页，每个页里有4096个字节，整个虚拟内存空间里就有512∗512∗512∗4096512 512 512 * 4096512∗512∗512∗4096个字节，是512GiB的地址空间。 那么为啥是512呢？注意，4096/8 = 512，我们恰好可以在一页里放下512个页表项！ 我们可以认为，Sv39的多级页表在逻辑上是一棵树，它的每个叶子节点（直接映射4KB的页的页表项）都对应内存的一页，它的每个内部节点都对应512个更低一层的节点，而每个内部节点向更低一层的节点的链接都使用内存里的一页进行存储。 或者说，Sv39页表的根节点占据一页4KiB的内存，存储512个页表项，分别对应512个1 GiB的大大页，其中有些页表项（大大页）是非法的，另一些合法的页表项（大大页）是根节点的儿子，可以通过合法的页表项跳转到一个物理页号，这个物理页对应树中一个“大大页”的节点，里面有512个页表项，每个页表项对应一个2MiB的大页。同样，这些大页可能合法，也可能非法，非法的页表项不对应内存里的页，合法的页表项会跳转到一个物理页号，这个物理页对应树中一个“大页”的节点，里面有512个页表项，每个页表项对应一个4KiB的页，在这里最终完成虚拟页到物理页的映射。 三级和二级页表项不一定要指向下一级页表。我们知道每个一级页表项控制一个虚拟页号，即控制 4KiB 虚拟内存；每个二级页表项则控制 9位虚拟页号，总计控制 4KiB×2^9=2MiB虚拟内存；每个三级页表项控制 18位虚拟页号，总计控制 2MiB×2^9=1GiB虚拟内存。我们可以将二级页表项的 R,W,X设置为不是全 0 的许可要求，那么它将与一级页表项类似，只不过可以映射一个 2MiB的大页 (Mega Page) 。同理，也可以将三级页表项看作一个叶子，来映射一个 1GiB的大大页(Giga Page)。 [!NOTE|style:flat] 这么看来，建立一个虚拟页到物理页的映射，我们需要在三个层级（页，大页，大大页）各自给它分配一个物理页帧，是不是还没把所有物理内存都建立映射，就把所有物理页帧都耗尽了？ 事实上这个问题是不存在的。关键点在于，我们要映射的是一段连续的虚拟内存区间，因此，每连续建立 512 页的映射才会新建一个一级页表，每连续建立 512^2页的映射才会新建一个二级页表，而三级页表最多只新建一个。因此这样进行映射花费的总物理页帧数,约占物理内存中物理页帧总数的约1/512≈0.2%。 页表基址 在翻译的过程中，我们首先需要知道树状页表的根节点的物理地址（思考：为啥不是“根节点的虚拟地址”？）。 这一般保存在一个特殊寄存器里。对于RISCV架构，是一个叫做satp（Supervisor Address Translation and Protection Register）的CSR。实际上，satp里面存的不是最高级页表的起始物理地址，而是它所在的物理页号。除了物理页号，satp还包含其他信息 63-60 59-44 43-0 MODE ASID PPN 4 16 44 MODE表示当前页表的模式 0000表示不使用页表，直接使用物理地址，在简单的嵌入式系统里用着很方便。 0100表示Sv39页表，也就是我们使用的，虚拟内存空间高达512GiB。 0101表示Sv48页表，它和Sv39兼容，可以猜猜它有几层。虚拟内存空间高达256TiB。 其他编码保留备用。 ASID （Address Space Identifier 地址空间标识符）域是可选的，它可以用来降低上下文切换的开销。 PPN字段保存了根页表的物理地址，它以 4 KiB的页面大小为单位。通常 M模式的程序在第一次进入 S模式之前会把零写入 satp以禁用分页，然后 S模式的程序在初始化页表以后会再次进行satp寄存器的写操作。 OS 可以在内存中为不同的应用分别建立不同虚实映射的页表，并通过修改寄存器 satp 的值指向不同的页表，从而可以修改 CPU 虚实地址映射关系及内存保护的行为。 快表 物理内存的访问速度要比 CPU 的运行速度慢很多, 去访问一次物理内存可能需要几百个时钟周期（带来所谓的“冯诺依曼瓶颈”）。如果我们按照页表机制一步步走，将一个虚拟地址转化为物理地址需要访问 333 次物理内存，得到物理地址之后还要再访问一次物理内存，才能读到我们想要的数据。这很大程度上降低了效率。 好在，实践表明虚拟地址的访问具有时间局部性和空间局部性。 时间局部性是指，被访问过一次的地址很有可能不远的将来再次被访问； 空间局部性是指，如果一个地址被访问，则这个地址附近的地址很有可能在不远的将来被访问。 因此，在 CPU 内部，我们使用快表 (TLB, Translation Lookaside Buffer) 来记录近期已完成的虚拟页号到物理页号的映射。由于局部性，当我们要做一个映射时，会有很大可能这个映射在近期被完成过，所以我们可以先到 TLB 里面去查一下，如果有的话我们就可以直接完成映射，而不用访问那么多次内存了。 但是，我们如果修改了 satp 寄存器，比如将上面的 PPN字段进行了修改，说明我们切换到了一个与先前映射方式完全不同的页表。此时快表里面存储的映射结果就跟不上时代了，很可能是错误的。这种情况下我们要使用 sfence.vma 指令刷新整个 TLB 。 同样，我们手动修改一个页表项之后，也修改了映射，但 TLB 并不会自动刷新，我们也需要使用 sfence.vma 指令刷新 TLB 。如果不加参数的， sfence.vma 会刷新整个 TLB 。你可以在后面加上一个虚拟地址，这样 sfence.vma 只会刷新这个虚拟地址的映射。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab2/wu-li-nei-cun-tan-ce.html":{"url":"lab2/wu-li-nei-cun-tan-ce.html","title":"物理内存探测","keywords":"","body":"物理内存探测 操作系统怎样知道物理内存所在的那段物理地址呢？在 RISC-V 中，这个一般是由 bootloader ，即 OpenSBI 来完成的。它来完成对于包括物理内存在内的各外设的扫描，将扫描结果以 DTB(Device Tree Blob) 的格式保存在物理内存中的某个地方。随后 OpenSBI 会将其地址保存在 a1 寄存器中，给我们使用。 这个扫描结果描述了所有外设的信息，当中也包括 Qemu 模拟的 RISC-V 计算机中的物理内存。 [!TIP|style:flat|label:Qemu 模拟的 RISC-V virt 计算机中的物理内存] 通过查看virt.c的virt_memmap[]的定义，可以了解到 Qemu 模拟的 RISC-V virt 计算机的详细物理内存布局。可以看到，整个物理内存中有不少内存空洞（即含义为unmapped的地址空间），也有很多外设特定的地址空间，现在我们看不懂没有关系，后面会慢慢涉及到。目前只需关心最后一块含义为DRAM的地址空间，这就是 OS 将要管理的 128MB 的内存空间。 起始地址 终止地址 含义 0x0 0x100 QEMU VIRT_DEBUG 0x100 0x1000 unmapped 0x1000 0x12000 QEMU MROM (包括 hard-coded reset vector; device tree) 0x12000 0x100000 unmapped 0x100000 0x101000 QEMU VIRT_TEST 0x101000 0x2000000 unmapped 0x2000000 0x2010000 QEMU VIRT_CLINT 0x2010000 0x3000000 unmapped 0x3000000 0x3010000 QEMU VIRT_PCIE_PIO 0x3010000 0xc000000 unmapped 0xc000000 0x10000000 QEMU VIRT_PLIC 0x10000000 0x10000100 QEMU VIRT_UART0 0x10000100 0x10001000 unmapped 0x10001000 0x10002000 QEMU VIRT_VIRTIO 0x10002000 0x20000000 unmapped 0x20000000 0x24000000 QEMU VIRT_FLASH 0x24000000 0x30000000 unmapped 0x30000000 0x40000000 QEMU VIRT_PCIE_ECAM 0x40000000 0x80000000 QEMU VIRT_PCIE_MMIO 0x80000000 0x88000000 DRAM 缺省 128MB，大小可配置 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab2/yi-ye-wei-dan-wei-guan-li-wu-li-nei-cun.html":{"url":"lab2/yi-ye-wei-dan-wei-guan-li-wu-li-nei-cun.html","title":"以页为单位管理物理内存","keywords":"","body":"以页为单位管理物理内存 Page结构体 在获得可用物理内存范围后，系统需要建立相应的数据结构来管理以物理页（按4KB对齐，且大小为4KB的物理内存单元）为最小单位的整个物理内存，以配合后续涉及的分页管理机制。每个物理页可以用一个 Page数据结构来表示。由于一个物理页需要占用一个Page结构的空间，Page结构在设计时须尽可能小，以减少对内存的占用。Page的定义在kern/mm/memlayout.h中。以页为单位的物理内存分配管理的实现在kern/default_pmm.[ch]。 为了与以后的分页机制配合，我们首先需要建立对整个计算机的每一个物理页的属性用结构Page来表示，它包含了映射此物理页的虚拟页个数，描述物理页属性的flags和双向链接各个Page结构的page_link双向链表。 struct Page { int ref; // page frame's reference counter uint32_t flags; // array of flags that describe the status of the page frame unsigned int property;// the num of free block, used in first fit pm manager list_entry_t page_link;// free list link }; 这里看看Page数据结构的各个成员变量有何具体含义。ref表示这页被页表的引用记数（在“实现分页机制”一节会讲到）。如果这个页被页表引用了，即在某页表中有一个页表项设置了一个虚拟页到这个Page管理的物理页的映射关系，就会把Page的ref加一；反之，若页表项取消，即映射关系解除，就会把Page的ref减一。flags表示此物理页的状态标记，进一步查看kern/mm/memlayout.h中的定义，可以看到： /* Flags describing the status of a page frame */ #define PG_reserved 0 // the page descriptor is reserved for kernel or unusable #define PG_property 1 // the member 'property' is valid 这表示flags目前用到了两个bit表示页目前具有的两种属性，bit 0表示此页是否被保留（reserved），如果是被保留的页，则bit 0会设置为1，且不能放到空闲页链表中，即这样的页不是空闲页，不能动态分配与释放。比如目前内核代码占用的空间就属于这样“被保留”的页。在本实验中，bit 1表示此页是否是free的，如果设置为1，表示这页是free的，可以被分配；如果设置为0，表示这页已经被分配出去了，不能被再二次分配。另外，本实验这里取的名字PG_property比较不直观 ，主要是我们可以设计不同的页分配算法（best fit, buddy system等），那么这个PG_property就有不同的含义了。 在本实验中，Page数据结构的成员变量property用来记录某连续内存空闲块的大小（即地址连续的空闲页的个数）。这里需要注意的是用到此成员变量的这个Page比较特殊，是这个连续内存空闲块地址最小的一页（即头一页， Head Page）。连续内存空闲块利用这个页的成员变量property来记录在此块内的空闲页的个数。这里去的名字property也不是很直观，原因与上面类似，在不同的页分配算法中，property有不同的含义。 Page数据结构的成员变量page_link是便于把多个连续内存空闲块链接在一起的双向链表指针（可回顾在lab0实验指导书中有关双向链表数据结构的介绍）。这里需要注意的是用到此成员变量的这个Page比较特殊，是这个连续内存空闲块地址最小的一页（即头一页， Head Page）。连续内存空闲块利用这个页的成员变量page_link来链接比它地址小和大的其他连续内存空闲块。 free_area_t结构体 在初始情况下，也许这个物理内存的空闲物理页都是连续的，这样就形成了一个大的连续内存空闲块。但随着物理页的分配与释放，这个大的连续内存空闲块会分裂为一系列地址不连续的多个小连续内存空闲块，且每个连续内存空闲块内部的物理页是连续的。那么为了有效地管理这些小连续内存空闲块。所有的连续内存空闲块可用一个双向链表管理起来，便于分配和释放，为此定义了一个free_area_t数据结构，包含了一个list_entry结构的双向链表指针和记录当前空闲页的个数的无符号整型变量nr_free。其中的链表指针指向了空闲的物理页。 /* free_area_t - maintains a doubly linked list to record free (unused) pages */ typedef struct { list_entry_t free_list; // the list header unsigned int nr_free; // # of free pages in this free list } free_area_t; 初始化Page结构体 为了管理物理内存，我们需要在内核里定义一些数据结构，来存储”当前使用了哪些物理页面，哪些物理页面没被使用“这样的信息，使用的是Page结构体。我们将一些Page结构体在内存里排列在内核后面，这要占用一些内存。而摆放这些Page结构体的物理页面，以及内核占用的物理页面，之后都无法再使用了。我们用page_init()函数给这些管理物理内存的结构体做初始化。 page_init()的代码里，我们调用了一个函数init_memmap(), 这和我们的另一个结构体pmm_manager有关。虽然C语言基本上不支持面向对象，但我们可以用类似面向对象的思路，把”物理内存管理“的功能集中给一个结构体。我们甚至可以让函数指针作为结构体的成员，强行在C语言里支持了”成员函数“。可以看到，我们调用的init_memmap()实际上又调用了pmm_manager的一个”成员函数“。 pmm_manager提供了各种接口：分配页面，释放页面，查看当前空闲页面数。但是我们好像始终没看见pmm_manager内部对这些接口的实现，那些接口只是作为函数指针，作为pmm_manager的一部分，我们需要把那些函数指针变量赋值为真正的函数名称。在这里面我们把pmm_manager的指针赋值成&default_pmm_manager。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab2/ye-mian-fen-pei-suan-fa.html":{"url":"lab2/ye-mian-fen-pei-suan-fa.html","title":"页面分配算法","keywords":"","body":"页面分配算法 如果要在ucore中实现连续物理内存分配算法，则需要考虑的事情比较多，相对课本上的物理内存分配算法描述要复杂不少。下面介绍一下如果要实现一个FirstFit内存分配算法的大致流程。原理FirstFit内存分配算法上很简单，但要在ucore中实现，需要充分了解和利用ucore已有的数据结构和相关操作、关键的一些全局变量等。 关键数据结构与变量 first_fit分配算法需要维护一个查找有序（地址按从小到大排列）空闲块（以页为最小单位的连续地址空间）的数据结构，而双向链表是一个很好的选择。 libs/list.h定义了可挂接任意元素的通用双向链表结构和对应的操作，所以需要了解如何使用这个文件提供的各种函数，从而可以完成对双向链表的初始化/插入/删除等。 显然，我们可以通过free_area_t数据结构来完成对空闲块的管理。而default_pmm.c中定义的free_area变量就是干这个事情的。 kern/mm/pmm.h中定义了一个通用的分配算法的函数列表，用pmm_manager 表示。其中init函数就是用来初始化free_area变量的, first_fit分配算法可直接重用default_init函数的实现。init_memmap函数需要根据现有的内存情况构建空闲块列表的初始状态。何时应该执行这个函数呢？ 通过分析代码，可以知道： kern_init --> pmm_init-->page_init-->init_memmap--> pmm_manager->init_memmap 所以，default_init_memmap需要根据page_init函数中传递过来的参数（某个连续地址的空闲块的起始页，页个数）来建立一个连续内存空闲块的双向链表。这里有一个假定page_init函数是按地址从小到大的顺序传来的连续内存空闲块的。链表头是free_area.free_list，链表项是Page数据结构的base->page_link。这样我们就依靠Page数据结构中的成员变量page_link形成了连续内存空闲块列表。 设计实现 default_init_memmap函数将根据每个物理页帧的情况来建立空闲页链表，且空闲页块应该是根据地址高低形成一个有序链表。根据上述变量的定义，default_init_memmap可大致实现如下： default_init_memmap(struct Page *base, size_t n) { struct Page *p = base; for (; p != base + n; p ++) { p->flags = p->property = 0; set_page_ref(p, 0); } base->property = n; SetPageProperty(base); nr_free += n; list_add(&free_list, &(base->page_link)); } 如果要分配一个页，那要考虑哪些呢？这里就需要考虑实现default_alloc_pages函数，注意参数n表示要分配n个页。另外，需要注意实现时尽量多考虑一些边界情况，这样确保软件的鲁棒性。比如 if (n > nr_free) { return NULL; } 这样可以确保分配不会超出范围。也可加一些 assert函数，在有错误出现时，能够迅速发现。比如 n应该大于0，我们就可以加上 assert(n \\> 0); 这样在nfirstfit需要从空闲链表头开始查找最小的地址，通过list_next找到下一个空闲块元素，通过le2page宏可以由链表元素获得对应的Page指针p。通过p->property可以了解此空闲块的大小。如果>=n，这就找到了！如果list_next== &free_list，这表示找完了一遍了。找到后，就要从新组织空闲块，然后把找到的page返回。所以default_alloc_pages可大致实现如下： static struct Page * default_alloc_pages(size_t n) { if (n > nr_free) { return NULL; } struct Page *page = NULL; list_entry_t *le = &free_list; while ((le = list_next(le)) != &free_list) { struct Page *p = le2page(le, page_link); if (p->property >= n) { page = p; break; } } if (page != NULL) { list_del(&(page->page_link)); if (page->property > n) { struct Page *p = page + n; p->property = page->property - n; list_add(&free_list, &(p->page_link)); } nr_free -= n; ClearPageProperty(page); } return page; } default_free_pages函数的实现其实是default_alloc_pages的逆过程，不过需要考虑空闲块的合并问题。这里就不再细讲了。自行阅读代码。 [!NOTE|style:flat] 看完了first fit算法了，就需要自己实现best fit算法了噢，思路都是一样的，依葫芦画瓢。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab2/xiang-mu-zu-cheng-yu-zhi-hang-liu.html":{"url":"lab2/xiang-mu-zu-cheng-yu-zhi-hang-liu.html","title":"项目组成与执行流","keywords":"","body":"项目组成与执行流 项目组成 lab2 ├── Makefile ├── kern │ ├── debug │ │ ├── assert.h │ │ ├── kdebug.c │ │ ├── kdebug.h │ │ ├── kmonitor.c │ │ ├── kmonitor.h │ │ ├── panic.c │ │ └── stab.h │ ├── driver │ │ ├── clock.c │ │ ├── clock.h │ │ ├── console.c │ │ ├── console.h │ │ ├── intr.c │ │ └── intr.h │ ├── init │ │ ├── entry.S │ │ └── init.c │ ├── libs │ │ └── stdio.c │ ├── mm │ │ ├── best_fit_pmm.c │ │ ├── best_fit_pmm.h │ │ ├── default_pmm.c │ │ ├── default_pmm.h │ │ ├── memlayout.h │ │ ├── mmu.h │ │ ├── pmm.c │ │ └── pmm.h │ ├── sync │ │ └── sync.h │ └── trap │ ├── trap.c │ ├── trap.h │ └── trapentry.S ├── lab2.md ├── libs │ ├── atomic.h │ ├── defs.h │ ├── error.h │ ├── list.h │ ├── printfmt.c │ ├── readline.c │ ├── riscv.h │ ├── sbi.c │ ├── sbi.h │ ├── stdarg.h │ ├── stdio.h │ ├── string.c │ └── string.h └── tools ├── boot.ld ├── function.mk ├── gdbinit ├── grade.sh ├── kernel.ld ├── kernel_nopage.ld ├── sign.c └── vector.c 10 directories, 51 files 链表 libs/list.h：定义了通用双向链表结构以及相关的查找、插入等基本操作，这是建立基于链表方法的物理内存管理（以及其他内核功能）的基础。其他有类似双向链表需求的内核功能模块可直接使用 list.h 中定义的函数。 物理内存管理器 kern/mm/pmm.c(h)物理内存管理器结构的实现，定义了内存管理相关函数。 默认的页面分配算法 kern/mm/default_pmm.c(h) 上一节已经介绍完毕，具体的内存管理函数的实现，还包括实现的检查。 Best Fit 页面分配算法 kern/mm/best_fit_pmm.c(h) 这就是需要自己实现啦~疯狂暗示：依葫芦画瓢！甚至是画葫芦。 执行流 结合前面所述自行理解、总结。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab3/":{"url":"lab3/","title":"LAB3：虚拟内存管理","keywords":"","body":"LAB3：虚拟内存管理 什么是虚拟内存？简单地说是指程序员或CPU“看到”的内存。但有几点需要注意： 虚拟内存单元不一定有实际的物理内存单元对应，即实际的物理内存单元可能不存在； 如果虚拟内存单元对应有实际的物理内存单元，那二者的地址一般是不相等的； 通过操作系统实现的某种内存映射可建立虚拟内存与物理内存的对应关系，使得程序员或CPU访问的虚拟内存地址会自动转换为一个物理内存地址。 那么这个“虚拟”的作用或意义在哪里体现呢？在操作系统中，虚拟内存其实包含多个虚拟层次，在不同的层次体现了不同的作用。首先，在有了分页机制后，程序员或CPU“看到”的地址已经不是实际的物理地址了，这已经有一层虚拟化，我们可简称为内存地址虚拟化。有了内存地址虚拟化，我们就可以通过设置页表项来限定软件运行时的访问空间，确保软件运行不越界，完成内存访问保护的功能。 通过内存地址虚拟化，可以使得软件在没有访问某虚拟内存地址时不分配具体的物理内存，而只有在实际访问某虚拟内存地址时，操作系统再动态地分配物理内存，建立虚拟内存到物理内存的页映射关系，这种技术称为按需分页（demand paging）。把不经常访问的数据所占的内存空间临时写到硬盘上，这样可以腾出更多的空闲内存空间给经常访问的数据；当CPU访问到不经常访问的数据时，再把这些数据从硬盘读入到内存中，这种技术称为页换入换出（page swap in/out）。这种内存管理技术给了程序员更大的内存“空间”，从而可以让更多的程序在内存中并发运行。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab3/shi-yan-mu-de.html":{"url":"lab3/shi-yan-mu-de.html","title":"实验目的","keywords":"","body":"实验目的 了解虚拟内存的Page Fault异常处理实现 掌握页替换算法在操作系统中的实现 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab3/shi-yan-nei-rong.html":{"url":"lab3/shi-yan-nei-rong.html","title":"实验内容","keywords":"","body":"实验内容 阅读理论课有关页面置换相关的内容。 阅读PageFault异常处理和FIFO页面替换算法的实现。 自己动手实现页面置换算法。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab3/lian-xi.html":{"url":"lab3/lian-xi.html","title":"练习","keywords":"","body":"练习 练习1：简述页面从换入到换出过程 描述FIFO页面置换算法下，一个页面从被换入到被换出的过程中，会经过代码里哪些函数/宏的处理（或者说，需要调用哪些函数/宏），并用简单的一两句话描述每个函数在过程中做了什么。我们认为只要函数原型不同，就算两个不同的函数。要求指出对执行过程有实际影响,删去后会导致输出结果不同的函数（例如assert）而不是cprintf这样的函数。 练习2：理解get_pte函数 get_pte()函数中有两段形式类似的代码， 结合sv32，sv39，sv48的异同，解释这两段代码为什么如此相像? 目前get_pte()函数将页表项的查找和页表项的分配合并在一个函数里，你认为这种写法好吗？有没有必要把两个功能拆开？ 练习3：实现Clock页替换算法 在我们给出的框架上，填写代码，实现 Clock页替换算法，比较它和FIFO算法的不同。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab3/ye-mian-zhi-huan.html":{"url":"lab3/ye-mian-zhi-huan.html","title":"页面置换","keywords":"","body":"页面置换 操作系统为何要进行页面置换呢？这是由于操作系统给用户态的应用程序提供了一个虚拟的“大容量”内存空间，而实际的物理内存空间又没有那么大。所以操作系统就就“瞒着”应用程序，只把应用程序中“常用”的数据和代码放在物理内存中，而不常用的数据和代码放在了硬盘这样的存储介质上。如果应用程序访问的是“常用”的数据和代码，那么操作系统已经放置在内存中了，不会出现什么问题。但当应用程序访问它认为应该在内存中的的数据或代码时，如果这些数据或代码不在内存中，则根据上一小节的介绍，会产生页访问异常。这时，操作系统必须能够应对这种页访问异常，即尽快把应用程序当前需要的数据或代码放到内存中来，然后重新执行应用程序产生异常的访存指令。如果在把硬盘中对应的数据或代码调入内存前，操作系统发现物理内存已经没有空闲空间了，这时操作系统必须把它认为“不常用”的页换出到磁盘上去，以腾出内存空闲空间给应用程序所需的数据或代码。 页面置换算法 操作系统迟早会碰到没有内存空闲空间而必须要置换出内存中某个“不常用”的页的情况。如何判断内存中哪些是“常用”的页，哪些是“不常用”的页，把“常用”的页保持在内存中，在物理内存空闲空间不够的情况下，把“不常用”的页置换到硬盘上就是页替换算法着重考虑的问题。容易理解，一个好的页替换算法会导致页访问异常次数少，也就意味着访问硬盘的次数也少，从而使得应用程序执行的效率就高。本次实验涉及的页替换算法（包括扩展练习）： 先进先出(First In First Out, FIFO)页替换算法：该算法总是淘汰最先进入内存的页，即选择在内存中驻留时间最久的页予以淘汰。只需把一个应用程序在执行过程中已调入内存的页按先后次序链接成一个队列，队列头指向内存中驻留时间最久的页，队列尾指向最近被调入内存的页。这样需要淘汰页时，从队列头很容易查找到需要淘汰的页。FIFO 算法只是在应用程序按线性顺序访问地址空间时效果才好，否则效率不高。因为那些常被访问的页，往往在内存中也停留得最久，结果它们因变“老”而不得不被置换出去。FIFO 算法的另一个缺点是，它有一种异常现象（Belady 现象），即在增加放置页的物理页帧的情况下，反而使页访问异常次数增多。 最久未使用(least recently used, LRU)算法：利用局部性，通过过去的访问情况预测未来的访问情况，我们可以认为最近还被访问过的页面将来被访问的可能性大，而很久没访问过的页面将来不太可能被访问。于是我们比较当前内存里的页面最近一次被访问的时间，把上一次访问时间离现在最久的页面置换出去。 时钟（Clock）页替换算法：是 LRU 算法的一种近似实现。时钟页替换算法把各个页面组织成环形链表的形式，类似于一个钟的表面。然后把一个指针（简称当前指针）指向最老的那个页面，即最先进来的那个页面。另外，时钟算法需要在页表项（PTE）中设置了一位访问位来表示此页表项对应的页当前是否被访问过。当该页被访问时，CPU 中的 MMU 硬件将把访问位置“1”。当操作系统需要淘汰页时，对当前指针指向的页所对应的页表项进行查询，如果访问位为“0”，则淘汰该页，如果该页被写过，则还要把它换出到硬盘上；如果访问位为“1”，则将该页表项的此位置“0”，继续访问下一个页。该算法近似地体现了 LRU 的思想，且易于实现，开销少，需要硬件支持来设置访问位。时钟页替换算法在本质上与 FIFO 算法是类似的，不同之处是在时钟页替换算法中跳过了访问位为 1 的页。 改进的时钟（Enhanced Clock）页替换算法：在时钟置换算法中，淘汰一个页面时只考虑了页面是否被访问过，但在实际情况中，还应考虑被淘汰的页面是否被修改过。因为淘汰修改过的页面还需要写回硬盘，使得其置换代价大于未修改过的页面，所以优先淘汰没有修改的页，减少磁盘操作次数。改进的时钟置换算法除了考虑页面的访问情况，还需考虑页面的修改情况。即该算法不但希望淘汰的页面是最近未使用的页，而且还希望被淘汰的页是在主存驻留期间其页面内容未被修改过的。这需要为每一页的对应页表项内容中增加一位引用位和一位修改位。当该页被访问时，CPU 中的 MMU 硬件将把访问位置“1”。当该页被“写”时，CPU 中的 MMU 硬件将把修改位置“1”。这样这两位就存在四种可能的组合情况：（0，0）表示最近未被引用也未被修改，首先选择此页淘汰；（0，1）最近未被使用，但被修改，其次选择；（1，0）最近使用而未修改，再次选择；（1，1）最近使用且修改，最后选择。该算法与时钟算法相比，可进一步减少磁盘的 I/O 操作次数，但为了查找到一个尽可能适合淘汰的页面，可能需要经过多次扫描，增加了算法本身的执行开销。 虚假的硬盘 在QEMU里实际上并没有真正模拟“硬盘”。为了实现“页面置换”的效果，我们采取的措施是，从内核的静态存储(static)区里面分出一块内存， 声称这块存储区域是”硬盘“，然后包裹一下给出”硬盘IO“的接口。思考一下，内存和硬盘，除了一个掉电后数据易失一个不易失，一个访问快一个访问慢，其实并没有本质的区别。对于我们的页面置换算法来说，也不要求硬盘上存多余页面的交换空间能够“不易失”，反正这些页面存在内存里的时候就是易失的。理论上，我们完全可以把一块机械硬盘加以改造，写好驱动之后，插到主板的内存插槽上作为内存条使用，当然性能就别想了。（如果半导体工业没有发明出成本和访问速率都介于寄存器和硬盘之间的ram, 我们将不得不这么做！）那么我们就把QEMU模拟出来的一块ram叫做“硬盘”，用作页面置换时的交换区，完全没有问题。你可能会觉得，这样折腾一通，我们总共能使用的页面数并没有增加，原先能直接在内存里使用的一些页面变成了“硬盘”，只是在自娱自乐。确实，我们在这里只是想介绍页面置换的原理，并不关心实际性能。 这一部分在driver/ide.h driver/ide.c fs/fs.h fs/swapfs.h fs/swapfs.c实现。 fs就是file system,我们这里其实并没有“文件”的概念，这个模块称作fs只是说明它是“硬盘”和内核之间的接口。 ide在这里不是integrated development environment的意思，而是Integrated Drive Electronics的意思，表示的是一种标准的硬盘接口。这里写的东西和Integrated Drive Electronics并不相关，这个命名是ucore的历史遗留。 // kern/driver/ide.c /* #include\"s */ void ide_init(void) {} #define MAX_IDE 2 #define MAX_DISK_NSECS 56 static char ide[MAX_DISK_NSECS * SECTSIZE]; bool ide_device_valid(unsigned short ideno) { return ideno 可以看到，我们这里所谓的“硬盘IO”，只是在内存里用memcpy把数据复制来复制去。同时为了逼真地模仿磁盘，我们只允许以磁盘扇区为数据传输的基本单位，也就是一次传输的数据必须是512字节的倍数，并且必须对齐。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab3/pagefault.html":{"url":"lab3/pagefault.html","title":"PageFault","keywords":"","body":"PageFault 出现 一般应用程序的对虚拟内存的“需 求”与物理内存空间的“供给”没有直接的对应关系，ucore是通过page fault异常处理来间接完成 这二者之间的衔接。 当我们引入了虚拟内存，就意味着虚拟内存的空间可以远远大于物理内存，意味着程序可以访问\"不对应物理内存页帧的虚拟内存地址\"，这时CPU应当抛出Page Fault这个异常。 [!NOTE|style:flat] 在操作系统的设计中，一个基本的原则是：并非所有的物理页都可以交换出去的，只有映射到用户空间且被用户程序直接访问的页面才能被交换，而被内核直接使用的内核空间的页面不能被换出。这里面的原因是什么呢？操作系统是执行的关键代码，需要保证运行的高效性和实时性，如果在操作系统执行过程中，发生了缺页现象，则操作系统不得不等很长时间（硬盘的访问速度比内存的访问速度慢 2~3 个数量级），这将导致整个系统运行低效。而且，不难想象，处理缺页过程所用到的内核代码或者数据如果被换出，整个内核都面临崩溃的危险。 但在实验三实现的 ucore 中，我们只是实现了换入换出机制，还没有设计用户态执行的程序，所以我们在实验三中仅仅通过执行 check_swap 函数在内核中分配一些页，模拟对这些页的访问，然后通过 do_pgfault 来调用 swap_map_swappable 函数来查询这些页的访问情况并间接调用相关函数，换出“不常用”的页到磁盘上。 处理 pgfault_handler 回想一下，我们处理异常的时候，是在kern/trap/trap.c的exception_handler()函数里进行的。按照scause寄存器对异常的分类里，有CAUSE_LOAD_PAGE_FAULT 和CAUSE_STORE_PAGE_FAULT两个case。之前我们并没有真正对异常进行处理，只是简单输出一下就返回了。现在我们要真正进行Page Fault的处理。 // kern/trap/trap.c static inline void print_pgfault(struct trapframe *tf) { cprintf(\"page falut at 0x%08x: %c/%c\\n\", tf->badvaddr, trap_in_kernel(tf) ? 'K' : 'U', tf->cause == CAUSE_STORE_PAGE_FAULT ? 'W' : 'R'); } static int pgfault_handler(struct trapframe *tf) { extern struct mm_struct *check_mm_struct; print_pgfault(tf); if (check_mm_struct != NULL) { return do_pgfault(check_mm_struct, tf->cause, tf->badvaddr); } panic(\"unhandled page fault.\\n\"); } void exception_handler(struct trapframe *tf) { int ret; switch (tf->cause) { /* .... other cases */ case CAUSE_FETCH_PAGE_FAULT:// 取指令时发生的Page Fault先不处理 cprintf(\"Instruction page fault\\n\"); break; case CAUSE_LOAD_PAGE_FAULT: cprintf(\"Load page fault\\n\"); if ((ret = pgfault_handler(tf)) != 0) { print_trapframe(tf); panic(\"handle pgfault failed. %e\\n\", ret); } break; case CAUSE_STORE_PAGE_FAULT: cprintf(\"Store/AMO page fault\\n\"); if ((ret = pgfault_handler(tf)) != 0) { //do_pgfault()页面置换成功时返回0 print_trapframe(tf); panic(\"handle pgfault failed. %e\\n\", ret); } break; default: print_trapframe(tf); break; } } 这里的异常处理程序，把Page Fault分发给kern/mm/vmm.c的do_pgfault()函数，尝试进行页面置换。接下来我们处理多级页表。之前的初始页表占据一个页的物理内存，只有一个页表项是有用的，映射了一个大大页(Giga Page)。 地址转换 之前我们物理页帧管理有个功能没有实现，那就是动态的内存分配。管理虚拟内存的数据结构（页表）需要有空间进行存储，而我们又没有给它预先分配内存（也无法预先分配，因为事先不确定我们的页表需要分配多少内存），就需要有malloc/free的接口来分配释放内存。我们在这里顺便看看pmm.h里对物理页面和虚拟地址，物理地址进行转换的一些函数。 // kern/mm/pmm.c void *kmalloc(size_t n) { //分配至少n个连续的字节，这里实现得不精细，占用的只能是整数个页。 void *ptr = NULL; struct Page *base = NULL; assert(n > 0 && n 0 && n = npage) { \\ panic(\"KADDR called with invalid pa %08lx\", __m_pa); \\ } \\ (void *)(__m_pa + va_pa_offset); \\ }) extern struct Page *pages; extern size_t npage; extern const size_t nbase; extern uint_t va_pa_offset; /* 我们曾经在内存里分配了一堆连续的Page结构体，来管理物理页面。可以把它们看作一个结构体数组。 pages指针是这个数组的起始地址，减一下，加上一个基准值nbase, 就可以得到正确的物理页号。 pages指针和nbase基准值我们都在其他地方做了正确的初始化 */ static inline ppn_t page2ppn(struct Page *page) { return page - pages + nbase; } /* 指向某个Page结构体的指针，对应一个物理页面，也对应一个起始的物理地址。 左移若干位就可以从物理页号得到页面的起始物理地址。 */ static inline uintptr_t page2pa(struct Page *page) { return page2ppn(page) = npage) { panic(\"pa2page called with invalid pa\"); } return &pages[PPN(pa) - nbase];//把pages指针当作数组使用 } static inline void *page2kva(struct Page *page) { return KADDR(page2pa(page)); } static inline struct Page *kva2page(void *kva) { return pa2page(PADDR(kva)); } //从页表项得到对应的页，这里用到了 PTE_ADDR(pte)宏，对页表项做操作，在mmu.h里 定义 static inline struct Page *pte2page(pte_t pte) { if (!(pte & PTE_V)) { panic(\"pte2page called with invalid pte\"); } return pa2page(PTE_ADDR(pte)); } //PDE(Page Directory Entry)指的是不在叶节点的页表项（指向低一级页表的页表项） static inline struct Page *pde2page(pde_t pde) { //PDE_ADDR这个宏和PTE_ADDR是一样的 return pa2page(PDE_ADDR(pde)); } Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab3/shi-yong-duo-ji-ye-biao.html":{"url":"lab3/shi-yong-duo-ji-ye-biao.html","title":"使用多级页表","keywords":"","body":"使用多级页表 我们需要把页表放在内存里，并且需要有办法修改页表，比如在页表里增加一个页面的映射或者删除某个页面的映射。 最主要的是两个接口： page_insert()，在页表里建立一个映射 page_remove()，在页表里删除一个映射 这些我们都在kern/mm/pmm.c里面编写。 我们来看page_insert(),page_remove()的实现。注意它们都要调用两个对页表项进行操作的函数：get_pte()和page_remove_pte() int page_insert(pde_t *pgdir, struct Page *page, uintptr_t la, uint32_t perm) { //pgdir是页表基址(satp)，page对应物理页面，la是虚拟地址 pte_t *ptep = get_pte(pgdir, la, 1); //先找到对应页表项的位置，如果原先不存在，get_pte()会分配页表项的内存 if (ptep == NULL) { return -E_NO_MEM; } page_ref_inc(page);//指向这个物理页面的虚拟地址增加了一个 if (*ptep & PTE_V) { //原先存在映射 struct Page *p = pte2page(*ptep); if (p == page) {//如果这个映射原先就有 page_ref_dec(page); } else {//如果原先这个虚拟地址映射到其他物理页面，那么需要删除映射 page_remove_pte(pgdir, la, ptep); } } *ptep = pte_create(page2ppn(page), PTE_V | perm);//构造页表项 tlb_invalidate(pgdir, la);//页表改变之后要刷新TLB return 0; } void page_remove(pde_t *pgdir, uintptr_t la) { pte_t *ptep = get_pte(pgdir, la, 0);//找到页表项所在位置 if (ptep != NULL) { page_remove_pte(pgdir, la, ptep);//删除这个页表项的映射 } } //删除一个页表项以及它的映射 static inline void page_remove_pte(pde_t *pgdir, uintptr_t la, pte_t *ptep) { if (*ptep & PTE_V) { //(1) check if this page table entry is valid struct Page *page = pte2page(*ptep); //(2) find corresponding page to pte page_ref_dec(page); //(3) decrease page reference if (page_ref(page) == 0) { //(4) and free this page when page reference reachs 0 free_page(page); } *ptep = 0; //(5) clear page table entry tlb_invalidate(pgdir, la); //(6) flush tlb } } //寻找(有必要的时候分配)一个页表项 pte_t *get_pte(pde_t *pgdir, uintptr_t la, bool create) { /* LAB2 EXERCISE 2: YOUR CODE * * If you need to visit a physical address, please use KADDR() * please read pmm.h for useful macros * * Maybe you want help comment, BELOW comments can help you finish the code * * Some Useful MACROs and DEFINEs, you can use them in below implementation. * MACROs or Functions: * PDX(la) = the index of page directory entry of VIRTUAL ADDRESS la. * KADDR(pa) : takes a physical address and returns the corresponding * kernel virtual address. * set_page_ref(page,1) : means the page be referenced by one time * page2pa(page): get the physical address of memory which this (struct * Page *) page manages * struct Page * alloc_page() : allocation a page * memset(void *s, char c, size_t n) : sets the first n bytes of the * memory area pointed by s * to the specified value c. * DEFINEs: * PTE_P 0x001 // page table/directory entry * flags bit : Present * PTE_W 0x002 // page table/directory entry * flags bit : Writeable * PTE_U 0x004 // page table/directory entry * flags bit : User can access */ pde_t *pdep1 = &pgdir[PDX1(la)];//找到对应的Giga Page if (!(*pdep1 & PTE_V)) {//如果下一级页表不存在，那就给它分配一页，创造新页表 struct Page *page; if (!create || (page = alloc_page()) == NULL) { return NULL; } set_page_ref(page, 1); uintptr_t pa = page2pa(page); memset(KADDR(pa), 0, PGSIZE); //我们现在在虚拟地址空间中，所以要转化为KADDR再memset. //不管页表怎么构造，我们确保物理地址和虚拟地址的偏移量始终相同，那么就可以用这种方式完成对物理内存的访问。 *pdep1 = pte_create(page2ppn(page), PTE_U | PTE_V);//注意这里R,W,X全零 } pde_t *pdep0 = &((pde_t *)KADDR(PDE_ADDR(*pdep1)))[PDX0(la)];//再下一级页表 //这里的逻辑和前面完全一致，页表不存在就现在分配一个 if (!(*pdep0 & PTE_V)) { struct Page *page; if (!create || (page = alloc_page()) == NULL) { return NULL; } set_page_ref(page, 1); uintptr_t pa = page2pa(page); memset(KADDR(pa), 0, PGSIZE); *pdep0 = pte_create(page2ppn(page), PTE_U | PTE_V); } //找到输入的虚拟地址la对应的页表项的地址(可能是刚刚分配的) return &((pte_t *)KADDR(PDE_ADDR(*pdep0)))[PTX(la)]; } Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab3/ye-mian-zhi-huan-ji-zhi.html":{"url":"lab3/ye-mian-zhi-huan-ji-zhi.html","title":"页面置换机制","keywords":"","body":"页面置换机制 结构体 现在我们来看看ucore页面置换机制的实现。 页面置换机制中， 我们需要维护一些”不在内存当中但是也许会用到“的页，它们存储在磁盘的交换区里，也有对应的虚拟地址，但是因为它们不在内存里，在页表里并没有对它们的虚拟地址进行映射。但是在发生Page Fault之后，会把访问到的页放到内存里，这时也许会把其他页扔出去，来给要用的页腾出地方。页面置换算法的核心任务，主要就是确定”把谁扔出去“。 页表里的信息只是\"当前哪些数据在内存条里以及它们物理地址和虚拟地址的对应关系\", 这里我们需要一些页表之外的数据结构来维护当前页表里没映射的页。也就是这些信息：有哪些虚拟地址对应的页当前在磁盘上，分别在磁盘上的哪个位置。有哪些虚拟地址对应的页面当前放在内存里。这两类页面（内存上的，磁盘上的）会相互转换(换入/换出内存)，所以我们将这两类页面一起维护，也就是维护”所有可用的虚拟地址/虚拟页的集合“（不论当前这个虚拟地址对应的页在内存上还是在硬盘上）。之后我们将要实现进程机制，对于不同的进程，可用的虚拟地址（虚拟页）的集合常常是不一样的，因此每个进程需要一个页表，也需要一个数据结构来维护“所有可用的虚拟地址”。 我们在vmm.h定义两个结构体 (vmm： virtural memory management)。 vma_struct结构体描述一段连续的虚拟地址，从vm_start到vm_end。 通过包含一个list_entry_t成员，我们可以把同一个页表对应的多个vma_struct结构体串成一个链表，在链表里把它们按照区间的起始点进行排序。 vm_flags表示的是一段虚拟地址对应的权限（可读，可写，可执行等），这个权限在页表项里也要进行对应的设置。 我们注意到，每个页表（每个虚拟地址空间）可能包含多个vma_struct, 也就是多个访问权限可能不同的，不相交的连续地址区间。我们用mm_struct结构体把一个页表对应的信息组合起来，包括vma_struct链表的首指针，对应的页表在内存里的指针，vma_struct链表的元素个数。 // kern/mm/vmm.h //pre define struct mm_struct; // the virtual continuous memory area(vma), [vm_start, vm_end), // addr belong to a vma means vma.vm_start 函数 create 我们需要为vma_struct和mm_struct定义和实现一些接口：包括它们的构造函数，以及如何把新的vma_struct插入到mm_struct对应的链表里。注意这两个结构体占用的内存空间需要用kmalloc()函数动态分配。 // kern/mm/vmm.c // mm_create - alloc a mm_struct & initialize it. struct mm_struct * mm_create(void) { struct mm_struct *mm = kmalloc(sizeof(struct mm_struct)); if (mm != NULL) { list_init(&(mm->mmap_list)); mm->mmap_cache = NULL; mm->pgdir = NULL; mm->map_count = 0; if (swap_init_ok) swap_init_mm(mm);//我们接下来解释页面置换的初始化 else mm->sm_priv = NULL; } return mm; } // vma_create - alloc a vma_struct & initialize it. (addr range: vm_start~vm_end) struct vma_struct * vma_create(uintptr_t vm_start, uintptr_t vm_end, uint_t vm_flags) { struct vma_struct *vma = kmalloc(sizeof(struct vma_struct)); if (vma != NULL) { vma->vm_start = vm_start; vma->vm_end = vm_end; vma->vm_flags = vm_flags; } return vma; } check_vma_overlap 在插入一个新的vma_struct之前，我们要保证它和原有的区间都不重合。 // kern/mm/vmm.c // check_vma_overlap - check if vma1 overlaps vma2 ? static inline void check_vma_overlap(struct vma_struct *prev, struct vma_struct *next) { assert(prev->vm_start vm_end); assert(prev->vm_end vm_start); assert(next->vm_start vm_end);// next 是我们想插入的区间，这里顺便检验了start insert_vma_struct&find_vma 我们可以插入一个新的vma_struct, 也可以查找某个虚拟地址对应的vma_struct是否存在 // kern/mm/vmm.c // insert_vma_struct -insert vma in mm's list link void insert_vma_struct(struct mm_struct *mm, struct vma_struct *vma) { assert(vma->vm_start vm_end); list_entry_t *list = &(mm->mmap_list); list_entry_t *le_prev = list, *le_next; list_entry_t *le = list; while ((le = list_next(le)) != list) { struct vma_struct *mmap_prev = le2vma(le, list_link); if (mmap_prev->vm_start > vma->vm_start) { break; } le_prev = le; } //保证插入后所有vma_struct按照区间左端点有序排列 le_next = list_next(le_prev); /* check overlap */ if (le_prev != list) { check_vma_overlap(le2vma(le_prev, list_link), vma); } if (le_next != list) { check_vma_overlap(vma, le2vma(le_next, list_link)); } vma->vm_mm = mm; list_add_after(le_prev, &(vma->list_link)); mm->map_count ++;//计数器 } // find_vma - find a vma (vma->vm_start mmap_cache; if (!(vma != NULL && vma->vm_start vm_end > addr)) { bool found = 0; list_entry_t *list = &(mm->mmap_list), *le = list; while ((le = list_next(le)) != list) { vma = le2vma(le, list_link); if (vma->vm_startvm_end) { found = 1; break; } } if (!found) { vma = NULL; } } if (vma != NULL) { mm->mmap_cache = vma; } } return vma; } pgfault_handler 考虑当发生Page Fault的时候我们怎么做。回顾异常处理程序，我们的trapFrame传递了badvaddr给do_pgfault()函数。这实际上是stval这个寄存器的数值（在旧版的RISCV标准里叫做sbadvaddr)，这个寄存器存储一些关于异常的数据，对于PageFault它存储的是访问出错的虚拟地址。 // kern/trap/trap.c static int pgfault_handler(struct trapframe *tf) { extern struct mm_struct *check_mm_struct;//当前使用的mm_struct的指针，在vmm.c定义 print_pgfault(tf); if (check_mm_struct != NULL) { return do_pgfault(check_mm_struct, tf->cause, tf->badvaddr); } panic(\"unhandled page fault.\\n\"); } // kern/mm/vmm.c struct mm_struct *check_mm_struct; // check_pgfault - check correctness of pgfault handler static void check_pgfault(void) { /* ...... */ check_mm_struct = mm_create(); /* ...... */ } do_pgfault() do_pgfault()函数在vmm.c定义，是页面置换机制的核心。如果可行，我们要对页表做对应的修改，加入对应的页表项，并把硬盘上的数据换进内存。这时可能要把内存里的一个页换出去。 // kern/mm/vmm.c int do_pgfault(struct mm_struct *mm, uint_t error_code, uintptr_t addr) { //addr: 访问出错的虚拟地址 int ret = -E_INVAL; //try to find a vma which include addr struct vma_struct *vma = find_vma(mm, addr); //我们首先要做的就是在mm_struct里判断这个虚拟地址是否可用 pgfault_num++; //If the addr is not in the range of a mm's vma? if (vma == NULL || vma->vm_start > addr) { cprintf(\"not valid addr %x, and can not find it in vma\\n\", addr); goto failed; } /* IF (write an existed addr ) OR * (write an non_existed addr && addr is writable) OR * (read an non_existed addr && addr is readable) * THEN * continue process */ uint32_t perm = PTE_U; if (vma->vm_flags & VM_WRITE) { perm |= (PTE_R | PTE_W); } addr = ROUNDDOWN(addr, PGSIZE); //按照页面大小把地址对齐 ret = -E_NO_MEM; pte_t *ptep=NULL; ptep = get_pte(mm->pgdir, addr, 1); //(1) try to find a pte, if pte's //PT(Page Table) isn't existed, then //create a PT. if (*ptep == 0) { if (pgdir_alloc_page(mm->pgdir, addr, perm) == NULL) { cprintf(\"pgdir_alloc_page in do_pgfault failed\\n\"); goto failed; } } else { /* * Now we think this pte is a swap entry, we should load data from disk * to a page with phy addr, * and map the phy addr with logical addr, trigger swap manager to record * the access situation of this page. * * swap_in(mm, addr, &page) : alloc a memory page, then according to * the swap entry in PTE for addr, find the addr of disk page, read the * content of disk page into this memroy page * page_insert ： build the map of phy addr of an Page with the virtual addr la * swap_map_swappable ： set the page swappable */ if (swap_init_ok) { struct Page *page = NULL; //在swap_in()函数执行完之后，page保存换入的物理页面。 //swap_in()函数里面可能把内存里原有的页面换出去 swap_in(mm, addr, &page); //(1）According to the mm AND addr, try //to load the content of right disk page //into the memory which page managed. page_insert(mm->pgdir, page, addr, perm); //更新页表，插入新的页表项 //(2) According to the mm, addr AND page, // setup the map of phy addr virtual addr swap_map_swappable(mm, addr, page, 1); //(3) make the page swappable. //标记这个页面将来是可以再换出的 page->pra_vaddr = addr; } else { cprintf(\"no swap_init_ok but ptep is %x, failed\\n\", *ptep); goto failed; } } ret = 0; failed: return ret; } Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab3/fifo-zhi-huan-suan-fa.html":{"url":"lab3/fifo-zhi-huan-suan-fa.html","title":"FIFO置换算法","keywords":"","body":"FIFO置换算法 所谓FIFO(First in, First out)页面置换算法，相当简单，就是把所有页面排在一个队列里，每次换入页面的时候，把队列里最靠前（最早被换入）的页面置换出去。 换出页面的时机相对复杂一些，针对不同的策略有不同的时机。ucore 目前大致有两种策略，即积极换出策略和消极换出策略。积极换出策略是指操作系统周期性地（或在系统不忙的时候）主动把某些认为“不常用”的页换出到硬盘上，从而确保系统中总有一定数量的空闲页存在，这样当需要空闲页时，基本上能够及时满足需求；消极换出策略是指，只是当试图得到空闲页时，发现当前没有空闲的物理页可供分配，这时才开始查找“不常用”页面，并把一个或多个这样的页换出到硬盘上。 alloc_pages 目前的框架支持第二种情况，在alloc_pages()里面，没有空闲的物理页时，尝试换出页面到硬盘上。 // kern/mm/pmm.c // alloc_pages - call pmm->alloc_pages to allocate a continuous n*PAGESIZE memory struct Page *alloc_pages(size_t n) { struct Page *page = NULL; bool intr_flag; while (1) { local_intr_save(intr_flag); { page = pmm_manager->alloc_pages(n); } local_intr_restore(intr_flag); //如果有足够的物理页面，就不必换出其他页面 //如果n>1, 说明希望分配多个连续的页面，但是我们换出页面的时候并不能换出连续的页面 //swap_init_ok标志是否成功初始化了 if (page != NULL || n > 1 || swap_init_ok == 0) break; extern struct mm_struct *check_mm_struct; swap_out(check_mm_struct, n, 0);//调用页面置换的”换出页面“接口。这里必有n=1 } return page; } swap_manager 类似pmm_manager, 我们定义swap_manager, 组合页面置换需要的一些函数接口。 // kern/mm/swap.h struct swap_manager { const char *name; /* Global initialization for the swap manager */ int (*init) (void); /* Initialize the priv data inside mm_struct */ int (*init_mm) (struct mm_struct *mm); /* Called when tick interrupt occured */ int (*tick_event) (struct mm_struct *mm); /* Called when map a swappable page into the mm_struct */ int (*map_swappable) (struct mm_struct *mm, uintptr_t addr, struct Page *page, int swap_in); /* When a page is marked as shared, this routine is called to * delete the addr entry from the swap manager */ int (*set_unswappable) (struct mm_struct *mm, uintptr_t addr); /* Try to swap out a page, return then victim */ int (*swap_out_victim) (struct mm_struct *mm, struct Page **ptr_page, int in_tick); /* check the page relpacement algorithm */ int (*check_swap)(void); }; swap_in/out 我们来看swap_in(), swap_out()如何换入/换出一个页面.注意我们对物理页面的 Page结构体做了一些改动。 // kern/mm/memlayout.h struct Page { int ref; // page frame's reference counter uint_t flags; // array of flags that describe the status of the page frame unsigned int property; // the num of free block, used in first fit pm manager list_entry_t page_link; // free list link list_entry_t pra_page_link; // used for pra (page replace algorithm) uintptr_t pra_vaddr; // used for pra (page replace algorithm) }; // kern/mm/swap.c int swap_in(struct mm_struct *mm, uintptr_t addr, struct Page **ptr_result) { struct Page *result = alloc_page();//这里alloc_page()内部可能调用swap_out() //找到对应的一个物理页面 assert(result!=NULL); pte_t *ptep = get_pte(mm->pgdir, addr, 0);//找到/构建对应的页表项 //将物理地址映射到虚拟地址是在swap_in()退出之后，调用page_insert()完成的 int r; if ((r = swapfs_read((*ptep), result)) != 0)//将数据从硬盘读到内存 { assert(r!=0); } cprintf(\"swap_in: load disk swap entry %d with swap_page in vadr 0x%x\\n\", (*ptep)>>8, addr); *ptr_result=result; return 0; } int swap_out(struct mm_struct *mm, int n, int in_tick) { int i; for (i = 0; i != n; ++ i) { uintptr_t v; struct Page *page; int r = sm->swap_out_victim(mm, &page, in_tick);//调用页面置换算法的接口 //r=0表示成功找到了可以换出去的页面 //要换出去的物理页面存在page里 if (r != 0) { cprintf(\"i %d, swap_out: call swap_out_victim failed\\n\",i); break; } cprintf(\"SWAP: choose victim page 0x%08x\\n\", page); v=page->pra_vaddr;//可以获取物理页面对应的虚拟地址 pte_t *ptep = get_pte(mm->pgdir, v, 0); assert((*ptep & PTE_V) != 0); if (swapfs_write( (page->pra_vaddr/PGSIZE+1)map_swappable(mm, v, page, 0); continue; } else { //成功换出 cprintf(\"swap_out: i %d, store page in vaddr 0x%x to disk swap entry %d\\n\", i, v, page->pra_vaddr/PGSIZE+1); *ptep = (page->pra_vaddr/PGSIZE+1)pgdir, v); } return i; } swap_init kern/mm/swap.c里其他的接口基本都是简单调用swap_manager的具体实现。值得一提的是swap_init()初始化里做的工作。 // kern/mm/swap.c static struct swap_manager *sm; int swap_init(void) { swapfs_init(); // Since the IDE is faked, it can only store 7 pages at most to pass the test if (!(7 init(); if (r == 0) { swap_init_ok = 1; cprintf(\"SWAP: manager = %s\\n\", sm->name); check_swap(); } return r; } int swap_init_mm(struct mm_struct *mm) { return sm->init_mm(mm); } int swap_tick_event(struct mm_struct *mm) { return sm->tick_event(mm); } int swap_map_swappable(struct mm_struct *mm, uintptr_t addr, struct Page *page, int swap_in) { return sm->map_swappable(mm, addr, page, swap_in); } int swap_set_unswappable(struct mm_struct *mm, uintptr_t addr) { return sm->set_unswappable(mm, addr); } swap_fifo.h kern/mm/swap_fifo.h完成了FIFO置换算法最终的具体实现。我们所做的就是维护了一个队列（用链表实现）。 // kern/mm/swap_fifo.h #ifndef __KERN_MM_SWAP_FIFO_H__ #define __KERN_MM_SWAP_FIFO_H__ #include extern struct swap_manager swap_manager_fifo; #endif // kern/mm/swap_fifo.c /* Details of FIFO PRA * (1) Prepare: In order to implement FIFO PRA, we should manage all swappable pages, so we can * link these pages into pra_list_head according the time order. At first you should * be familiar to the struct list in list.h. struct list is a simple doubly linked list * implementation. You should know howto USE: list_init, list_add(list_add_after), * list_add_before, list_del, list_next, list_prev. Another tricky method is to transform * a general list struct to a special struct (such as struct page). You can find some MACRO: * le2page (in memlayout.h), (in future labs: le2vma (in vmm.h), le2proc (in proc.h),etc. */ list_entry_t pra_list_head; /* * (2) _fifo_init_mm: init pra_list_head and let mm->sm_priv point to the addr of pra_list_head. * Now, From the memory control struct mm_struct, we can access FIFO PRA */ static int _fifo_init_mm(struct mm_struct *mm) { list_init(&pra_list_head); mm->sm_priv = &pra_list_head; return 0; } /* * (3)_fifo_map_swappable: According FIFO PRA, we should link the most recent arrival page at the back of pra_list_head qeueue */ static int _fifo_map_swappable(struct mm_struct *mm, uintptr_t addr, struct Page *page, int swap_in) { list_entry_t *head=(list_entry_t*) mm->sm_priv; list_entry_t *entry=&(page->pra_page_link); assert(entry != NULL && head != NULL); //record the page access situlation //(1)link the most recent arrival page at the back of the pra_list_head qeueue. list_add(head, entry); return 0; } /* * (4)_fifo_swap_out_victim: According FIFO PRA, we should unlink the earliest arrival page in front of pra_list_head qeueue, * then set the addr of addr of this page to ptr_page. */ static int _fifo_swap_out_victim(struct mm_struct *mm, struct Page ** ptr_page, int in_tick) { list_entry_t *head=(list_entry_t*) mm->sm_priv; assert(head != NULL); assert(in_tick==0); /* Select the victim */ //(1) unlink the earliest arrival page in front of pra_list_head qeueue //(2) set the addr of addr of this page to ptr_page list_entry_t* entry = list_prev(head); if (entry != head) { list_del(entry); *ptr_page = le2page(entry, pra_page_link); } else { *ptr_page = NULL; } return 0; } static int _fifo_init(void)//初始化的时候什么都不做 { return 0; } static int _fifo_set_unswappable(struct mm_struct *mm, uintptr_t addr) { return 0; } static int _fifo_tick_event(struct mm_struct *mm)//时钟中断的时候什么都不做 { return 0; } struct swap_manager swap_manager_fifo = { .name = \"fifo swap manager\", .init = &_fifo_init, .init_mm = &_fifo_init_mm, .tick_event = &_fifo_tick_event, .map_swappable = &_fifo_map_swappable, .set_unswappable = &_fifo_set_unswappable, .swap_out_victim = &_fifo_swap_out_victim, .check_swap = &_fifo_check_swap, }; 我们通过_fifo_check_swap(), check_swap(), check_vma_struct(), check_pgfault()等接口对页面置换机制进行了简单的测试。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab3/xiang-mu-zu-cheng-yu-zhi-hang-liu.html":{"url":"lab3/xiang-mu-zu-cheng-yu-zhi-hang-liu.html","title":"项目组成与执行流","keywords":"","body":"项目组成与执行流 项目组成 lab3 ├── Makefile ├── kern │ ├── debug │ │ ├── assert.h │ │ ├── kdebug.c │ │ ├── kdebug.h │ │ ├── kmonitor.c │ │ ├── kmonitor.h │ │ ├── panic.c │ │ └── stab.h │ ├── driver │ │ ├── clock.c │ │ ├── clock.h │ │ ├── console.c │ │ ├── console.h │ │ ├── ide.c │ │ ├── ide.h │ │ ├── intr.c │ │ └── intr.h │ ├── fs │ │ ├── fs.h │ │ ├── swapfs.c │ │ └── swapfs.h │ ├── init │ │ ├── entry.S │ │ └── init.c │ ├── libs │ │ └── stdio.c │ ├── mm │ │ ├── default_pmm.c │ │ ├── default_pmm.h │ │ ├── memlayout.h │ │ ├── mmu.h │ │ ├── pmm.c │ │ ├── pmm.h │ │ ├── swap.c │ │ ├── swap.h │ │ ├── swap_clock.c │ │ ├── swap_clock.h │ │ ├── swap_fifo.c │ │ ├── swap_fifo.h │ │ ├── vmm.c │ │ └── vmm.h │ ├── sync │ │ └── sync.h │ └── trap │ ├── trap.c │ ├── trap.h │ └── trapentry.S ├── lab3.md ├── libs │ ├── atomic.h │ ├── defs.h │ ├── error.h │ ├── list.h │ ├── printfmt.c │ ├── rand.c │ ├── readline.c │ ├── riscv.h │ ├── sbi.h │ ├── stdarg.h │ ├── stdio.h │ ├── stdlib.h │ ├── string.c │ └── string.h └── tools ├── boot.ld ├── function.mk ├── gdbinit ├── grade.sh ├── kernel.ld ├── sign.c └── vector.c 11 directories, 62 files 页面定义 kern/mm/memlayout.h：修改了struct Page，增加了两项pra_*成员结构，其中pra_page_link可以用来建立描述各个页访问情况（比如根据访问先后）的链表。在本实验中会涉及使用这两个成员结构，以及le2page等宏。 虚拟内存信息 kern/mm/vmm.[ch]：vmm.h描述了mm_struct，vma_struct等表述可访问的虚存地址访问的一些信息，下面会进一步详细讲解。vmm.c涉及mm,vma结构数据的创建/销毁/查找/插入等函数，这些函数在check_vma、check_vmm等中被使用，理解即可。而page fault处理相关的do_pgfault函数是本次实验需要涉及完成的。 替换算法框架 kern/mm/swap.[ch]：定义了实现页替换算法类框架struct swap_manager。swap.c包含了对此页替换算法类框架的初始化、页换入/换出等各种函数实现。重点是要理解何时调用swap_out和swap_in函数。和如何在此框架下连接具体的页替换算法实现。 FIFO算法 kern/mm/swap_fifo.[ch]：演示的算法实现。 Clock算法 kern/mm/swap_clock.[ch]：需要自己实现的算法，有注释提示。 执行流 结合前面所述自行理解、总结。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab4/":{"url":"lab4/","title":"LAB4：进程管理","keywords":"","body":"LAB4：进程管理 在lab2和lab3中，我们已经将物理内存纳入了掌控，并且实现了虚拟内存的机制，使得我们可以建立一些真正操作系统级别的抽象。在本章和下一章当中，我们要实现操作系统当中非常重要的一个部分：进程管理。我们主要分成了两个部分来实现，在本章中我们会实现内核进程的管理，在下一章再实现用户进程的管理。 内核进程和用户进程有什么区别呢？首先，内核进程运行于内核态，而用户进程一般处于用户态，只有在需要系统调用时才会进入内核态。其次，内核进程不需要很复杂的内存管理，共用整个内核内存空间。这是因为内核进程往往用来完成很多和操作系统有关的任务，操作系统应当信任内核进程；而用户进程由用户提供，为了避免恶意的用户影响操作系统以及其他进程的运行状态，需要对于地址空间进行隔离。 本次实验主要介绍ucore如何实现内核进程。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab4/shi-yan-mu-de.html":{"url":"lab4/shi-yan-mu-de.html","title":"实验目的","keywords":"","body":"实验目的 了解内核进程创建、执行的管理过程 了解内核线程的切换和基本调度过程 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab4/shi-yan-nei-rong.html":{"url":"lab4/shi-yan-nei-rong.html","title":"实验内容","keywords":"","body":"实验内容 阅读线程、进程相关知识。 阅读框架代码，深刻理解内核进程创建、切换过程。 自己动手参与创建一个内核线程。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab4/lian-xi.html":{"url":"lab4/lian-xi.html","title":"练习","keywords":"","body":"练习 练习1：分配并初始化一个进程控制块 alloc_proc 函数（位于 kern/process/proc.c 中）负责分配并返回一个新的struct proc_struct结构，用于存储新建立的内核线程的管理信息。ucore 需要对这个结构进行最基本的初始化，你需要完成这个初始化过程。 [!TIP|style:flat] 在alloc_proc函数的实现中，需要初始化的 proc_struct 结构中的成员变量至少包括：state/pid/runs/kstack/need_resched/parent/mm/context/tf/cr3/flags/name。 请在实验报告中简要说明你的设计实现过程。并回答如下问题： 请说明 proc_struct 中struct context context和struct trapframe *tf成员变量含义和在本实验中的作用是啥？（提示：通过看代码和编程调试可以判断出来） 练习2：为新创建的内核线程分配资源 创建一个内核线程需要分配和设置好很多资源。kernel_thread 函数通过调用do_fork函数完成具体内核线程的创建工作。do_kernel 函数会调用 alloc_proc 函数来分配并初始化一个进程控制块，但 alloc_proc只是找到了一小块内存用以记录进程的必要信息，并没有实际分配这些资源。ucore 一般通过 do_fork实际创建新的内核线程。do_fork 的作用是，创建当前内核线程的一个副本，它们的执行上下文、代码、数据都一样，但是存储位置不同。在这个过程中，需要给新内核线程分配资源，并且复制原进程的状态。你需要完成在 kern/process/proc.c中的 do_fork函数中的处理过程。它的大致执行步骤包括： 调用alloc_proc，首先获得一块用户信息块。 为进程分配一个内核栈。 复制原进程的内存管理信息到新进程（但内核线程不必做此事） 复制原进程上下文到新进程 将新进程添加到进程列表 唤醒新进程 返回新进程号 请在实验报告中简要说明你的设计实现过程。请回答如下问题： 请说明 ucore 是否做到给每个新 fork 的线程一个唯一的 id？请说明你的分析和理由。 练习3：理解proc_run函数和调用的函数如何完成进程切换的 请在实验报告中简要说明你对 proc_run 函数的分析。并回答如下问题： 在本实验的执行过程中，创建且运行了几个内核线程？ 语句local_intr_save(intr_flag);....local_intr_restore(intr_flag);在这里有何作用?请说明理由 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab4/jin-cheng-yu-xian-cheng.html":{"url":"lab4/jin-cheng-yu-xian-cheng.html","title":"进程与线程","keywords":"","body":"进程与线程 进程与线程 在操作系统中，我们经常谈到的两个概念就是进程与线程的概念。这两个概念虽然有许多相似的地方，但也有很多的不同。 我们平时编写的源代码，经过编译器编译就变成了可执行文件，我们管这一类文件叫做程序。而当一个程序被用户或操作系统启动，分配资源，装载进内存开始执行后，它就成为了一个进程。进程与程序之间最大的不同在于进程是一个“正在运行”的实体，而程序只是一个不动的文件。进程包含程序的内容，也就是它的静态的代码部分，也包括一些在运行时在可以体现出来的信息，比如堆栈，寄存器等数据，这些组成了进程“正在运行”的特性。 如果我们只关注于那些“正在运行”的部分，我们就从进程当中剥离出来了线程。一个进程可以对应一个线程，也可以对应很多线程。这些线程之间往往具有相同的代码，共享一块内存，但是却有不同的CPU执行状态。相比于线程，进程更多的作为一个资源管理的实体（因为操作系统分配网络等资源时往往是基于进程的），这样线程就作为可以被调度的最小单元，给了调度器更多的调度可能。 为什么需要进程 进程的一个重要特点在于其可以调度。在我们操作系统启动的时候，操作系统相当是一个初始的进程。之后，操作系统会创建不同的进程负责不同的任务。用户可以通过命令行启动进程，从而使用计算机。想想如果没有进程会怎么样？所有的代码可能需要在操作系统编译的时候就打包在一块，安装软件将变成一件非常难的事情，这显然对于用户使用计算机是不利的。 另一方面，从2000年开始，CPU越来越多的使用多核心的设计。这主要是因为芯片设计师们发现在一个核心上提高主频变得越来越难（这其中有许多原因，相信组成原理课上已经有过介绍），所以采用多个核心，将利用多核性能的任务交给了程序员。在这种环境下，操作系统也需要进行相应的调整，以适应这种多核的趋势。使用进程的概念有助于各个进程同时的利用CPU的各个核心，这是单进程系统往往做不到的。 但是，多进程的引入其实远早于多核心处理器。在计算机的远古时代，存在许多“巨无霸”计算机。但是，如果只让这些计算机服务于一个用户，有时候又有点浪费。有没有可能让一个计算机服务于多个用户呢（哪怕只有一个核心）？分时操作系统解决了这个问题，就是通过时间片轮转的方法使得多个用户可以“同时”使用计算资源。这个时候，引入进程的概念，成为操作系统调度的单元就显得十分必要了。 综合以上可以看出，操作系统的确离不开进程管理。在下一节，我们会介绍ucore中与进程相关的数据结构，看一看如果定义一个进程。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab4/xiang-guan-jie-gou-ti.html":{"url":"lab4/xiang-guan-jie-gou-ti.html","title":"相关结构体","keywords":"","body":"相关结构体 在实现和进程相关的代码之前，我们先来实现一些数据结构来帮助我们对于进程进行管理。在本章实现的进程管理模型中，我们主要维护两个数据结构：进程控制块和进程上下文。进程控制块维护进程的各个信息，包括内存映射，进程名等等。进程上下文里面保存了和进程运行状态相关的各个寄存器的值，这些是为了之后恢复进程运行状态用的。下面我们来看一看这两个数据结构。 进程控制块 我们在ucore中使用结构体struct proc_struct来保存和进程相关的控制信息。 struct proc_struct内部结构如下： struct proc_struct { enum proc_state state; // Process state int pid; // Process ID int runs; // the running times of Proces uintptr_t kstack; // Process kernel stack volatile bool need_resched; // bool value: need to be rescheduled to release CPU? struct proc_struct *parent; // the parent process struct mm_struct *mm; // Process's memory management field struct context context; // Switch here to run process struct trapframe *tf; // Trap frame for current interrupt uintptr_t cr3; // CR3 register: the base addr of Page Directroy Table(PDT) uint32_t flags; // Process flag char name[PROC_NAME_LEN + 1]; // Process name list_entry_t list_link; // Process link list list_entry_t hash_link; // Process hash list }; 这里面值得我们关注的主要有以下几个成员变量： parent：里面保存了进程的父进程的指针。在内核中，只有内核创建的idle进程没有父进程，其他进程都有父进程。进程的父子关系组成了一棵进程树，这种父子关系有利于维护父进程对于子进程的一些特殊操作。 mm：这里面保存了内存管理的信息，包括内存映射，虚存管理等内容。具体内在实现可以参考之前的章节。 context：context中保存了进程执行的上下文，也就是几个关键的寄存器的值。这些寄存器的值用于在进程切换中还原之前进程的运行状态（进程切换的详细过程在后面会介绍）。切换过程的实现在kern/process/switch.S。 tf：tf里保存了进程的中断帧。当进程从用户空间跳进内核空间的时候，进程的执行状态被保存在了中断帧中（注意这里需要保存的执行状态数量不同于上下文切换）。系统调用可能会改变用户寄存器的值，我们可以通过调整中断帧来使得系统调用返回特定的值。 cr3：cr3寄存器是x86架构的特殊寄存器，用来保存页表所在的基址。出于legacy的原因，我们这里仍然保留了这个名字，但其值仍然是页表基址所在的位置。 进程上下文 进程上下文使用结构体struct context保存，其中包含了ra，sp，s0~s11共14个寄存器。 可能感兴趣的同学就会问了，为什么我们不需要保存所有的寄存器呢？这里我们巧妙地利用了编译器对于函数的处理。我们知道寄存器可以分为调用者保存（caller-saved）寄存器和被调用者保存（callee-saved）寄存器。因为线程切换在一个函数当中（我们下一小节就会看到），所以编译器会自动帮助我们生成保存和恢复调用者保存寄存器的代码，在实际的进程切换过程中我们只需要保存被调用者保存寄存器就好啦！ Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab4/jin-cheng-mo-kuai-chu-shi-hua.html":{"url":"lab4/jin-cheng-mo-kuai-chu-shi-hua.html","title":"进程模块初始化","keywords":"","body":"进程模块初始化 创建idle进程 进程初始化的函数定义在文件kern/process/proc.c中的proc_init。进程模块的初始化主要分为两步，首先创建第0个内核进程，idle。 // kern/process/proc.c // proc_init - set up the first kernel thread idleproc \"idle\" by itself and // - create the second kernel thread init_main void proc_init(void) { int i; list_init(&proc_list);//进程链表 for (i = 0; i pid = 0; idleproc->state = PROC_RUNNABLE; idleproc->kstack = (uintptr_t)bootstack; idleproc->need_resched = 1; set_proc_name(idleproc, \"idle\"); nr_process ++; //全局变量current保存当前正在执行的进程 current = idleproc; int pid = kernel_thread(init_main, \"Hello world!!\", 0); if (pid pid == 0); assert(initproc != NULL && initproc->pid == 1); } 在进程模块初始化时，首先需要初始化进程链表。进程链表就是把所有进程控制块串联起来的数据结构，可以记录和追踪每一个进程。然后，调用proc_alloc函数来为第一个进程分配其进程控制块。当我们的操作系统开始运行的时候，其实它已经可以被视作一个进程了。但是我们还没有为他设计好进程控制块，也就没法进行管理。proc_alloc函数会使用kmalloc分配一段空间来保存进程控制块，并且设定一些初值告诉我们这个进程目前还在初始化中。 在分配完空间后，我们对于idle进程的控制块进行一定的初始化： idleproc->pid = 0; idleproc->state = PROC_RUNNABLE; idleproc->kstack = (uintptr_t)bootstack; idleproc->need_resched = 1; set_proc_name(idleproc, \"idle\"); nr_process ++; 从这里开始，idle进程具有了合法的进程编号，0。我们把idle进程的状态设置为RUNNABLE，表示其可以执行。因为这是第一个内核进程，所以我们可以直接将ucore的启动栈分配给他。需要注意的是，后面再分配新进程时我们需要为其分配一个栈，而不能再使用启动栈了。我们再把idle进程标志为需要调度，这样一旦idle进程开始执行，马上就可以让调度器调度另一个进程进行执行。 创建内核进程 接下来我们对于第一个真正的内核进程进行初始化（因为idle进程仅仅算是“继承了”ucore的运行）。我们的目标是使用新的内核进程进行一下内核初始化的工作，但在这章我们先仅仅让它输出一个Hello World，证明我们的内核进程实现的没有问题。下面是创建内核进程的代码： int kernel_thread(int (*fn)(void *), void *arg, uint32_t clone_flags) { struct trapframe tf; memset(&tf, 0, sizeof(struct trapframe)); tf.gpr.s0 = (uintptr_t)fn; tf.gpr.s1 = (uintptr_t)arg; tf.status = (read_csr(sstatus) | SSTATUS_SPP | SSTATUS_SPIE) & ~SSTATUS_SIE; tf.epc = (uintptr_t)kernel_thread_entry; return do_fork(clone_flags | CLONE_VM, 0, &tf); } 我们将寄存器s0和s1分别设置为需要进程执行的函数和相关参数列表，之后设置了status寄存器使得进程切换后处于中断使能的状态。我们还设置了epc使其指向kernel_thread_entry，这是进程执行的入口函数。最后，调用do_fork函数把当前的进程复制一份。 do_fork函数内部主要进行了如下操作： 分配并初始化进程控制块（alloc_proc函数） 分配并初始化内核栈（setup_stack函数） 根据clone_flags决定是复制还是共享内存管理系统（copy_mm函数） 设置进程的中断帧和上下文（copy_thread函数） 把设置好的进程加入链表 将新建的进程设为就绪态 将返回值设为线程id 如果执行失败，则需要调用相应的错误处理函数释放空间。更多的实现细节可以参考代码，在练习中也会有更多的涉及。 在这里我们需要尤其关注copy_thread函数： static void copy_thread(struct proc_struct *proc, uintptr_t esp, struct trapframe *tf) { proc->tf = (struct trapframe *)(proc->kstack + KSTACKSIZE - sizeof(struct trapframe)); *(proc->tf) = *tf; // Set a0 to 0 so a child process knows it's just forked proc->tf->gpr.a0 = 0; proc->tf->gpr.sp = (esp == 0) ? (uintptr_t)proc->tf : esp; proc->context.ra = (uintptr_t)forkret; proc->context.sp = (uintptr_t)(proc->tf); } 在这里我们首先在上面分配的内核栈上分配出一片空间来保存trapframe。然后，我们将trapframe中的a0寄存器（返回值）设置为0，说明这个进程是一个子进程。之后我们将上下文中的ra设置为了forkret函数的入口，并且把trapframe放在上下文的栈顶。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab4/jin-cheng-qie-huan.html":{"url":"lab4/jin-cheng-qie-huan.html","title":"进程切换","keywords":"","body":"进程切换 在ucore完成初始化后，ucore内核就没事做了，于是进入了“idle”的状态（这也是我们给第0个进程起名叫idle的原因）。这是，它会陷入死循环，不断检查自己是否需要调度： void cpu_idle(void) { while (1) { if (current->need_resched) { schedule(); ...... 因为我们之前在初始化中把idle进程的need_resched设为了1，所以其总会调用schedule函数来检查是否有进程可以调度。我们已经初始化了另外一个内核进程，所以调度器发现了这个进程，并且准备调度到这个进程。 我们实现的schedule函数非常的简单：当需要调度的时候，把当前的进程放在队尾，从队列中取出第一个可以运行的进程，切换到它运行。这就是FIFO调度算法。schedule函数会调用proc_run来唤醒选定的进程。proc_run函数内部如下： void proc_run(struct proc_struct *proc) { if (proc != current) { bool intr_flag; struct proc_struct *prev = current, *next = proc; local_intr_save(intr_flag); { current = proc; lcr3(next->cr3); switch_to(&(prev->context), &(next->context)); } local_intr_restore(intr_flag); } } 函数中主要进行了三个操作： 将当前运行的进程设置为要切换过去的进程 将页表换成新进程的页表 使用switch_to切换到新进程 switch_to函数如下： .text # void switch_to(struct proc_struct* from, struct proc_struct* to) .globl switch_to switch_to: # save from's registers STORE ra, 0*REGBYTES(a0) STORE sp, 1*REGBYTES(a0) STORE s0, 2*REGBYTES(a0) STORE s1, 3*REGBYTES(a0) STORE s2, 4*REGBYTES(a0) STORE s3, 5*REGBYTES(a0) STORE s4, 6*REGBYTES(a0) STORE s5, 7*REGBYTES(a0) STORE s6, 8*REGBYTES(a0) STORE s7, 9*REGBYTES(a0) STORE s8, 10*REGBYTES(a0) STORE s9, 11*REGBYTES(a0) STORE s10, 12*REGBYTES(a0) STORE s11, 13*REGBYTES(a0) # restore to's registers LOAD ra, 0*REGBYTES(a1) LOAD sp, 1*REGBYTES(a1) LOAD s0, 2*REGBYTES(a1) LOAD s1, 3*REGBYTES(a1) LOAD s2, 4*REGBYTES(a1) LOAD s3, 5*REGBYTES(a1) LOAD s4, 6*REGBYTES(a1) LOAD s5, 7*REGBYTES(a1) LOAD s6, 8*REGBYTES(a1) LOAD s7, 9*REGBYTES(a1) LOAD s8, 10*REGBYTES(a1) LOAD s9, 11*REGBYTES(a1) LOAD s10, 12*REGBYTES(a1) LOAD s11, 13*REGBYTES(a1) ret 可以看出来这段代码就是将需要保存的寄存器进行保存和调换。在之前我们也已经谈到过了，这里只需要调换被调用者保存寄存器即可。由于我们在初始化时把上下文的ra寄存器设定成了forkret函数的入口，所以这里会返回到forkret函数，进一步进入到forkrets。forkrets函数很短： .globl forkrets forkrets: # set stack to this new process's trapframe move sp, a0 j __trapret 这里把传进来的参数，也就是进程的中断帧放在了sp，这样在__trapret中就可以直接从中断帧里面恢复所有的寄存器啦！我们在初始化的时候对于中断帧做了一点手脚，epc寄存器指向的是kernel_thread_entry，s0寄存器里放的是新进程要执行的函数，s1寄存器里放的是传给函数的参数。在kernel_thread_entry函数中： .text .globl kernel_thread_entry kernel_thread_entry: # void kernel_thread(void) move a0, s1 jalr s0 jal do_exit 我们把参数放在了a0寄存器，并跳转到s0执行我们指定的函数！这样，一个进程的初始化就完成了。至此，我们实现了基本的进程管理，并且成功创建并切换到了我们的第一个内核进程。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab4/xiang-mu-zu-cheng-yu-zhi-hang-liu.html":{"url":"lab4/xiang-mu-zu-cheng-yu-zhi-hang-liu.html","title":"项目组成与执行流","keywords":"","body":"项目组成与执行流 项目组成 lab4 ├── Makefile ├── kern │ ├── debug │ │ ├── assert.h │ │ ├── kdebug.c │ │ ├── kdebug.h │ │ ├── kmonitor.c │ │ ├── kmonitor.h │ │ ├── panic.c │ │ └── stab.h │ ├── driver │ │ ├── clock.c │ │ ├── clock.h │ │ ├── console.c │ │ ├── console.h │ │ ├── ide.c │ │ ├── ide.h │ │ ├── intr.c │ │ ├── intr.h │ │ ├── kbdreg.h │ │ ├── picirq.c │ │ └── picirq.h │ ├── fs │ │ ├── fs.h │ │ ├── swapfs.c │ │ └── swapfs.h │ ├── init │ │ ├── entry.S │ │ └── init.c │ ├── libs │ │ ├── readline.c │ │ └── stdio.c │ ├── mm │ │ ├── default_pmm.c │ │ ├── default_pmm.h │ │ ├── kmalloc.c │ │ ├── kmalloc.h │ │ ├── memlayout.h │ │ ├── mmu.h │ │ ├── pmm.c │ │ ├── pmm.h │ │ ├── swap.c │ │ ├── swap.h │ │ ├── swap_fifo.c │ │ ├── swap_fifo.h │ │ ├── vmm.c │ │ └── vmm.h │ ├── process │ │ ├── entry.S │ │ ├── proc.c │ │ ├── proc.h │ │ └── switch.S │ ├── schedule │ │ ├── sched.c │ │ └── sched.h │ ├── sync │ │ └── sync.h │ └── trap │ ├── trap.c │ ├── trap.h │ └── trapentry.S ├── lab4.md ├── libs │ ├── atomic.h │ ├── defs.h │ ├── elf.h │ ├── error.h │ ├── hash.c │ ├── list.h │ ├── printfmt.c │ ├── rand.c │ ├── riscv.h │ ├── sbi.h │ ├── stdarg.h │ ├── stdio.h │ ├── stdlib.h │ ├── string.c │ └── string.h └── tools ├── boot.ld ├── function.mk ├── gdbinit ├── grade.sh ├── kernel.ld ├── sign.c └── vector.c 13 directories, 73 files 进程管理 kern/process/proc.[ch]：新增：实现进程、线程相关功能，包括：创建进程/线程，初始化进程/线程，处理进程/线程退出等功能 kern/process/entry.S：新增：内核线程入口函数kernel_thread_entry的实现 kern/process/switch.S：新增：上下文切换，利用堆栈保存、恢复进程上下文 进程系统初始化 kern/init/init.c：修改：完成进程系统初始化，并在内核初始化后切入idle进程 进程调度 kern/schedule/sched.[ch]：新增：实现FIFO策略的进程调度 执行流 结合前面所述自行理解、总结。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab5/":{"url":"lab5/","title":"LAB5：用户程序","keywords":"","body":"LAB5：用户程序 之前我们已经实现了内存的管理和内核进程的建立。但是那都是在内核态。 接下来我们将在用户态运行一些程序。 用户程序，也就是我们在计算机系课程里一直在写的那些程序，到底怎样在操作系统上跑起来？ 首先需要编译器把用户程序的源代码编译为可以在CPU执行的目标程序，这个目标程序里，既要有执行的代码，又要有关于内存分配的一些信息，告诉我们应该怎样为这个程序分配内存。 我们先不考虑怎样在ucore里运行编译器，只考虑ucore如何把编译好的用户程序运行起来。这需要给它分配一些内存，把程序代码加载进来，建立一个进程，然后通过调度让这个用户进程开始执行。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab5/shi-yan-mu-de.html":{"url":"lab5/shi-yan-mu-de.html","title":"实验目的","keywords":"","body":"实验目的 掌握第一个用户进程创建过程 了解系统调用框架的实现机制 了解用户进程是如何被管理的 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab5/shi-yan-nei-rong.html":{"url":"lab5/shi-yan-nei-rong.html","title":"实验内容","keywords":"","body":"实验内容 构造出第一个用户进程。 学会使用系统调用来运行不同的应用程序。 完成对用户进程的执行过程的基本管理。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab5/lian-xi.html":{"url":"lab5/lian-xi.html","title":"练习","keywords":"","body":"练习 练习1：解释do_fork()与alloc_proc() 为了正常运行lab5, 需要对lab4已有的函数进行一些改动. 我们在lab5的代码框架里标注了两个有改动的函数, 一处在do_fork(), 一处在alloc_proc()，请对每处改动解释一下如果lab5的代码里不做这个改动，那么现在或者将来会出什么bug，或者说会影响哪些功能。 练习2：编写load_icode函数 do_execve函数调用 load_icode（位于 kern/process/proc.c中）来加载并解析一个处于内存中的 ELF 执行文件格式的应用程序，建立相应的用户内存空间来放置应用程序的代码段、数据段等，且要设置好 proc_struct 结构中的成员变量 trapframe 中的内容，确保在执行此进程后，能够从应用程序设定的起始执行地址开始执行。需设置正确的 trapframe 内容。 请在实验报告中简要说明你的设计实现过程。 请在实验报告中描述当创建一个用户态进程并加载了应用程序后，CPU 是如何让这个应用程序最终在用户态执行起来的。即这个用户态进程被 ucore 选择占用 CPU 执行（RUNNING 态）到具体执行应用程序第一条指令的整个经过。 练习3：填写copy_range函数 创建子进程的函数do_fork在执行中将拷贝当前进程（即父进程）的用户内存地址空间中的合法内容到新进程中（子进程），完成内存资源的复制。具体是通过 copy_range函数（位于 kern/mm/pmm.c中）实现的，请补充 copy_range 的实现，确保能够正确执行。 请在实验报告中简要说明如何设计实现”Copy on Write 机制“，给出概要设计，鼓励给出详细设计。 [!NOTE|style:flat] Copy-on-write（简称 COW）的基本概念是指如果有多个使用者对一个资源 A（比如内存块）进行读操作，则每个使用者只需获得一个指向同一个资源 A 的指针，就可以该资源了。若某使用者需要对这个资源 A 进行写操作，系统会对该资源进行拷贝操作，从而使得该“写操作”使用者获得一个该资源 A 的“私有”拷贝—资源 B，可对资源 B 进行写操作。该“写操作”使用者对资源 B 的改变对于其他的使用者而言是不可见的，因为其他使用者看到的还是资源 A。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab5/yong-hu-jin-cheng.html":{"url":"lab5/yong-hu-jin-cheng.html","title":"用户进程","keywords":"","body":"用户进程 系统调用 操作系统应当提供给用户程序一些接口，让用户程序使用操作系统提供的服务。这些接口就是系统调用。用户程序在用户态运行(U mode), 系统调用在内核态执行(S mode)。这里有一个CPU的特权级切换的过程, 要用到ecall指令从U mode进入S mode。想想我们之前用ecall做过什么？在S mode调用OpenSBI提供的M mode接口。当时我们用ecall进入了M mode, 剩下的事情就交给OpenSBI来完成，然后我们收到OpenSBI返回的结果。 现在我们用ecall从U mode进入S mode之后，对应的处理需要我们编写内核系统调用的代码来完成。 另外，我们总不能让用户程序里直接调用ecall。通常我们会把这样的系统调用操作封装成一个个的函数，作为“标准库”提供给用户使用。例如在linux里，写一个C程序时使用printf()函数进行输出, 实际上是要进行write()的系统调用，通过内核把输出打印到命令行或其他地方。 对于用户进程的管理，有四个系统调用比较重要。 sys_fork()：把当前的进程复制一份，创建一个子进程，原先的进程是父进程。接下来两个进程都会收到sys_fork()的返回值，如果返回0说明当前位于子进程中，返回一个非0的值（子进程的PID）说明当前位于父进程中。然后就可以根据返回值的不同，在两个进程里进行不同的处理。 sys_exec()：在当前的进程下，停止原先正在运行的程序，开始执行一个新程序。PID不变，但是内存空间要重新分配，执行的机器代码发生了改变。我们可以用fork()和exec()配合，在当前程序不停止的情况下，开始执行另一个程序。 sys_exit()：退出当前的进程。 sys_wait()：挂起当前的进程，等到特定条件满足的时候再继续执行。 从内核线程到用户进程 在实验四中设计实现了进程控制块，并实现了内核线程的创建和简单的调度执行。但实验四中没有在用户态执行用户进程的管理机制，既无法体现用户进程的地址空间，以及用户进程间地址空间隔离的保护机制，不支持进程执行过程的用户态和核心态之间的切换，且没有用户进程的完整状态变化的生命周期。其实没有实现的原因是内核线程不需要这些功能。那内核线程相对于用户态线程有何特点呢？ 但其实我们已经在实验四中看到了内核线程，内核线程的管理实现相对是简单的，其特点是直接使用操作系统（比如 ucore）在初始化中建立的内核虚拟内存地址空间，不同的内核线程之间可以通过调度器实现线程间的切换，达到分时使用 CPU 的目的。由于内核虚拟内存空间是一一映射计算机系统的物理空间的，这使得可用空间的大小不会超过物理空间大小，所以操作系统程序员编写内核线程时，需要考虑到有限的地址空间，需要保证各个内核线程在执行过程中不会破坏操作系统的正常运行。这样在实现内核线程管理时，不必考虑涉及与进程相关的虚拟内存管理中的缺页处理、按需分页、写时复制、页换入换出等功能。如果在内核线程执行过程中出现了访存错误异常或内存不够的情况，就认为操作系统出现错误了，操作系统将直接宕机。在 ucore 中，就是调用 panic() 函数，进入内核调试监控器 kernel_debug_monitor。 内核线程管理思想相对简单，但编写内核线程对程序员的要求很高。从理论上讲（理想情况），如果程序员都是能够编写操作系统级别的“高手”，能够勤俭和高效地使用计算机系统中的资源，且这些“高手”都为他人着想，具有奉献精神，在别的应用需要计算机资源的时候，能够从大局出发，从整个系统的执行效率出发，让出自己占用的资源，那这些“高手”编写出来的程序直接作为内核线程运行即可，也就没有用户进程存在的必要了。 但现实与理论的差距是巨大的，能编写操作系统的程序员是极少数的，与当前的应用程序员相比，估计大约差了 3~4 个数量级。如果还要求编写操作系统的程序员考虑其他未知程序员的未知需求，那这样的程序员估计可以成为是编程界的“上帝”了。 从应用程序编写和运行的角度看，既然程序员都不是“上帝”，操作系统程序员就需要给应用程序员编写的程序提供一个既“宽松”又“严格”的执行环境，让对内存大小和 CPU 使用时间等资源的限制没有仔细考虑的应用程序都能在操作系统中正常运行，且即使程序太可靠，也只能破坏自己，而不能破坏其他运行程序和整个系统。“严格”就是安全性保证，即应用程序执行不会破坏在内存中存在的其他应用程序和操作系统的内存空间等独占的资源；“宽松”就算是方便性支持，即提供给应用程序尽量丰富的服务功能和一个远大于物理内存空间的虚拟地址空间，使得应用程序在执行过程中不必考虑很多繁琐的细节（比如如何初始化 PCI 总线和外设等，如何管理物理内存等）。 让用户进程正常运行的用户环境 在操作系统原理的介绍中，一般提到进程的概念其实主要是指用户进程。从操作系统的设计和实现的角度看，其实用户进程是指一个应用程序在操作系统提供的一个用户环境中的一次执行过程。这里的重点是用户环境。用户环境有啥功能？用户环境指的是什么？ 从功能上看，操作系统提供的这个用户环境有两方面的特点。一方面与存储空间相关，即限制用户进程可以访问的物理地址空间，且让各个用户进程之间的物理内存空间访问不重叠，这样可以保证不同用户进程之间不能相互破坏各自的内存空间，利用虚拟内存的功能（页换入换出）。给用户进程提供了远大于实际物理内存空间的虚拟内存空间。 另一方面与执行指令相关，即限制用户进程可执行的指令，不能让用户进程执行特权指令（比如修改页表起始地址），从而保证用户进程无法破坏系统。但如果不能执行特权指令，则很多功能（比如访问磁盘等）无法实现，所以需要提供某种机制，让操作系统完成需要特权指令才能做的各种服务功能，给用户进程一个“服务窗口”,用户进程可以通过这个“窗口”向操作系统提出服务请求，由操作系统来帮助用户进程完成需要特权指令才能做的各种服务。另外，还要有一个“中断窗口”，让用户进程不主动放弃使用 CPU 时，操作系统能够通过这个“中断窗口”强制让用户进程放弃使用 CPU，从而让其他用户进程有机会执行。 基于功能分析，我们就可以把这个用户环境定义为如下组成部分： 建立用户虚拟空间的页表和支持页换入换出机制的用户内存访存错误异常服务例程：提供地址隔离和超过物理空间大小的虚存空间。 应用程序执行的用户态 CPU 特权级：在用户态 CPU 特权级，应用程序只能执行一般指令，如果特权指令，结果不是无效就是产生“执行非法指令”异常； 系统调用机制：给用户进程提供“服务窗口”； 中断响应机制：给用户进程设置“中断窗口”，这样产生中断后，当前执行的用户进程将被强制打断，CPU 控制权将被操作系统的中断服务例程使用。 用户态进程的执行过程分析 在这个环境下运行的进程就是用户进程。那如果用户进程由于某种原因下面进入内核态后，那在内核态执行的是什么呢？还是用户进程吗？首先分析一下用户进程这样会进入内核态呢？回顾一下 lab1，就可以知道当产生外设中断、CPU 执行异常（比如访存错误）、陷入（系统调用），用户进程就会切换到内核中的操作系统中来。表面上看，到内核态后，操作系统取得了 CPU 控制权，所以现在执行的应该是操作系统代码，由于此时 CPU 处于核心态特权级，所以操作系统的执行过程就就应该是内核进程了。这样理解忽略了操作系统的具体实现。如果考虑操作系统的具体实现，应该如果来理解进程呢？ 从进程控制块的角度看，如果执行了进程执行现场（上下文）的切换，就认为到另外一个进程执行了，及进程的分界点设定在执行进程切换的前后。到底切换了什么呢？其实只是切换了进程的页表和相关硬件寄存器，这些信息都保存在进程控制块中的相关域中。所以，我们可以把执行应用程序的代码一直到执行操作系统中的进程切换处为止都认为是一个应用程序的执行过程（其中有操作系统的部分代码执行过过程）即进程。因为在这个过程中，没有更换到另外一个进程控制块的进程的页表和相关硬件寄存器。 从指令执行的角度看，如果再仔细分析一下操作系统这个软件的特点并细化一下进入内核原因，就可以看出进一步进行划分。操作系统的主要功能是给上层应用提供服务，管理整个计算机系统中的资源。所以操作系统虽然是一个软件，但其实是一个基于事件的软件，这里操作系统需要响应的事件包括三类：外设中断、CPU 执行异常（比如访存错误）、陷入（系统调用）。如果用户进程通过系统调用要求操作系统提供服务，那么用户进程的角度看，操作系统就是一个特殊的软件库（比如相对于用户态的 libc 库，操作系统可看作是内核态的 libc 库），完成用户进程的需求，从执行逻辑上看，是用户进程“主观”执行的一部分，即用户进程“知道”操作系统要做的事情。那么在这种情况下，进程的代码空间包括用户态的执行程序和内核态响应用户进程通过系统调用而在核心特权态执行服务请求的操作系统代码，为此这种情况下的进程的内存虚拟空间也包括两部分：用户态的虚地址空间和核心态的虚地址空间。但如果此时发生的事件是外设中断和 CPU 执行异常，虽然 CPU 控制权也转入到操作系统中的中断服务例程，但这些内核执行代码执行过程是用户进程“不知道”的，是另外一段执行逻辑。那么在这种情况下，实际上是执行了两段目标不同的执行程序，一个是代表应用程序的用户进程，一个是代表中断服务例程处理外设中断和 CPU 执行异常的内核线程。这个用户进程和内核线程在产生中断或异常的时候，CPU 硬件就完成了它们之间的指令流切换。 用户进程的运行状态分析 用户进程在其执行过程中会存在很多种不同的执行状态，根据操作系统原理，一个用户进程一般的运行状态有五种：创建（new）态、就绪（ready）态、运行（running）态、等待（blocked）态、退出（exit）态。各个状态之间会由于发生了某事件而进行状态转换。 但在用户进程的执行过程中，具体在哪个时间段处于上述状态的呢？上述状态是如何转变的呢？首先，我们看创建（new）态，操作系统完成进程的创建工作，而体现进程存在的就是进程控制块，所以一旦操作系统创建了进程控制块，则可以认为此时进程就已经存在了，但由于进程能够运行的各种资源还没准备好，所以此时的进程处于创建（new）态。创建了进程控制块后，进程并不能就执行了，还需准备好各种资源，如果把进程执行所需要的虚拟内存空间，执行代码，要处理的数据等都准备好了，则此时进程已经可以执行了，但还没有被操作系统调度，需要等待操作系统选择这个进程执行，于是把这个做好“执行准备”的进程放入到一个队列中，并可以认为此时进程处于就绪（ready）态。当操作系统的调度器从就绪进程队列中选择了一个就绪进程后，通过执行进程切换，就让这个被选上的就绪进程执行了，此时进程就处于运行（running）态了。到了运行态后，会出现三种事件。如果进程需要等待某个事件（比如主动睡眠 10 秒钟，或进程访问某个内存空间，但此内存空间被换出到硬盘 swap 分区中了，进程不得不等待操作系统把缓慢的硬盘上的数据重新读回到内存中），那么操作系统会把 CPU 给其他进程执行，并把进程状态从运行（running）态转换为等待（blocked）态。如果用户进程的应用程序逻辑流程执行结束了，那么操作系统会把 CPU 给其他进程执行，并把进程状态从运行（running）态转换为退出（exit）态，并准备回收用户进程占用的各种资源，当把表示整个进程存在的进程控制块也回收了，这进程就不存在了。在这整个回收过程中，进程都处于退出（exit）态。2 考虑到在内存中存在多个处于就绪态的用户进程，但只有一个 CPU，所以为了公平起见，每个就绪态进程都只有有限的时间片段，当一个运行态的进程用完了它的时间片段后，操作系统会剥夺此进程的 CPU 使用权，并把此进程状态从运行（running）态转换为就绪（ready）态，最后把 CPU 给其他进程执行。如果某个处于等待（blocked）态的进程所等待的事件产生了（比如睡眠时间到，或需要访问的数据已经从硬盘换入到内存中），则操作系统会通过把等待此事件的进程状态从等待（blocked）态转到就绪（ready）态。这样进程的整个状态转换形成了一个有限状态自动机。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab5/yong-hu-cheng-xu.html":{"url":"lab5/yong-hu-cheng-xu.html","title":"用户程序","keywords":"","body":"用户程序 应用程序的组成与编译 我们首先来看一个应用程序，这里我们假定是hello应用程序，在user/hello.c中实现，代码如下： #include #include int main(void) { cprintf(\"Hello world!!.\\n\"); cprintf(\"I am process %d.\\n\", getpid()); cprintf(\"hello pass.\\n\"); return 0; } hello应用程序只是输出一些字符串，并通过系统调用sys_getpid（在getpid函数中调用）输出代表hello应用程序执行的用户进程的进程标识--pid。 首先，我们需要了解ucore操作系统如何能够找到hello应用程序。这需要分析ucore和hello是如何编译的。修改Makefile，把第六行注释掉。然后在本实验源码目录下执行make，可得到如下输出： + cc user/hello.c riscv64-unknown-elf-gcc -Iuser/ -mcmodel=medany -O2 -std=gnu99 -Wno-unused -fno-builtin -Wall -nostdinc -fno-stack-protector -ffunction-sections -fdata-sections -Ilibs/ -Iuser/include/ -Iuser/libs/ -c user/hello.c -o obj/user/hello.o riscv64-unknown-elf-ld -m elf64lriscv -nostdlib --gc-sections -T tools/user.ld -o obj/__user_hello.out obj/user/libs/panic.o obj/user/libs/syscall.o obj/user/libs/ulib.o obj/user/libs/initcode.o obj/user/libs/stdio.o obj/user/libs/umain.o obj/libs/string.o obj/libs/printfmt.o obj/libs/hash.o obj/libs/rand.o obj/user/hello.o + ld bin/kernel riscv64-unknown-elf-ld -m elf64lriscv -nostdlib --gc-sections -T tools/kernel.ld -o bin/kernel obj/kern/init/entry.o obj/kern/init/init.o obj/kern/libs/stdio.o obj/kern/libs/readline.o obj/kern/debug/panic.o obj/kern/debug/kdebug.o obj/kern/debug/kmonitor.o obj/kern/driver/ide.o obj/kern/driver/clock.o obj/kern/driver/console.o obj/kern/driver/picirq.o obj/kern/driver/intr.o obj/kern/trap/trap.o obj/kern/trap/trapentry.o obj/kern/mm/vmm.o obj/kern/mm/swap.o obj/kern/mm/kmalloc.o obj/kern/mm/swap_fifo.o obj/kern/mm/default_pmm.o obj/kern/mm/pmm.o obj/kern/fs/swapfs.o obj/kern/process/entry.o obj/kern/process/switch.o obj/kern/process/proc.o obj/kern/schedule/sched.o obj/kern/syscall/syscall.o obj/libs/string.o obj/libs/printfmt.o obj/libs/hash.o obj/libs/rand.o --format=binary obj/__user_hello.out obj/__user_badarg.out obj/__user_forktree.out obj/__user_faultread.out obj/__user_divzero.out obj/__user_exit.out obj/__user_softint.out obj/__user_waitkill.out obj/__user_spin.out obj/__user_yield.out obj/__user_badsegment.out obj/__user_testbss.out obj/__user_faultreadkernel.out obj/__user_forktest.out obj/__user_pgdir.out --format=default riscv64-unknown-elf-objcopy bin/kernel --strip-all -O binary bin/ucore.img 从中可以看出，hello应用程序不仅仅是hello.c，还包含了支持hello应用程序的用户态库： user/libs/initcode.S：所有应用程序的起始用户态执行地址“_start”，调整了EBP和ESP后，调用umain函数。 user/libs/umain.c：实现了umain函数，这是所有应用程序执行的第一个C函数，它将调用应用程序的main函数，并在main函数结束后调用exit函数，而exit函数最终将调用sys_exit系统调用，让操作系统回收进程资源。 user/libs/ulib.[ch]：实现了最小的C函数库，除了一些与系统调用无关的函数，其他函数是对访问系统调用的包装。 user/libs/syscall.[ch]：用户层发出系统调用的具体实现。 user/libs/stdio.c：实现cprintf函数，通过系统调用sys_putc来完成字符输出。 user/libs/panic.c：实现__panic/__warn函数，通过系统调用sys_exit完成用户进程退出。 除了这些用户态库函数实现外，还有一些libs/*.[ch]是操作系统内核和应用程序共用的函数实现。这些用户库函数其实在本质上与UNIX系统中的标准libc没有区别，只是实现得很简单，但hello应用程序的正确执行离不开这些库函数。 [!NOTE|style:flat] libs/.[ch]、user/libs/.[ch]、user/*.[ch]的源码中没有任何特权指令。 在make的最后一步执行了一个ld命令，把hello应用程序的执行码obj/__user_hello.out连接在了ucore kernel的末尾。且ld命令会在kernel中会把__user_hello.out的位置和大小记录在全局变量_binary_obj___user_hello_out_start和_binary_obj___user_hello_out_size中，这样这个hello用户程序就能够和ucore内核一起被 OpenSBI加载到内存里中，并且通过这两个全局变量定位hello用户程序执行码的起始位置和大小。而到了与文件系统相关的实验后，ucore会提供一个简单的文件系统，那时所有的用户程序就都不再用这种方法进行加载了，而可以用大家熟悉的文件方式进行加载了。 用户进程的虚拟地址空间 在tools/user.ld描述了用户程序的用户虚拟空间的执行入口虚拟地址： SECTIONS { /* Load programs at this address: \".\" means the current address */ . = 0x800020; 在tools/kernel.ld描述了操作系统的内核虚拟空间的起始入口虚拟地址： BASE_ADDRESS = 0xFFFFFFFFC0200000; SECTIONS { /* Load the kernel at this address: \".\" means the current address */ . = BASE_ADDRESS; 这样ucore把用户进程的虚拟地址空间分了两块，一块与内核线程一样，是所有用户进程都共享的内核虚拟地址空间，映射到同样的物理内存空间中，这样在物理内存中只需放置一份内核代码，使得用户进程从用户态进入核心态时，内核代码可以统一应对不同的内核程序；另外一块是用户虚拟地址空间，虽然虚拟地址范围一样，但映射到不同且没有交集的物理内存空间中。这样当ucore把用户进程的执行代码（即应用程序的执行代码）和数据（即应用程序的全局变量等）放到用户虚拟地址空间中时，确保了各个进程不会“非法”访问到其他进程的物理内存空间。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab5/chuang-jian-bing-zhi-hang-yong-hu-jin-cheng.html":{"url":"lab5/chuang-jian-bing-zhi-hang-yong-hu-jin-cheng.html","title":"创建并执行用户进程","keywords":"","body":"创建并执行用户进程 我们在proc_init()函数里初始化进程的时候, 认为启动时运行的ucore程序, 是一个内核进程(\"第0个\"内核进程), 并将其初始化为idleproc进程。然后我们新建了一个内核进程执行init_main()函数。 我们比较lab4和lab5的init_main()有何不同。 // kern/process/proc.c (lab4) static int init_main(void *arg) { cprintf(\"this initproc, pid = %d, name = \\\"%s\\\"\\n\", current->pid, get_proc_name(current)); cprintf(\"To U: \\\"%s\\\".\\n\", (const char *)arg); cprintf(\"To U: \\\"en.., Bye, Bye. :)\\\"\\n\"); return 0; } // kern/process/proc.c (lab5) static int init_main(void *arg) { size_t nr_free_pages_store = nr_free_pages(); size_t kernel_allocated_store = kallocated(); int pid = kernel_thread(user_main, NULL, 0); if (pid cptr == NULL && initproc->yptr == NULL && initproc->optr == NULL); assert(nr_process == 2); assert(list_next(&proc_list) == &(initproc->list_link)); assert(list_prev(&proc_list) == &(initproc->list_link)); cprintf(\"init check memory pass.\\n\"); return 0; } 注意到，lab5新建了一个内核进程，执行函数user_main(),这个内核进程里我们将要开始执行用户进程。 do_wait(0, NULL)等待子进程退出，也就是等待user_main()退出。 我们来看user_main()和do_wait()里做了什么 // kern/process/proc.c #define __KERNEL_EXECVE(name, binary, size) ({ \\ cprintf(\"kernel_execve: pid = %d, name = \\\"%s\\\".\\n\", \\ current->pid, name); \\ kernel_execve(name, binary, (size_t)(size)); \\ }) #define KERNEL_EXECVE(x) ({ \\ extern unsigned char _binary_obj___user_##x##_out_start[], \\ _binary_obj___user_##x##_out_size[]; \\ __KERNEL_EXECVE(#x, _binary_obj___user_##x##_out_start, \\ _binary_obj___user_##x##_out_size); \\ }) #define __KERNEL_EXECVE2(x, xstart, xsize) ({ \\ extern unsigned char xstart[], xsize[]; \\ __KERNEL_EXECVE(#x, xstart, (size_t)xsize); \\ }) #define KERNEL_EXECVE2(x, xstart, xsize) __KERNEL_EXECVE2(x, xstart, xsize) // user_main - kernel thread used to exec a user program static int user_main(void *arg) { #ifdef TEST KERNEL_EXECVE2(TEST, TESTSTART, TESTSIZE); #else KERNEL_EXECVE(exit); #endif panic(\"user_main execve failed.\\n\"); } 于是，我们在user_main()所做的，就是执行了 kern_execve(\"exit\", _binary_obj___user_exit_out_start,_binary_obj___user_exit_out_size) 这么一个函数。 如果你熟悉execve()函数，或许已经猜到这里我们做了什么。 实际上，就是加载了存储在这个位置的程序exit并在user_main这个进程里开始执行。这时user_main就从内核进程变成了用户进程。我们在下一节介绍kern_execve()的实现。 我们在user目录下存储了一些用户程序，在编译的时候放到生成的镜像里。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab5/xi-tong-tiao-yong.html":{"url":"lab5/xi-tong-tiao-yong.html","title":"系统调用","keywords":"","body":"系统调用 系统调用，是用户态(U mode)的程序获取内核态（S mode)服务的方法，所以需要在用户态和内核态都加入对应的支持和处理。我们也可以认为用户态只是提供一个调用的接口，真正的处理都在内核态进行。 系统调用转发 首先我们在头文件里定义一些系统调用的编号。 // libs/unistd.h #ifndef __LIBS_UNISTD_H__ #define __LIBS_UNISTD_H__ #define T_SYSCALL 0x80 /* syscall number */ #define SYS_exit 1 #define SYS_fork 2 #define SYS_wait 3 #define SYS_exec 4 #define SYS_clone 5 #define SYS_yield 10 #define SYS_sleep 11 #define SYS_kill 12 #define SYS_gettime 17 #define SYS_getpid 18 #define SYS_brk 19 #define SYS_mmap 20 #define SYS_munmap 21 #define SYS_shmem 22 #define SYS_putc 30 #define SYS_pgdir 31 /* SYS_fork flags */ #define CLONE_VM 0x00000100 // set if VM shared between processes #define CLONE_THREAD 0x00000200 // thread group #endif /* !__LIBS_UNISTD_H__ */ 我们注意在用户态进行系统调用的核心操作是，通过内联汇编进行ecall环境调用。这将产生一个trap, 进入S mode进行异常处理。 // user/libs/syscall.c #include #include #include #include #define MAX_ARGS 5 static inline int syscall(int num, ...) { //va_list, va_start, va_arg都是C语言处理参数个数不定的函数的宏 //在stdarg.h里定义 va_list ap; //ap: 参数列表(此时未初始化) va_start(ap, num); //初始化参数列表, 从num开始 //First, va_start initializes the list of variable arguments as a va_list. uint64_t a[MAX_ARGS]; int i, ret; for (i = 0; i 我们下面看看trap.c是如何转发这个系统调用的。 // kern/trap/trap.c void exception_handler(struct trapframe *tf) { int ret; switch (tf->cause) { //通过中断帧里 scause寄存器的数值，判断出当前是来自USER_ECALL的异常 case CAUSE_USER_ECALL: //cprintf(\"Environment call from U-mode\\n\"); tf->epc += 4; //sepc寄存器是产生异常的指令的位置，在异常处理结束后，会回到sepc的位置继续执行 //对于ecall, 我们希望sepc寄存器要指向产生异常的指令(ecall)的下一条指令 //否则就会回到ecall执行再执行一次ecall, 无限循环 syscall();// 进行系统调用处理 break; /*other cases .... */ } } // kern/syscall/syscall.c #include #include #include #include #include #include #include //这里把系统调用进一步转发给proc.c的do_exit(), do_fork()等函数 static int sys_exit(uint64_t arg[]) { int error_code = (int)arg[0]; return do_exit(error_code); } static int sys_fork(uint64_t arg[]) { struct trapframe *tf = current->tf; uintptr_t stack = tf->gpr.sp; return do_fork(0, stack, tf); } static int sys_wait(uint64_t arg[]) { int pid = (int)arg[0]; int *store = (int *)arg[1]; return do_wait(pid, store); } static int sys_exec(uint64_t arg[]) { const char *name = (const char *)arg[0]; size_t len = (size_t)arg[1]; unsigned char *binary = (unsigned char *)arg[2]; size_t size = (size_t)arg[3]; //用户态调用的exec(), 归根结底是do_execve() return do_execve(name, len, binary, size); } static int sys_yield(uint64_t arg[]) { return do_yield(); } static int sys_kill(uint64_t arg[]) { int pid = (int)arg[0]; return do_kill(pid); } static int sys_getpid(uint64_t arg[]) { return current->pid; } static int sys_putc(uint64_t arg[]) { int c = (int)arg[0]; cputchar(c); return 0; } //这里定义了函数指针的数组syscalls, 把每个系统调用编号的下标上初始化为对应的函数指针 static int (*syscalls[])(uint64_t arg[]) = { [SYS_exit] sys_exit, [SYS_fork] sys_fork, [SYS_wait] sys_wait, [SYS_exec] sys_exec, [SYS_yield] sys_yield, [SYS_kill] sys_kill, [SYS_getpid] sys_getpid, [SYS_putc] sys_putc, }; #define NUM_SYSCALLS ((sizeof(syscalls)) / (sizeof(syscalls[0]))) void syscall(void) { struct trapframe *tf = current->tf; uint64_t arg[5]; int num = tf->gpr.a0;//a0寄存器保存了系统调用编号 if (num >= 0 && num gpr.a1; arg[1] = tf->gpr.a2; arg[2] = tf->gpr.a3; arg[3] = tf->gpr.a4; arg[4] = tf->gpr.a5; tf->gpr.a0 = syscalls[num](arg); //把寄存器里的参数取出来，转发给系统调用编号对应的函数进行处理 return ; } } //如果执行到这里，说明传入的系统调用编号还没有被实现，就崩掉了。 print_trapframe(tf); panic(\"undefined syscall %d, pid = %d, name = %s.\\n\", num, current->pid, current->name); } 这样我们就完成了系统调用的转发。接下来就是在do_exit(), do_execve()等函数中进行具体处理了。 do_execve() 我们看看do_execve()函数 // kern/mm/vmm.c bool user_mem_check(struct mm_struct *mm, uintptr_t addr, size_t len, bool write) { //检查从addr开始长为len的一段内存能否被用户态程序访问 if (mm != NULL) { if (!USER_ACCESS(addr, addr + len)) { return 0; } struct vma_struct *vma; uintptr_t start = addr, end = addr + len; while (start vm_start) { return 0; } if (!(vma->vm_flags & ((write) ? VM_WRITE : VM_READ))) { return 0; } if (write && (vma->vm_flags & VM_STACK)) { if (start vm_start + PGSIZE) { //check stack start & size return 0; } } start = vma->vm_end; } return 1; } return KERN_ACCESS(addr, addr + len); } // kern/process/proc.c // do_execve - call exit_mmap(mm)&put_pgdir(mm) to reclaim memory space of current process // - call load_icode to setup new memory space accroding binary prog. int do_execve(const char *name, size_t len, unsigned char *binary, size_t size) { struct mm_struct *mm = current->mm; if (!user_mem_check(mm, (uintptr_t)name, len, 0)) { //检查name的内存空间能否被访问 return -E_INVAL; } if (len > PROC_NAME_LEN) { //进程名字的长度有上限 PROC_NAME_LEN，在proc.h定义 len = PROC_NAME_LEN; } char local_name[PROC_NAME_LEN + 1]; memset(local_name, 0, sizeof(local_name)); memcpy(local_name, name, len); if (mm != NULL) { cputs(\"mm != NULL\"); lcr3(boot_cr3); if (mm_count_dec(mm) == 0) { exit_mmap(mm); put_pgdir(mm); mm_destroy(mm);//把进程当前占用的内存释放，之后重新分配内存 } current->mm = NULL; } //把新的程序加载到当前进程里的工作都在load_icode()函数里完成 int ret; if ((ret = load_icode(binary, size)) != 0) { goto execve_exit;//返回不为0，则加载失败 } set_proc_name(current, local_name); //如果set_proc_name的实现不变, 为什么不能直接set_proc_name(current, name)? return 0; execve_exit: do_exit(ret); panic(\"already exit: %e.\\n\", ret); } kernel_execve() 那么我们如何实现kernel_execve()函数？ 能否直接调用do_execve()? // kern/process/proc.c static int kernel_execve(const char *name, unsigned char *binary, size_t size) { int64_t ret=0, len = strlen(name); ret = do_execve(name, len, binary, size); cprintf(\"ret = %d\\n\", ret); return ret; } 很不幸。这么做行不通。do_execve() load_icode()里面只是构建了用户程序运行的上下文，但是并没有完成切换。上下文切换实际上要借助中断处理的返回来完成。直接调用do_execve()是无法完成上下文切换的。如果是在用户态调用exec(), 系统调用的ecall产生的中断返回时， 就可以完成上下文切换。 由于目前我们在S mode下，所以不能通过ecall来产生中断。我们这里采取一个取巧的办法，用ebreak产生断点中断进行处理，通过设置a7寄存器的值为10说明这不是一个普通的断点中断，而是要转发到syscall(), 这样用一个不是特别优雅的方式，实现了在内核态使用系统调用。 // kern/process/proc.c // kernel_execve - do SYS_exec syscall to exec a user program called by user_main kernel_thread static int kernel_execve(const char *name, unsigned char *binary, size_t size) { int64_t ret=0, len = strlen(name); asm volatile( \"li a0, %1\\n\" \"lw a1, %2\\n\" \"lw a2, %3\\n\" \"lw a3, %4\\n\" \"lw a4, %5\\n\" \"li a7, 10\\n\" \"ebreak\\n\" \"sw a0, %0\\n\" : \"=m\"(ret) : \"i\"(SYS_exec), \"m\"(name), \"m\"(len), \"m\"(binary), \"m\"(size) : \"memory\"); //这里内联汇编的格式，和用户态调用ecall的格式类似，只是ecall换成了ebreak cprintf(\"ret = %d\\n\", ret); return ret; } // kern/trap/trap.c void exception_handler(struct trapframe *tf) { int ret; switch (tf->cause) { case CAUSE_BREAKPOINT: cprintf(\"Breakpoint\\n\"); if(tf->gpr.a7 == 10){ tf->epc += 4; //注意返回时要执行ebreak的下一条指令 syscall(); } break; /* other cases ... */ } } 注意我们需要让CPU进入U mode执行do_execve()加载的用户程序。进行系统调用sys_exec之后，我们在trap返回的时候调用了sret指令，这时只要sstatus寄存器的SPP二进制位为0，就会切换到U mode，但SPP存储的是“进入trap之前来自什么特权级”，也就是说我们这里ebreak之后SPP的数值为1，sret之后会回到S mode在内核态执行用户程序。所以load_icode()函数在构造新进程的时候，会把SSTATUS_SPP设置为0，使得sret的时候能回到U mode。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab5/yong-hu-jin-cheng-de-tui-chu-he-deng-dai.html":{"url":"lab5/yong-hu-jin-cheng-de-tui-chu-he-deng-dai.html","title":"用户进程的退出和等待","keywords":"","body":"用户进程的退出和等待 退出 在进程执行完工作后，需要退出，释放资源，正我们在do_execve函数末尾看到的一样，退出进程是调用do_exit函数来实现的。 execve_exit: do_exit(ret); panic(\"already exit: %e.\\n\", ret); // do_exit - called by sys_exit // 1. call exit_mmap & put_pgdir & mm_destroy to free the almost all memory space of process // 2. set process' state as PROC_ZOMBIE, then call wakeup_proc(parent) to ask parent reclaim itself. // 3. call scheduler to switch to other process int do_exit(int error_code) { if (current == idleproc) { panic(\"idleproc exit.\\n\"); } if (current == initproc) { panic(\"initproc exit.\\n\"); } struct mm_struct *mm = current->mm; if (mm != NULL) { lcr3(boot_cr3); if (mm_count_dec(mm) == 0) { exit_mmap(mm); put_pgdir(mm); mm_destroy(mm); } current->mm = NULL; } current->state = PROC_ZOMBIE; current->exit_code = error_code; bool intr_flag; struct proc_struct *proc; local_intr_save(intr_flag); { proc = current->parent; if (proc->wait_state == WT_CHILD) { wakeup_proc(proc); } while (current->cptr != NULL) { proc = current->cptr; current->cptr = proc->optr; proc->yptr = NULL; if ((proc->optr = initproc->cptr) != NULL) { initproc->cptr->yptr = proc; } proc->parent = initproc; initproc->cptr = proc; if (proc->state == PROC_ZOMBIE) { if (initproc->wait_state == WT_CHILD) { wakeup_proc(initproc); } } } } local_intr_restore(intr_flag); schedule(); panic(\"do_exit will not return!! %d.\\n\", current->pid); } 如果是内核线程则不需要回收空间 如果是用户进程，就开始回收，首先执行lcr3(boot_cr3);切换到内核的页表上，这样用户进程就只能在内核的虚拟地址空间上执行，因为内核权限高。如果当前进程的被调用数减一后等于0，那么就没有其他进程在使用了，就可以进行回收，先回收内存资源，调用exit_mmap函数释放mm中的vma描述的进程合法空间中实际分配的内存，然后把对应的页表项内容清空，最后把页表项和页目录表清空。然后调用put_pgdir函数释放页目录表所占用的内存。最后调用mm_destroy释放vma与mm的内存。把mm置为NULL，表示与当前进程相关的用户虚拟内存空间和对应的内存管 理成员变量所占的内核虚拟内存空间已经回收完毕； 设置进程的状态为PROC_ZOMBIE表示该进程要死了，等待父进程来回收资源，回收内核栈和进程控制块。当前进程的退出码为error_code表示该进程已经不能被调度。 如果当前进程的父进程处于等待子进程的状态，则唤醒父进程让父进程回收资源。 如果该进程还有子进程，那么就指向第一个孩子，把后面的孩子全部置为空，然后把孩子过继给内核线程initproc，把子进程插入到initproc的孩子链表中，如果某个子进程的状态时要死的状态，并且initproc的状态时等待孩子的状态，则唤醒initproc来回收子进程的资源。 然后开启中断，执行schedule函数，选择新的进程执行 等待 那么父进程如何完成对子进程的最后回收工作呢？这要求父进程要执行wait用户函数或wait_pid用户函数，这两个函数的区别是，wait函数等待任意子进程的结束通知，而wait_pid函数等待进程id号为pid的子进程结束通知。这两个函数最终访问sys_wait系统调用接口让ucore来完成对子进程的最后回收工作，即回收子进程的内核栈和进程控制块所占内存空间，具体流程如下： // do_wait - wait one OR any children with PROC_ZOMBIE state, and free memory space of kernel stack // - proc struct of this child. // NOTE: only after do_wait function, all resources of the child proces are free. int do_wait(int pid, int *code_store) { struct mm_struct *mm = current->mm; if (code_store != NULL) { if (!user_mem_check(mm, (uintptr_t)code_store, sizeof(int), 1)) { return -E_INVAL; } } struct proc_struct *proc; bool intr_flag, haskid; repeat: haskid = 0; if (pid != 0) { proc = find_proc(pid); if (proc != NULL && proc->parent == current) { haskid = 1; if (proc->state == PROC_ZOMBIE) { goto found; } } } else { proc = current->cptr; for (; proc != NULL; proc = proc->optr) { haskid = 1; if (proc->state == PROC_ZOMBIE) { goto found; } } } if (haskid) { current->state = PROC_SLEEPING; current->wait_state = WT_CHILD; schedule(); if (current->flags & PF_EXITING) { do_exit(-E_KILLED); } goto repeat; } return -E_BAD_PROC; found: if (proc == idleproc || proc == initproc) { panic(\"wait idleproc or initproc.\\n\"); } if (code_store != NULL) { *code_store = proc->exit_code; } local_intr_save(intr_flag); { unhash_proc(proc); remove_links(proc); } local_intr_restore(intr_flag); put_kstack(proc); kfree(proc); return 0; } 首先进行检查 若pid等于0，就去找对应的孩子进程，否则就任意的一个快死的孩子进程。如果此子进程的执行状态不为PROC_ZOMBIE，表明此子进程还没有退出，则当前进程只 好设置自己的执行状态为PROC_SLEEPING，睡眠原因为WT_CHILD（即等待子进程退 出），调用schedule()函数选择新的进程执行，自己睡眠等待，如果被唤醒，则重复该步骤执行； 如果此子进程的执行状态为PROC_ZOMBIE，表明此子进程处于退出状态，需要当前进程 （即子进程的父进程）完成对子进程的最终回收工作，即首先把子进程控制块从两个进程队列proc_list和hash_list中删除，并释放子进程的内核堆栈和进程控制块。自此，子进程才彻底 地结束了它的执行过程，消除了它所占用的所有资源。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab5/xiang-mu-zu-cheng-yu-zhi-hang-liu.html":{"url":"lab5/xiang-mu-zu-cheng-yu-zhi-hang-liu.html","title":"项目组成与执行流","keywords":"","body":"项目组成与执行流 项目组成 lab5 ├── Makefile ├── boot │ ├── asm.h │ ├── bootasm.S │ └── bootmain.c ├── kern │ ├── debug │ │ ├── assert.h │ │ ├── kdebug.c │ │ ├── kdebug.h │ │ ├── kmonitor.c │ │ ├── kmonitor.h │ │ ├── panic.c │ │ └── stab.h │ ├── driver │ │ ├── clock.c │ │ ├── clock.h │ │ ├── console.c │ │ ├── console.h │ │ ├── ide.c │ │ ├── ide.h │ │ ├── intr.c │ │ ├── intr.h │ │ ├── kbdreg.h │ │ ├── picirq.c │ │ └── picirq.h │ ├── fs │ │ ├── fs.h │ │ ├── swapfs.c │ │ └── swapfs.h │ ├── init │ │ ├── entry.S │ │ └── init.c │ ├── libs │ │ ├── readline.c │ │ └── stdio.c │ ├── mm │ │ ├── default_pmm.c │ │ ├── default_pmm.h │ │ ├── kmalloc.c │ │ ├── kmalloc.h │ │ ├── memlayout.h │ │ ├── mmu.h │ │ ├── pmm.c │ │ ├── pmm.h │ │ ├── swap.c │ │ ├── swap.h │ │ ├── swap_fifo.c │ │ ├── swap_fifo.h │ │ ├── vmm.c │ │ └── vmm.h │ ├── process │ │ ├── entry.S │ │ ├── proc.c │ │ ├── proc.h │ │ └── switch.S │ ├── schedule │ │ ├── sched.c │ │ └── sched.h │ ├── sync │ │ └── sync.h │ ├── syscall │ │ ├── syscall.c │ │ └── syscall.h │ └── trap │ ├── trap.c │ ├── trap.h │ └── trapentry.S ├── lab5.md ├── libs │ ├── atomic.h │ ├── defs.h │ ├── elf.h │ ├── error.h │ ├── hash.c │ ├── list.h │ ├── printfmt.c │ ├── rand.c │ ├── riscv.h │ ├── sbi.h │ ├── stdarg.h │ ├── stdio.h │ ├── stdlib.h │ ├── string.c │ ├── string.h │ └── unistd.h ├── tools │ ├── boot.ld │ ├── function.mk │ ├── gdbinit │ ├── grade.sh │ ├── kernel.ld │ ├── sign.c │ ├── user.ld │ └── vector.c └── user ├── badarg.c ├── badsegment.c ├── divzero.c ├── exit.c ├── faultread.c ├── faultreadkernel.c ├── forktest.c ├── forktree.c ├── hello.c ├── libs │ ├── initcode.S │ ├── panic.c │ ├── stdio.c │ ├── syscall.c │ ├── syscall.h │ ├── ulib.c │ ├── ulib.h │ └── umain.c ├── pgdir.c ├── softint.c ├── spin.c ├── testbss.c ├── waitkill.c └── yield.c 17 directories, 103 files 用户进程内存管理 kern/mm/pmm.[ch]：添加了用于进程退出（do_exit）的内存资源回收的page_remove_pte、unmap_range、exit_range函数和用于创建子进程（do_fork）中拷贝父进程内存空间的copy_range函数，修改了pgdir_alloc_page函数 kern/mm/vmm.[ch]：修改：扩展了mm_struct数据结构，增加了一系列函数 mm_map/dup_mmap/exit_mmap：设定/取消/复制/删除用户进程的合法内存空间 copy_from_user/copy_to_user：用户内存空间内容与内核内存空间内容的相互拷贝的实现 user_mem_check：搜索vma链表，检查是否是一个合法的用户空间范围 用户进程管理 kern/process/proc.[ch]：扩展了proc_struct数据结构。增加或修改了一系列函数 setup_pgdir/put_pgdir：创建并设置/释放页目录表 copy_mm：复制用户进程的内存空间和设置相关内存管理（如页表等）信息 do_exit：释放进程自身所占内存空间和相关内存管理（如页表等）信息所占空间，唤醒父进程，好让父进程收了自己，让调度器切换到其他进程 load_icode：被do_execve调用，完成加载放在内存中的执行程序到进程空间，这涉及到对页表等的修改，分配用户栈 do_execve：先回收自身所占用户空间，然后调用load_icode，用新的程序覆盖内存空间，形成一个执行新程序的新进程 do_yield：让调度器执行一次选择新进程的过程 do_wait：父进程等待子进程，并在得到子进程的退出消息后，彻底回收子进程所占的资源（比如子进程的内核栈和进程控制块） do_kill：给一个进程设置PF_EXITING标志（“kill”信息，即要它死掉），这样在trap函数中，将根据此标志，让进程退出 KERNEL_EXECVE/__KERNEL_EXECVE/__KERNEL_EXECVE2：被user_main调用，执行一用户进程 执行流 结合前面所述自行理解、总结。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab6/":{"url":"lab6/","title":"LAB6：进程调度","keywords":"","body":"LAB6：进程调度 在前两章中，我们已经分别实现了内核进程和用户进程，并且让他们正确运行了起来。我们同时也实现了一个简单的调度算法，FIFO调度算法，来对我们的进程进行调度。但是，单单如此就够了吗？显然，我们可以让ucore支持更加丰富的调度算法，从而满足各方面的调度需求。在本章里，我们要在调度框架的基础上实现各种各样的调度算法。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab6/shi-yan-mu-de.html":{"url":"lab6/shi-yan-mu-de.html","title":"实验目的","keywords":"","body":"实验目的 理解操作系统的调度管理机制 熟悉ucore的系统调度器框架 熟悉Round-Robin调度算法 熟悉Stride Scheduling调度算法 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab6/shi-yan-nei-rong.html":{"url":"lab6/shi-yan-nei-rong.html","title":"实验内容","keywords":"","body":"实验内容 掌握理论课有关调度算法的内容。 阅读框架代码，掌握进程调度的主要流程。 在框架的RR调度算法的基础上，实现Stride调度算法。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab6/lian-xi.html":{"url":"lab6/lian-xi.html","title":"练习","keywords":"","body":"练习 练习1：比较函数 比较一个在lab5和lab6都有, 但是实现不同的函数, 说说为什么要做这个改动, 不做这个改动会出什么问题。 [!TIP|style:flat] 如kern/schedule/sched.c里的函数。你也可以找个其他地方做了改动的函数。 练习2：分析sched_class 理解并分析 sched_class 中各个函数指针的用法，并描述ucore如何通过Round Robin算法来调度两个进程，并解释sched_class里的每个函数（函数指针）是怎么被调用的。 练习3：二选一 简要说明如何设计实现”多级反馈队列调度算法“，给出概要设计，鼓励给出详细设计 简要证明/说明（不必特别严谨，但应当能够”说服你自己“），为什么Stride算法中，经过足够多的时间片之后，每个进程分配到的时间片数目和优先级成正比。 练习4：实现Stride调度算法 在kern/schedule/default_sched_stride.c填写Stride调度算法实现。 [!TIP|style:flat] 你需要调试proc.c中的一个bug，才能使调度算法正常运行。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab6/jin-cheng-zhuang-tai.html":{"url":"lab6/jin-cheng-zhuang-tai.html","title":"进程状态","keywords":"","body":"进程状态 在本次实验中，我们在init/init.c中加入了对sched_init函数的调用。这个函数主要完成调度器和特定调度算法的绑定。初始化后，我们在调度函数中就可以使用相应的接口了。这也是在C语言环境下对于面向对象编程模式的一种模仿。这样之后，我们只需要关注于实现调度类的接口即可，操作系统也同样不关心调度类具体的实现，方便了新调度算法的开发。 在ucore中，进程有如下几个状态： PROC_UNINIT：这个状态表示进程刚刚被分配相应的进程控制块，但还没有初始化，需要进一步的初始化才能进入PROC_RUNNABLE的状态。 PROC_SLEEPING：这个状态表示进程正在等待某个事件的发生，通常由于等待锁的释放，或者主动交出CPU资源（do_sleep）。这个状态下的进程是不会被调度的。 PROC_RUNNABLE：这个状态表示进程已经准备好要执行了，只需要操作系统给他分配相应的CPU资源就可以运行。 PROC_ZOMBIE：这个状态表示进程已经退出，相应的资源被回收（大部分），almost dead。 一个进程的生命周期一般由如下过程组成： 1. 刚刚开始初始化，进程处在PROC_UNINIT的状态 2. 进程已经完成初始化，时刻准备执行，进入PROC_RUNNABLE状态 3. 在调度的时候，调度器选中该进程进行执行，进程处在running的状态 4.(1) 正在运行的进程由于wait等系统调用被阻塞，进入PROC_SLEEPING，等待相应的资源或者信号。 4.(2) 另一种可能是正在运行的进程被外部中断打断，此时进程变为PROC_RUNNABLE状态，等待下次被调用 5. 等待的事件发生，进程又变成PROC_RUNNABLE状态 6. 重复3~6，直到进程执行完毕，通过exit进入PROC_ZOMBIE状态，由父进程对他的资源进行回收，释放进程控制块。至此，这个进程的生命周期彻底结束。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab6/zai-ci-ren-shi-jin-cheng-qie-huan.html":{"url":"lab6/zai-ci-ren-shi-jin-cheng-qie-huan.html","title":"再次认识进程切换","keywords":"","body":"再次认识进程切换 我们在第四章已经简单了解过了在内核启动过程中进程切换的过程，在这一节我们再来重新回顾一下这些内容，并且深入讨论下其中的几个细节。 首先我们需要考虑的是，什么时候可以进行进程的切换。这里可以主要分为下面几种情况： 进程主动放弃当前的CPU资源，比如显式调用wait或sleep通知操作系统当前进程需要等待 进程想要获取的资源当前不可用，比如尝试获得未被释放的锁，或进行磁盘操作的时候 进程由于外部中断被打断进入内核态，内核发现某些条件满足（比如当前进程时间片用尽），进行进程切换 在我们实现的ucore中，内核进程是不可抢占的。这也就意味着当内核执行的时候，另一个内核进程不可以夺走它的的CPU资源。但这是不是就意味着一个内核进程执行的时候，它就会一直执行到结束呢？虽说内核进程不可以抢占，但是它可以主动放弃自己占有的CPU资源。如果不这样设计的话，内核当中很有可能出现各种死锁导致内核崩溃。 另一方面，内核不能相信用户进程不会无限执行下去，所以需要提供手段在用户进程执行的时候打断他，比如时钟中断。在ucore的实现中，用户进程可以随时被打断进入内核，操作系统会检查当前进程是否需要调度，从而把运行的机会交给别的进程。 由于内核进程是不可抢占的，所以我们在内核中有许多地方使用了显式的函数调用来进行调度，主要有以下几个地方： 函数 原因 proc.c/do_exit 用户进程退出，放弃CPU资源 proc.c/do_wait 用户进程等待，放弃CPU资源 proc.c/init_main init线程会等待所有的用户线程执行完毕，之后调用kswapd内核线程回收内存资源 proc.c/cpu_idle idle线程等待处于就绪态的线程，如果有就调用schedule sync.c/lock 如果获取锁失败就进入等待 trap.c/trap 用户进程在被中断打断后，内核会检查是否需要调度，如果是则调用调度器进行调度 当用户进程A发生中断或系统调用之后，首先其中断帧会被保存，CPU进入内核态，执行中断处理函数。在执行完毕中断处理函数后，操作系统检查当前进程是否需要调度，如果需要，就把当前的进程状态保存，switch到另一个进程B中。注意在执行上面的操作的时候，进程A处于内核态，类似的，调度后我们到达的是进程B的内核态。进程B从系统调用中返回，继续执行。如果进程B在中断或系统调用中被调度，控制权可能转交给进程A的内核态，这样进程A从内核态返回后就可以继续执行之前的代码了。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab6/tiao-du-suan-fa-kuang-jia.html":{"url":"lab6/tiao-du-suan-fa-kuang-jia.html","title":"调度算法框架","keywords":"","body":"调度算法框架 结构体 调度算法框架实现为一个结构体，其中保存了各个函数指针。通过实现这些函数指针即可实现各个调度算法。结构体的定义如下： struct sched_class { // 调度类的名字 const char *name; // 初始化run queue void (*init)(struct run_queue *rq); // 把进程放进run queue，这个是run queue的维护函数 void (*enqueue)(struct run_queue *rq, struct proc_struct *proc); // 把进程取出run queue void (*dequeue)(struct run_queue *rq, struct proc_struct *proc); // 选择下一个要执行的进程 struct proc_struct *(*pick_next)(struct run_queue *rq); // 每次时钟中断调用 void (*proc_tick)(struct run_queue *rq, struct proc_struct *proc); }; 所有的进程被组织成一个run_queue数据结构。这个数据结构虽然没有保存在调度类中，但是是由调度类来管理的。目前ucore仅支持单个CPU核心，所以只有一个全局的run_queue。 我们在进程控制块中也记录了一些和调度有关的信息： struct proc_struct { // ... // 表示这个进程是否需要调度 volatile bool need_resched; // run queue的指针 struct run_queue *rq; // 与这个进程相关的run queue表项 list_entry_t run_link; // 这个进程剩下的时间片 int time_slice; // 以下几个都和Stride调度算法实现有关 // 这个进程在优先队列中对应的项 skew_heap_entry_t lab6_run_pool; // 该进程的Stride值 uint32_t lab6_stride; // 该进程的优先级 uint32_t lab6_priority; }; 前面的几个成员变量的含义都比较直接，最后面的几个的含义可以参见Stride调度算法。这也是本次lab的实验内容。 结构体run_queue实现了运行队列，其内部结构如下： struct run_queue { // 保存着链表头指针 list_entry_t run_list; // 运行队列中的线程数 unsigned int proc_num; // 最大的时间片大小 int max_time_slice; // Stride调度算法中的优先队列 skew_heap_entry_t *lab6_run_pool; }; RR算法 有了这些基础，我们就来实现一个最简单的调度算法：Round-Robin调度算法，也叫时间片轮转调度算法。 时间片轮转调度算法非常简单。它为每一个进程维护了一个最大运行时间片。当一个进程运行够了其最大运行时间片那么长的时间后，调度器会把它标记为需要调度，并且把它的进程控制块放在队尾，重置其时间片。这种调度算法保证了公平性，每个进程都有均等的机会使用CPU，但是没有区分不同进程的优先级（这个也就是在Stride算法中需要考虑的问题）。下面我们来实现以下时间片轮转算法相对应的调度器借口吧！ 首先是enqueue操作。RR算法直接把需要入队的进程放在调度队列的尾端，并且如果这个进程的剩余时间片为0（刚刚用完时间片被收回CPU），则需要把它的剩余时间片设为最大时间片。具体的实现如下： static void RR_enqueue(struct run_queue *rq, struct proc_struct *proc) { assert(list_empty(&(proc->run_link))); list_add_before(&(rq->run_list), &(proc->run_link)); if (proc->time_slice == 0 || proc->time_slice > rq->max_time_slice) { proc->time_slice = rq->max_time_slice; } proc->rq = rq; rq->proc_num ++; } dequeue操作非常普通，将相应的项从队列中删除即可： static void RR_dequeue(struct run_queue *rq, struct proc_struct *proc) { assert(!list_empty(&(proc->run_link)) && proc->rq == rq); list_del_init(&(proc->run_link)); rq->proc_num --; } pick_next选取队列头的表项，用le2proc函数获得对应的进程控制块，返回： static struct proc_struct * RR_pick_next(struct run_queue *rq) { list_entry_t *le = list_next(&(rq->run_list)); if (le != &(rq->run_list)) { return le2proc(le, run_link); } return NULL; } proc_tick函数在每一次时钟中断调用。在这里，我们需要对当前正在运行的进程的剩余时间片减一。如果在减一后，其剩余时间片为0，那么我们就把这个进程标记为“需要调度”，这样在中断处理完之后内核判断进程是否需要调度的时候就会把它进行调度： static void RR_proc_tick(struct run_queue *rq, struct proc_struct *proc) { if (proc->time_slice > 0) { proc->time_slice --; } if (proc->time_slice == 0) { proc->need_resched = 1; } } 至此我们就实现完了和时间片轮转算法相关的所有重要接口。类似于RR算法，我们也可以参照这个方法实现自己的调度算法。本次实验中需要同学们自己实现Stride调度算法。 Stride算法 考察round-robin调度器，在假设所有进程都充分使用了其拥有的 CPU 时间资源的情况下，所有进程得到的 CPU 时间应该是相等的。但是有时候我们希望调度器能够更智能地为每个进程分配合理的 CPU 资源。假设我们为不同的进程分配不同的优先级，则我们有可能希望每个进程得到的时间资源与他们的优先级成正比关系。Stride调度是基于这种想法的一个较为典型和简单的算法。除了简单易于实现以外，它还有如下的特点： 可控性：如我们之前所希望的，可以证明 Stride Scheduling对进程的调度次数正比于其优先级。 确定性：在不考虑计时器事件的情况下，整个调度机制都是可预知和重现的。该算法的基本思想可以考虑如下： 为每个runnable的进程设置一个当前状态stride，表示该进程当前的调度权。另外定义其对应的pass值，表示对应进程在调度后，stride 需要进行的累加值。 每次需要调度时，从当前 runnable 态的进程中选择 stride最小的进程调度。 对于获得调度的进程P，将对应的stride加上其对应的步长pass（只与进程的优先权有关系）。 在一段固定的时间之后，回到 2.步骤，重新调度当前stride最小的进程。 可以证明，如果令 P.pass =BigStride / P.priority 其中P.priority表示进程的优先权（大于 1），而 BigStride表示一个预先定义的大常数，则该调度方案为每个进程分配的时间将与其优先级成正比。证明过程我们在这里略去，有兴趣的同学可以在网上查找相关资料。将该调度器应用到 ucore 的调度器框架中来，则需要将调度器接口实现如下： init: 初始化调度器类的信息（如果有的话）。 初始化当前的运行队列为一个空的容器结构。（比如和RR调度算法一样，初始化为一个有序列表） enqueue 初始化刚进入运行队列的进程 proc的stride属性。 将 proc插入放入运行队列中去（注意：这里并不要求放置在队列头部）。 dequeue 从运行队列中删除相应的元素。 pick_next 扫描整个运行队列，返回其中stride值最小的对应进程。 更新对应进程的stride值，即pass = BIG_STRIDE / P->priority; P->stride += pass。 proc_tick: 检测当前进程是否已用完分配的时间片。如果时间片用完，应该正确设置进程结构的相关标记来引起进程切换。 一个 process 最多可以连续运行 rq.max_time_slice个时间片。 [!NOTE|style:flat] 在具体实现时，有一个需要注意的地方：stride属性的溢出问题，在之前的实现里面我们并没有考虑 stride 的数值范围，而这个值在理论上是不断增加的，在 stride溢出以后，基于stride的比较可能会出现错误。比如假设当前存在两个进程A和B，stride属性采用16位无符号整数进行存储。当前队列中元素如下（假设当前运行的进程已经被重新放置进运行队列中）： A.stride(实际值) A.stride(理论值) A.pass(=BigStride/A.priority) 65534 65534 100 B.stride(实际值) B.stride(理论值) B.pass(=BigStride/B.priority) 65535 65535 50 此时应该选择A作为调度的进程，而在一轮调度后，队列将如下： A.stride(实际值) A.stride(理论值) A.pass(=BigStride/A.priority) 98 65634 100 B.stride(实际值) B.stride(理论值) B.pass(=BigStride/B.priority) 65535 65535 50 可以看到由于溢出的出现，进程间stride的理论比较和实际比较结果出现了偏差。我们首先在理论上分析这个问题：令PASS_MAX为当前所有进程里最大的步进值。则我们可以证明如下结论：对每次Stride调度器的调度步骤中，有其最大的步进值STRIDE_MAX和最小的步进值STRIDE_MIN之差： STRIDE_MAX – STRIDE_MIN 有了该结论，在加上之前对优先级有Priority > 1限制，我们有STRIDE_MAX – STRIDE_MIN ,于是我们只要将BigStride取在某个范围之内，即可保证对于任意两个 Stride 之差都会在机器整数表示的范围之内。而我们可以通过其与0的比较结构，来得到两个Stride的大小关系。在上例中，虽然在直接的数值表示上 98 65535。基于这种特殊考虑的比较方法，即便Stride有可能溢出，我们仍能够得到理论上的当前最小Stride，并做出正确的调度决定。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab6/xiang-mu-zu-cheng-yu-zhi-hang-liu.html":{"url":"lab6/xiang-mu-zu-cheng-yu-zhi-hang-liu.html","title":"项目组成与执行流","keywords":"","body":"项目组成与执行流 项目组成 lab6 ├── Makefile ├── kern │ ├── debug │ │ ├── assert.h │ │ ├── kdebug.c │ │ ├── kdebug.h │ │ ├── kmonitor.c │ │ ├── kmonitor.h │ │ ├── panic.c │ │ └── stab.h │ ├── driver │ │ ├── clock.c │ │ ├── clock.h │ │ ├── console.c │ │ ├── console.h │ │ ├── ide.c │ │ ├── ide.h │ │ ├── intr.c │ │ ├── intr.h │ │ ├── kbdreg.h │ │ ├── picirq.c │ │ └── picirq.h │ ├── fs │ │ ├── fs.h │ │ ├── swapfs.c │ │ └── swapfs.h │ ├── init │ │ ├── entry.S │ │ └── init.c │ ├── libs │ │ ├── readline.c │ │ └── stdio.c │ ├── mm │ │ ├── default_pmm.c │ │ ├── default_pmm.h │ │ ├── kmalloc.c │ │ ├── kmalloc.h │ │ ├── memlayout.h │ │ ├── mmu.h │ │ ├── pmm.c │ │ ├── pmm.h │ │ ├── swap.c │ │ ├── swap.h │ │ ├── swap_fifo.c │ │ ├── swap_fifo.h │ │ ├── vmm.c │ │ └── vmm.h │ ├── process │ │ ├── entry.S │ │ ├── proc.c │ │ ├── proc.h │ │ └── switch.S │ ├── schedule │ │ ├── default_sched.h │ │ ├── default_sched_c │ │ ├── default_sched_stride.c │ │ ├── sched.c │ │ └── sched.h │ ├── sync │ │ └── sync.h │ ├── syscall │ │ ├── syscall.c │ │ └── syscall.h │ └── trap │ ├── trap.c │ ├── trap.h │ └── trapentry.S ├── libs │ ├── atomic.h │ ├── defs.h │ ├── elf.h │ ├── error.h │ ├── hash.c │ ├── list.h │ ├── printfmt.c │ ├── rand.c │ ├── riscv.h │ ├── sbi.h │ ├── skew_heap.h │ ├── stdarg.h │ ├── stdio.h │ ├── stdlib.h │ ├── string.c │ ├── string.h │ └── unistd.h ├── tools │ ├── boot.ld │ ├── function.mk │ ├── gdbinit │ ├── grade.sh │ ├── kernel.ld │ ├── sign.c │ ├── user.ld │ └── vector.c └── user ├── badarg.c ├── badsegment.c ├── divzero.c ├── exit.c ├── faultread.c ├── faultreadkernel.c ├── forktest.c ├── forktree.c ├── hello.c ├── libs │ ├── initcode.S │ ├── panic.c │ ├── stdio.c │ ├── syscall.c │ ├── syscall.h │ ├── ulib.c │ ├── ulib.h │ └── umain.c ├── matrix.c ├── pgdir.c ├── priority.c ├── softint.c ├── spin.c ├── testbss.c ├── waitkill.c └── yield.c 16 directories, 105 files 优先队列 libs/skew_heap.h: 提供了基本的优先队列数据结构，为本次实验提供了抽象数据结构方面的支持。 调度器框架 kern/schedule/sched.[ch]: 定义了 ucore 的调度器框架，其中包括相关的数据结构（包括调度器的接口和运行队列的结构），和具体的运行时机制。 RR算法 kern/schedule/default_sched.[ch]: 具体的 round-robin 算法，在本次实验中你需要了解其实现。 Stride算法 kern/schedule/default_sched_stride.c: Stride Scheduling调度器的基本框架，在此次实验中你需要填充其中的空白部分以实现一个完整的 Stride 调度器。 执行流 结合前面所述自行理解、总结。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab7/":{"url":"lab7/","title":"LAB7：同步互斥","keywords":"","body":"LAB7：同步互斥 在之前的几章中我们已经实现了进程以及调度算法，可以让多个进程并发的执行。在现实的系统当中，有许多多线程的系统都需要协同的完成某一项任务。但是在协同的过程中，存在许多资源共享的问题，比如对一个文件读写的并发访问等等。这些问题需要我们提供一些同步互斥的机制来让程序可以有序的、无冲突的完成他们的工作，这也是这一章内我们要解决的问题。 我们最终实现的目标是解决“哲学家就餐问题”。“哲学家就餐问题”是一个非常有名的同步互斥问题：有五个哲学家围成一圈吃饭，每两个哲学家中间有一根筷子。每个需要就餐的哲学家需要两根筷子才可以就餐。哲学家处于两种状态之间：思考和饥饿。当哲学家处于思考的状态时，哲学家便无欲无求；而当哲学家处于饥饿状态时，他必须通过就餐来解决饥饿，重新回到思考的状态。如何让这5个哲学家可以不发生死锁的把这一顿饭吃完就是我们要解决的目标。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab7/shi-yan-mu-de.html":{"url":"lab7/shi-yan-mu-de.html","title":"实验目的","keywords":"","body":"实验目的 理解操作系统的同步互斥的设计实现 掌握在ucore中信号量机制的具体实现 理解管程机制，在ucore中增加基于管程的条件变量的支持 了解经典进程同步问题，并能使用同步机制解决进程同步问题 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab7/shi-yan-nei-rong.html":{"url":"lab7/shi-yan-nei-rong.html","title":"实验内容","keywords":"","body":"实验内容 阅读教材与实验指导书。 分析信号量与条件变量的实现。 设计为用户态进程提供信号量机制与条件变量机制的方案。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab7/lian-xi.html":{"url":"lab7/lian-xi.html","title":"练习","keywords":"","body":"练习 练习1：说明不会出现死锁的原因 证明/说明为什么我们给出的信号量实现的哲学家问题不会出现死锁。不必特别严谨，但要能说服你自己/助教？ 证明/说明为什么我们给出的条件变量实现的哲学家问题不会出现死锁。不必特别严谨，但要能说服你自己/助教？ 练习2：设计方案 给出为用户态进程/线程提供信号量机制的设计方案，并比较说明给内核级提供信号量机制的异同。 给出为用户态进程/线程提供条件变量机制的设计方案，并比较说明给内核级提供条件变量机制的异同。 练习3：信号量实现条件变量 能否不基于信号量机制来完成条件变量？如果不能，请给出理由，如果能，请给出设计说明和具体实现。 练习4：禁用中断 kern/sync/sem.c的信号量实现中，出现了这样的暂时禁用中断的代码： static __noinline uint32_t __down(semaphore_t *sem, uint32_t wait_state) { bool intr_flag; local_intr_save(intr_flag); /*some code*/ local_intr_restore(intr_flag); } 我们其实从正确的框架里删除了两句local_intr_restore(intr_flag)和一句local_intr_save(intr_flag);，请你找出是哪里删掉了，并举出删掉之后不能正常工作的一种情况。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab7/tong-bu-hu-chi-de-ji-ben-gai-nian.html":{"url":"lab7/tong-bu-hu-chi-de-ji-ben-gai-nian.html","title":"同步互斥的基本概念","keywords":"","body":"同步互斥的基本概念 虽然我们经常把同步和互斥放在一起说，但是这两个词是两个概念。同步指的是进程间的执行需要按照某种先后顺序，即访问是有序的。互斥指的是对于某些共享资源的访问不能同时进行，同一时间只能有一定数量的进程进行。这两种情况基本构成了我们在多线程执行中遇到的各种问题。 与同步互斥相关的另一个概念是临界区。临界区指的是进程的一段代码，其特征要求了同一时间段只能有一个进程执行，否则就有可能出现问题。一般而言，进程处理临界区的思路是设计一个协议，不同的进程遵守这个相同的协议来进行临界区的协调。在进入临界区前，进程请求进入的许可，这段代码称为进入区；退出临界区时，进程应该通过协议告知别的进程自己已经使用完临界区，这段代码称为退出区；临界区其他的部分称为剩余区。我们本章解决的问题，就是在进入区和退出区为进程提供同步互斥的机制。 为了提供同步互斥机制，操作系统有多种实现方法，包括时钟中断管理，屏蔽使能中断，等待队列，信号量，管程等等。下面我们来分别看一看上面提到的部分机制： 时钟中断管理：在第一个lab中我们已经实现了时钟中断。通过时钟中断，操作系统可以提供基于时间节点的事件。通过时钟中断，操作系统可以提供任意长度的等待唤醒机制，由此可以给应用程序机会来实现更加复杂的自定义调度操作。 屏蔽使能中断：这部分主要是处理内核内的同步互斥问题。因为内核在执行的过程中可能会被外部的中断打断，我们实现的ucore也是不可抢占的系统，所以可以在操作系统进行某些需要同步互斥的操作的时候先禁用中断，等执行完之后再使能。这样保证了操作系统在执行临界区的时候不会被打断，也就实现了同步互斥。在ucore中经常可以看到下面这样的代码： ...... local_intr_save(intr_flag); { 临界区代码 } local_intr_restore(intr_flag); ...... 这段代码就是使用屏蔽使能中断处理内核同步互斥问题的例子。 等待队列：等待队列是操作系统提供的一种事件机制。一些进程可能会在执行的过程中等待某些特定事件的发生，这个时候进程进入睡眠状态。操作系统维护一个等待队列，把这个进程放进他等待的事件的等待队列中。当对应的事件发生之后，操作系统就唤醒相应等待队列中的进程。这也是ucore内部实现信号量的机制。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab7/xin-hao-liang.html":{"url":"lab7/xin-hao-liang.html","title":"信号量","keywords":"","body":"信号量 介绍 信号量（semaphore）是一种同步互斥的实现。semaphore一词来源于荷兰语，原来是指火车信号灯。想象一些火车要进站，火车站里有n个站台，那么同一时间只能有n辆火车进站装卸货物。当火车站里已经有了n辆火车，信号灯应该通知后面的火车不能进站了。当有火车出站之后，信号灯应该告诉后面的火车可以进站。 这个问题放在操作系统的语境下就是有一个共享资源只能支持n个线程并行的访问，信号量统计目前有多少进程正在访问，当同时访问的进程数小于n时就可以让新的线程进入，当同时访问的进程数为n时想要访问的进程就需要等待。 在信号量中，一般用两种操作来刻画申请资源和释放资源：P操作申请一份资源，如果申请不到则等待；V操作释放一份资源，如果此时有进程正在等待，则唤醒该进程。在ucore中，我们使用down函数实现P操作，up函数实现V操作。 结构体 首先是信号量结构体的定义： typedef struct { int value; wait_queue_t wait_queue; } semaphore_t; 其中的value表示信号量的值，其正值表示当前可用的资源数量，负值表示正在等待资源的进程数量。wait_queue即为这个信号量相对应的等待队列。 P操作 down函数实现的是P操作。首先关闭中断，然后判断信号量的值是否为正，如果是正值说明进程可以获得信号量，将信号量的值减一，打开中断然后函数返回即可。否则表示无法获取信号量，将自己的进程保存进等待队列，打开中断，调用schedule函数进行调度。等到V操作唤醒进程的时候，其会回到调用schedule函数后面，将自身从等待队列中删除（此过程需要关闭中断）并返回即可。具体的实现如下： static __noinline uint32_t __down(semaphore_t *sem, uint32_t wait_state) { bool intr_flag; local_intr_save(intr_flag); if (sem->value > 0) { sem->value --; local_intr_restore(intr_flag); return 0; } wait_t __wait, *wait = &__wait; wait_current_set(&(sem->wait_queue), wait, wait_state); local_intr_restore(intr_flag); schedule(); local_intr_save(intr_flag); wait_current_del(&(sem->wait_queue), wait); local_intr_restore(intr_flag); if (wait->wakeup_flags != wait_state) { return wait->wakeup_flags; } return 0; } V操作 up函数实现了V操作。首先关闭中断，如果释放的信号量没有进程正在等待，那么将信号量的值加一，打开中断直接返回即可。如果有进程正在等待，那么唤醒这个进程，把它放进就绪队列，把信号量的值加一，打开中断并返回。具体的实现如下： static __noinline void __up(semaphore_t *sem, uint32_t wait_state) { bool intr_flag; local_intr_save(intr_flag); { wait_t *wait; if ((wait = wait_queue_first(&(sem->wait_queue))) == NULL) { sem->value ++; } else { assert(wait->proc->wait_state == wait_state); wakeup_wait(&(sem->wait_queue), wait, wait_state, 1); } } local_intr_restore(intr_flag); } Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab7/tiao-jian-bian-liang-yu-guan-cheng.html":{"url":"lab7/tiao-jian-bian-liang-yu-guan-cheng.html","title":"条件变量与管程","keywords":"","body":"条件变量与管程 介绍 我们已经有了基本的同步互斥机制实现，下面让我们用这些机制来实现一个管程，从而解决哲学家就餐问题。 为什么要使用管程呢？引入管程相当于将底层的同步互斥机制封装了起来，对外提供已经经过同步的接口供进程使用，大大降低了并行进程开发的门槛。管程主要由四个部分组成： 管程内部的共享变量 管程内部的条件变量 管程内部并发执行的进程 对局部于管程内部的共享数据设置初始值的语句 由此可见，管程把需要互斥访问的变量直接包装了起来，对共享变量的访问只能通过管程提供的相应接口，方便了多进程的编写。但是管程只有同步互斥是不够的，可能需要条件变量。条件变量类似于信号量，只不过在信号量中进程等待某一个资源可用，而条件变量中进程等待条件变量相应的资源为真。 结构体 条件变量的结构体如下： typedef struct condvar{ // 信号量 semaphore_t sem; // 正在等待的线程数 int count; // 自己属于哪一个管程 monitor_t * owner; } condvar_t; 等待唤醒 我们主要需要实现两个函数：wait函数，等待某一个条件；signal函数，提醒某一个条件已经达成。具体实现比较简单，可以参考代码如下： // wait cv.count++; if(monitor.next_count > 0) sem_signal(monitor.next); else sem_signal(monitor.mutex); sem_wait(cv.sem); cv.count -- ; // signal if( cv.count > 0) { monitor.next_count ++; sem_signal(cv.sem); sem_wait(monitor.next); monitor.next_count -- ; } 实现 管程的内部实现如下所示： typedef struct monitor{ // 保证管程互斥访问的信号量 semaphore_t mutex; // 里面放着正在等待进入管程执行的进程 semaphore_t next; // 正在等待进入管程的进程数 int next_count; // 条件变量 condvar_t *cv; } monitor_t; 条件变量cv被设置时，会使得当前在管程内的进程等待条件变量而睡眠，其他进程进入管程执行。当cv被唤醒的时候，之前等待这个条件变量的进程也会被唤醒，进入管程执行。由于管程内部只能由一个条件变量，所以通过设置next来维护下一个要运行的进程是哪一个。 使用了管程，我们的哲学家就餐问题可以被实现为如下： monitor dp { enum {THINKING, HUNGRY, EATING} state[5]; condition self[5]; void pickup(int i) { state[i] = HUNGRY; test(i); if (state[i] != EATING) self[i].wait_cv(); } void putdown(int i) { state[i] = THINKING; test((i + 4) % 5); test((i + 1) % 5); } void test(int i) { if ((state[(i + 4) % 5] != EATING) && (state[i] == HUNGRY) && (state[(i + 1) % 5] != EATING)) { state[i] = EATING; self[i].signal_cv(); } } initialization code() { for (int i = 0; i Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab7/xiang-mu-zu-cheng-yu-zhi-hang-liu.html":{"url":"lab7/xiang-mu-zu-cheng-yu-zhi-hang-liu.html","title":"项目组成与执行流","keywords":"","body":"项目组成与执行流 项目组成 lab7 ├── Makefile ├── kern │ ├── debug │ │ ├── assert.h │ │ ├── kdebug.c │ │ ├── kdebug.h │ │ ├── kmonitor.c │ │ ├── kmonitor.h │ │ ├── panic.c │ │ └── stab.h │ ├── driver │ │ ├── clock.c │ │ ├── clock.h │ │ ├── console.c │ │ ├── console.h │ │ ├── ide.c │ │ ├── ide.h │ │ ├── intr.c │ │ ├── intr.h │ │ ├── kbdreg.h │ │ ├── picirq.c │ │ └── picirq.h │ ├── fs │ │ ├── fs.h │ │ ├── swapfs.c │ │ └── swapfs.h │ ├── init │ │ ├── entry.S │ │ └── init.c │ ├── libs │ │ ├── readline.c │ │ └── stdio.c │ ├── mm │ │ ├── default_pmm.c │ │ ├── default_pmm.h │ │ ├── kmalloc.c │ │ ├── kmalloc.h │ │ ├── memlayout.h │ │ ├── mmu.h │ │ ├── pmm.c │ │ ├── pmm.h │ │ ├── swap.c │ │ ├── swap.h │ │ ├── swap_fifo.c │ │ ├── swap_fifo.h │ │ ├── vmm.c │ │ └── vmm.h │ ├── process │ │ ├── entry.S │ │ ├── proc.c │ │ ├── proc.h │ │ └── switch.S │ ├── schedule │ │ ├── default_sched.h │ │ ├── default_sched_c │ │ ├── default_sched_stride.c │ │ ├── sched.c │ │ └── sched.h │ ├── sync │ │ ├── check_sync.c │ │ ├── monitor.c │ │ ├── monitor.h │ │ ├── sem.c │ │ ├── sem.h │ │ ├── sync.h │ │ ├── wait.c │ │ └── wait.h │ ├── syscall │ │ ├── syscall.c │ │ └── syscall.h │ └── trap │ ├── trap.c │ ├── trap.h │ └── trapentry.S ├── lab5.md ├── libs │ ├── atomic.h │ ├── defs.h │ ├── elf.h │ ├── error.h │ ├── hash.c │ ├── list.h │ ├── printfmt.c │ ├── rand.c │ ├── riscv.h │ ├── sbi.h │ ├── skew_heap.h │ ├── stdarg.h │ ├── stdio.h │ ├── stdlib.h │ ├── string.c │ ├── string.h │ └── unistd.h ├── tools │ ├── boot.ld │ ├── function.mk │ ├── gdbinit │ ├── grade.sh │ ├── kernel.ld │ ├── sign.c │ ├── user.ld │ └── vector.c └── user ├── badarg.c ├── badsegment.c ├── divzero.c ├── exit.c ├── faultread.c ├── faultreadkernel.c ├── forktest.c ├── forktree.c ├── hello.c ├── libs │ ├── initcode.S │ ├── panic.c │ ├── stdio.c │ ├── syscall.c │ ├── syscall.h │ ├── ulib.c │ ├── ulib.h │ └── umain.c ├── matrix.c ├── pgdir.c ├── priority.c ├── sleep.c ├── sleepkill.c ├── softint.c ├── spin.c ├── testbss.c ├── waitkill.c └── yield.c 16 directories, 115 files 等待队列 kern/sync/wait.[ch]: 定义了等待队列wait_queue结构和等待entry的wait结构以及在此之上的函数，这是ucore中的信号量semophore机制和条件变量机制的基础。 信号量 kern/sync/sem.[ch]:定义并实现了ucore中内核级信号量相关的数据结构和函数。 条件变量与管程 kern/sync/monitor.[ch]:基于管程的条件变量的实现程序。 执行流 结合前面所述自行理解、总结。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab8/":{"url":"lab8/","title":"LAB8：文件系统","keywords":"","body":"LAB8：文件系统 我们来到了最后一个lab。文件系统(file system)，指的是操作系统中管理（硬盘上）持久存储数据的模块。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab8/shi-yan-mu-de.html":{"url":"lab8/shi-yan-mu-de.html","title":"实验目的","keywords":"","body":"实验目的 了解基本的文件系统系统调用的实现方法； 了解一个基于索引节点组织方式的Simple FS文件系统的设计与实现； 了解文件系统抽象层-VFS的设计与实现； Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab8/shi-yan-nei-rong.html":{"url":"lab8/shi-yan-nei-rong.html","title":"实验内容","keywords":"","body":"实验内容 通过分析了解ucore文件系统的总体架构设计，完善读写文件操作。 重新实现基于文件系统的执行程序机制。 完成执行存储在磁盘上的文件和实现文件读写等功能。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab8/lian-xi.html":{"url":"lab8/lian-xi.html","title":"练习","keywords":"","body":"练习 练习1：填写sfs_io_nolock()函数 首先了解打开文件的处理流程，然后参考本实验后续的文件读写操作的过程分析，填写在 kern/fs/sfs/sfs_inode.c中 的sfs_io_nolock()函数，实现读文件中数据的代码。 练习2：填写load_icode()函数 改写 proc.c 中的 load_icode 函数和其他相关函数，实现基于文件系统的执行程序机制。执行：make qemu。如果能看看到 sh 用户程序的执行界面，则基本成功了。如果在 sh 用户界面上可以执行”ls”,”hello”等其他放置在 sfs 文件系统中的其他执行程序，则可以认为本实验基本成功。 练习3：管道（Pipe)机制 如果要在ucore里加入UNIX的管道（Pipe)机制，至少需要定义哪些数据结构和接口？（接口给出语义即可，不必具体实现。数据结构的设计应当给出一个(或多个）具体的C语言struct定义。你的设计应当体现出对可能出现的同步互斥问题的处理。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab8/wen-jian-xi-tong-jie-shao.html":{"url":"lab8/wen-jian-xi-tong-jie-shao.html","title":"文件系统介绍","keywords":"","body":"文件系统介绍 为什么需要文件/文件系统 能够\"持久存储数据的设备\", 可能包括: 机械硬盘(HDD, hard disk drive), 固态硬盘(solid-state storage device), 光盘（加上它的驱动器），软盘，U盘，甚至磁带或纸带等等。一般来说，每个设备上都会连出很多针脚（这些针脚很可能符合某个特定协议，如USB协议，SATA协议），它们都可以按照事先约定的接口/协议把数据从设备中读到内存里，或者把内存里的数据按照一定的格式储存到设备里。 我们希望做到的第一件事情，就是把以上种种存储设备当作“同样的设备”进行使用。不管机械硬盘的扇区有多少个，或者一块固态硬盘里有多少存储单元，我们希望都能用同样的接口进行使用，提供“把硬盘的第a到第b个字节读出来”和“把内存里这些内容写到硬盘的从第a个字节开始的位置”的接口，在使用时只会感受到不同设备速度的不同。否则，我们还需要自己处理什么SATA协议，NVMe协议啥的。处理具体设备，具体协议并向上提供简单接口的软件，我们叫做设备驱动(device driver)，简称驱动。 文件的概念在我们脑子里根深蒂固，但“文件”其实也是一种抽象。理论上，只要提供了上面提到的读写两个接口，我们就可以进行编程，而并不需要什么文件的概念。只要你自己在小本本上记住，你的某些数据存储在硬盘上从某个地址开始的多么长的位置，以及硬盘上哪些位置是没有被占用的。编程的时候，如果用到/修改硬盘上的数据， 就把小本本上记录的位置给硬编码到程序里。 但这很不灵活！如果你的小本本丢了，你就再也无法使用硬盘里的数据了，因为你不知道那些数据都是谁跟谁。另外，你也可能一不小心修改到无关的数据。最后，这个小本本经常需要修改，你很容易出错。 显然，我们应该把这个小本本交给计算机来保存和自动维护。这就是 文件系统。 我们把一段逻辑上相关联的数据看作一个整体，叫做文件。 除了硬盘，我们还可以把其他设备（如标准输入，标准输出）看成是文件，由文件系统进行统一的管理。之前我们模拟过一个很简单的文件系统，用来存放内存置换出的页面（参见lab3 页面置换）。现在我们要来实现功能更强大也更复杂的文件系统。 虚拟文件系统 我们可以实现一个虚拟文件系统（virtual filesystem, VFS）, 作为操作系统和更具体的文件系统之间的接口。所谓“具体文件系统”，更接近具体设备和文件系统的内部实现，而“虚拟文件系统”更接近用户使用的接口。 电脑上本地的硬盘和远程的云盘(例如清华云盘，OneDrive）的具体文件管理方式很可能不同，但是操作系统可以让我们用相同的操作访问这两个地方的文件，可以认为用到了虚拟文件系统。 例如，远程的云盘OneDrive，在Windows资源管理器(可以看作是虚拟文件系统提供给用户的接口)里，看起来和本地磁盘上的文件夹一模一样，也可以做和本地的文件夹相同的操作，复制，粘贴，打开。虽然背后有着网络传输，可能速度会慢，但是接口一致。（理论上可以在OneDrive的远程存储服务器上使用linux操作系统）。 在Linux系统中，如/floppy是一块MS-DOS文件系统的软盘的挂载点，/tmp/test是Ext2文件系统的一个目录，我们执行cp /floppy/TEST /tmp/test, 进行目录的拷贝，相当于执行下面这段代码： inf = open(\"/floppy/TEST\", O_RDONLY, 0); outf = open(\"/tmp/test\", O_WRONLY|O_CREAT|O_TRUNC, 0600); do { i = read(inf, buf, 4096); write(outf, buf, i); } while (i); close(outf); close(inf); 对于不同文件系统的目录，我们可以使用相同的open, read, write, close接口，好像它们在同一个文件系统里一样。这是虚拟文件系统的功能。 UNIX文件系统 ucore 的文件系统模型和传统的 UNIX文件系统类似。 UNIX 文件中的内容可理解为是一段有序的字节 ，占用磁盘上可能连续或不连续的一些空间（实际占用的空间可能比你存储的数据要多）。每个文件都有一个方便应用程序识别的文件名（也可以称作路径path），另外有一个文件系统内部使用的编号（用户程序不知道这个底层编号）。你可以对着一个文件又读又写（写的太多的时候可能会自动给文件分配更多的硬盘存储空间），也可以创建或删除文件。 目录(directory)是特殊的文件，一个目录里包含若干其他文件或目录。 在 UNIX 中，文件系统可以被安装在一个特定的文件路径位置，这个位置就是挂载点（mount point)。所有的已安装文件系统都作为根文件系统树中的叶子出现在系统中。比如当你把U盘插进来，系统检测到U盘之后，会给U盘的文件系统一个挂载点，这个挂载点是原先的UNIX操作系统的叶子，但也可以认为是U盘文件系统的根节点。 UNIX的文件系统中，有一个通用文件模型（Common File Model），所有具体的文件系统（不管是Ext4, ZFS还是FAT)，都需要提供通用文件模型所约定的行为。我们可以认为，通用文件模型是面向对象的，由若干对象(Object)组成，每个对象有成员属性和函数接口。类UNIX系统的内核一般使用C语言实现，”成员函数“一般体现为函数指针。 通用文件模型定义了一些对象: 超级块(superblock)：存储整个文件系统的相关信息。对于磁盘上的文件系统，对应磁盘里的文件系统控制块(filesystem control block) 索引节点（inode)：存储关于某个文件的元数据信息（如访问控制权限、大小、拥有者、创建时间、数据内容等等），通常对应磁盘上的文件控制块（file control block). 每个索引节点有一个编号，唯一确定文件系统里的一个文件。 文件(file): 这里定义的file object不是指磁盘上的一个”文件“， 而是指一个进程和它打开的一个文件之间的关系，这个对象存储在内核态的内存中，仅当某个进程打开某个文件的时候才存在。 目录项（dentry）：维护从”目录里的某一项“到”对应的文件“的链接/指针。一个目录也是一个文件，包含若干个子目录和其他文件。从某个子目录、文件的名称，对应到具体的文件/子目录的地址(或者索引节点inode)的链接，通过目录项(dentry)来描述。 上述抽象概念形成了 UNIX 文件系统的逻辑数据结构，并需要通过一个具体文件系统的架构，把上述信息映射并储存到磁盘介质上，从而在具体文件系统的磁盘布局（即数据在磁盘上的物理组织）上体现出上述抽象概念。比如文件元数据信息存储在磁盘块中的索引节点上。当文件被载入内存时，内核需要使用磁盘块中的索引点来构造内存中的索引节点。又比如dentry对象在磁盘上不存在，但是当一个目录包含的某一项（可能是子目录或文件）的信息被载入到内存时，内核会构建对应的dentry对象，如/tmp/test这个路径，在解析的过程中，内核为根目录/创建一个dentry对象，为根目录的成员tmp构建一个dentry对象，为/tmp目录的成员test也构建一个dentry对象。 ucore 文件系统总体介绍 我们将在ucore里用虚拟文件系统管理三类设备： 硬盘，我们管理硬盘的具体文件系统是Simple File System（地位和Ext2等文件系统相同） 标准输出（控制台输出），只能写不能读 标准输入（键盘输入），只能读不能写 其中，标准输入和标准输出都是比较简单的设备。管理硬盘的Simple File System相对而言比较复杂。 我们的“硬盘”依然需要通过用一块内存来模拟。 lab8的Makefile和之前不同，我们分三段构建内核镜像。 sfs.img: 一块符合SFS文件系统的硬盘，里面存储编译好的用户程序 swap.img: 一段初始化为0的硬盘交换区 kernel objects: ucore内核代码的目标文件 这三部分共同组成ucore.img, 加载到QEMU里运行。ucore代码中，我们通过链接时添加的首尾符号，把swap.img和sfs.img两段“硬盘”（实际上对应两段内存空间）找出来，然后作为“硬盘”进行管理。 注意，我们要在ucore内核开始执行之前，构造好“一块符合SFS文件系统的硬盘”，这就得另外写个程序做这个事情。这个程序就是tools/mksfs.c。它有500多行，如果感兴趣的话可以通过它了解Simple File System的结构。 ucore 模仿了 UNIX 的文件系统设计，ucore 的文件系统架构主要由四部分组成： 通用文件系统访问接口层：该层提供了一个从用户空间到文件系统的标准访问接口。这一层访问接口让应用程序能够通过一个简单的接口获得 ucore 内核的文件系统服务。 文件系统抽象层：向上提供一个一致的接口给内核其他部分（文件系统相关的系统调用实现模块和其他内核功能模块）访问。向下提供一个同样的抽象函数指针列表和数据结构屏蔽不同文件系统的实现细节。 Simple FS 文件系统层：一个基于索引方式的简单文件系统实例。向上通过各种具体函数实现以对应文件系统抽象层提出的抽象函数。向下访问外设接口 外设接口层：向上提供 device 访问接口屏蔽不同硬件细节。向下实现访问各种具体设备驱动的接口，比如 disk 设备接口/串口设备接口/键盘设备接口等。 对照上面的层次我们再大致介绍一下文件系统的访问处理过程，加深对文件系统的总体理解。假如应用程序操作文件（打开/创建/删除/读写），首先需要通过文件系统的通用文件系统访问接口层给用户空间提供的访问接口进入文件系统内部，接着由文件系统抽象层把访问请求转发给某一具体文件系统（比如 SFS 文件系统），具体文件系统（Simple FS 文件系统层）把应用程序的访问请求转化为对磁盘上的 block 的处理请求，并通过外设接口层交给磁盘驱动例程来完成具体的磁盘操作。结合用户态写文件函数 write 的整个执行过程，我们可以比较清楚地看出 ucore 文件系统架构的层次和依赖关系。 ucore 文件系统总体结构 从 ucore 操作系统不同的角度来看，ucore 中的文件系统架构包含四类主要的数据结构, 它们分别是： 超级块（SuperBlock），它主要从文件系统的全局角度描述特定文件系统的全局信息。它的作用范围是整个 OS 空间。 索引节点（inode）：它主要从文件系统的单个文件的角度它描述了文件的各种属性和数据所在位置。它的作用范围是整个 OS 空间。 目录项（dentry）：它主要从文件系统的文件路径的角度描述了文件路径中的一个特定的目录项（注：一系列目录项形成目录/文件路径）。它的作用范围是整个 OS 空间。对于 SFS 而言，inode(具体为 struct sfs_disk_inode)对应于物理磁盘上的具体对象，dentry（具体为 struct sfs_disk_entry）是一个内存实体，其中的 ino 成员指向对应的 inode number，另外一个成员是 file name(文件名). 文件（file），它主要从进程的角度描述了一个进程在访问文件时需要了解的文件标识，文件读写的位置，文件引用情况等信息。它的作用范围是某一具体进程。 如果一个用户进程打开了一个文件，那么在 ucore 中涉及的相关数据结构和关系如下图所示： Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab8/wen-jian-xi-tong-chou-xiang-ceng-vfs.html":{"url":"lab8/wen-jian-xi-tong-chou-xiang-ceng-vfs.html","title":"文件系统抽象层VFS","keywords":"","body":"文件系统抽象层VFS 文件系统抽象层是把不同文件系统的对外共性接口提取出来，形成一个函数指针数组，这样，通用文件系统访问接口层只需访问文件系统抽象层，而不需关心具体文件系统的实现细节和接口。 file&dir接口 file&dir 接口层定义了进程在内核中直接访问的文件相关信息，这定义在 file 数据结构中，具体描述如下： // kern/fs/file.h struct file { enum { FD_NONE, FD_INIT, FD_OPENED, FD_CLOSED, } status; //访问文件的执行状态 bool readable; //文件是否可读 bool writable; //文件是否可写 int fd; //文件在filemap中的索引值 off_t pos; //访问文件的当前位置 struct inode *node; //该文件对应的内存inode指针 int open_count; //打开此文件的次数 }; 而在 kern/process/proc.h中的 proc_struct 结构中加入了描述了进程访问文件的数据接口 files_struct，其数据结构定义如下： // kern/fs/fs.h struct files_struct { struct inode *pwd; //进程当前执行目录的内存inode指针 struct file *fd_array; //进程打开文件的数组 atomic_t files_count; //访问此文件的线程个数 semaphore_t files_sem; //确保对进程控制块中fs_struct的互斥访问 }; 当创建一个进程后，该进程的 files_struct 将会被初始化或复制父进程的 files_struct。当用户进程打开一个文件时，将从 fd_array 数组中取得一个空闲 file 项，然后会把此 file 的成员变量 node 指针指向一个代表此文件的 inode 的起始地址。 inode接口 index node 是位于内存的索引节点，它是 VFS 结构中的重要数据结构，因为它实际负责把不同文件系统的特定索引节点信息（甚至不能算是一个索引节点）统一封装起来，避免了进程直接访问具体文件系统。其定义如下： // kern/vfs/inode.h struct inode { union { //包含不同文件系统特定inode信息的union成员变量 struct device __device_info; //设备文件系统内存inode信息 struct sfs_inode __sfs_inode_info; //SFS文件系统内存inode信息 } in_info; enum { inode_type_device_info = 0x1234, inode_type_sfs_inode_info, } in_type; //此inode所属文件系统类型 atomic_t ref_count; //此inode的引用计数 atomic_t open_count; //打开此inode对应文件的个数 struct fs *in_fs; //抽象的文件系统，包含访问文件系统的函数指针 const struct inode_ops *in_ops; //抽象的inode操作，包含访问inode的函数指针 }; 在 inode 中，有一成员变量为 in_ops，这是对此 inode 的操作函数指针列表，其数据结构定义如下： struct inode_ops { unsigned long vop_magic; int (*vop_open)(struct inode *node, uint32_t open_flags); int (*vop_close)(struct inode *node); int (*vop_read)(struct inode *node, struct iobuf *iob); int (*vop_write)(struct inode *node, struct iobuf *iob); int (*vop_getdirentry)(struct inode *node, struct iobuf *iob); int (*vop_create)(struct inode *node, const char *name, bool excl, struct inode **node_store); int (*vop_lookup)(struct inode *node, char *path, struct inode **node_store); …… }; 参照上面对 SFS 中的索引节点操作函数的说明，可以看出 inode_ops 是对常规文件、目录、设备文件所有操作的一个抽象函数表示。对于某一具体的文件系统中的文件或目录，只需实现相关的函数，就可以被用户进程访问具体的文件了，且用户进程无需了解具体文件系统的实现细节。 open系统调用的执行过程 下面我们通过打开文件的系统调用open()的执行过程, 看看文件系统的不同层次是如何交互的。 首先，经过syscall.c的处理之后，进入内核态，执行sysfile_open()函数 // kern/fs/sysfile.c /* sysfile_open - open file */ int sysfile_open(const char *__path, uint32_t open_flags) { int ret; char *path; if ((ret = copy_path(&path, __path)) != 0) { return ret; } ret = file_open(path, open_flags); kfree(path); return ret; } 可以看到，sysfile_open 把路径复制了一份，然后调用了file_open， file_open调用了vfs_open, 使用了VFS的接口。 // kern/fs/file.c // open file int file_open(char *path, uint32_t open_flags) { bool readable = 0, writable = 0; switch (open_flags & O_ACCMODE) { //解析 open_flags case O_RDONLY: readable = 1; break; case O_WRONLY: writable = 1; break; case O_RDWR: readable = writable = 1; break; default: return -E_INVAL; } int ret; struct file *file; if ((ret = fd_array_alloc(NO_FD, &file)) != 0) { //在当前进程分配file descriptor return ret; } struct inode *node; if ((ret = vfs_open(path, open_flags, &node)) != 0) { //打开文件的工作在vfs_open完成 fd_array_free(file); //打开失败，释放file descriptor return ret; } file->pos = 0; if (open_flags & O_APPEND) { struct stat __stat, *stat = &__stat; if ((ret = vop_fstat(node, stat)) != 0) { vfs_close(node); fd_array_free(file); return ret; } file->pos = stat->st_size; //追加写模式，设置当前位置为文件尾 } file->node = node; file->readable = readable; file->writable = writable; fd_array_open(file); //设置该文件的状态为“打开” return file->fd; } // fs_array_alloc - allocate a free file item (with FD_NONE status) in open files table static int fd_array_alloc(int fd, struct file **file_store) { struct file *file = get_fd_array(); if (fd == NO_FD) { for (fd = 0; fd status == FD_NONE) { goto found; } } return -E_MAX_OPEN; } else { if (testfd(fd)) { file += fd; if (file->status == FD_NONE) { goto found; } return -E_BUSY; } return -E_INVAL; } found: assert(fopen_count(file) == 0); file->status = FD_INIT, file->node = NULL; *file_store = file; return 0; } void fd_array_open(struct file *file) { assert(file->status == FD_INIT && file->node != NULL); file->status = FD_OPENED; //设置状态为“打开” fopen_count_inc(file); //增加文件的“打开计数” } vfs_open是一个比较复杂的函数，这里我们使用的打开文件的flags, 基本是参照linux，如果希望详细了解，可以阅读linux manual: open。 // kern/fs/vfs/vfsfile.c // open file in vfs, get/create inode for file with filename path. int vfs_open(char *path, uint32_t open_flags, struct inode **node_store) { bool can_write = 0; // 解析open_flags并做合法性检查 switch (open_flags & O_ACCMODE) { case O_RDONLY: break; case O_WRONLY: case O_RDWR: can_write = 1; break; default: return -E_INVAL; } if (open_flags & O_TRUNC) { if (!can_write) { return -E_INVAL; } } /* linux manual O_TRUNC If the file already exists and is a regular file and the access mode allows writing (i.e., is O_RDWR or O_WRONLY) it will be truncated to length 0. If the file is a FIFO or ter‐ minal device file, the O_TRUNC flag is ignored. Otherwise, the effect of O_TRUNC is unspecified. */ int ret; struct inode *node; bool excl = (open_flags & O_EXCL) != 0; bool create = (open_flags & O_CREAT) != 0; ret = vfs_lookup(path, &node); // vfs_lookup根据路径构造inode if (ret != 0) {//要打开的文件还不存在，可能出错，也可能需要创建新文件 if (ret == -16 && (create)) { char *name; struct inode *dir; if ((ret = vfs_lookup_parent(path, &dir, &name)) != 0) { return ret;//需要在已经存在的目录下创建文件，目录不存在，则出错 } ret = vop_create(dir, name, excl, &node);//创建新文件 } else return ret; } else if (excl && create) { return -E_EXISTS; /* linux manual O_EXCL Ensure that this call creates the file: if this flag is specified in conjunction with O_CREAT, and pathname already exists, then open() fails with the error EEXIST. */ } assert(node != NULL); if ((ret = vop_open(node, open_flags)) != 0) { vop_ref_dec(node); return ret; } vop_open_inc(node); if (open_flags & O_TRUNC || create) { if ((ret = vop_truncate(node, 0)) != 0) { vop_open_dec(node); vop_ref_dec(node); return ret; } } *node_store = node; return 0; } 我们看看vfs_look_up的实现 /* * get_device- Common code to pull the device name, if any, off the front of a * path and choose the inode to begin the name lookup relative to. */ static int get_device(char *path, char **subpath, struct inode **node_store) { int i, slash = -1, colon = -1; for (i = 0; path[i] != '\\0'; i ++) { if (path[i] == ':') { colon = i; break; } if (path[i] == '/') { slash = i; break; } } if (colon 0) { /* device:path - get root of device's filesystem */ path[colon] = '\\0'; /* device:/path - skip slash, treat as device:path */ while (path[++ colon] == '/'); *subpath = path + colon; return vfs_get_root(path, node_store); } /* * * we have either /path or :path * /path is a path relative to the root of the \"boot filesystem\" * :path is a path relative to the root of the current filesystem * */ int ret; if (*path == '/') { if ((ret = vfs_get_bootfs(node_store)) != 0) { return ret; } } else { assert(*path == ':'); struct inode *node; if ((ret = vfs_get_curdir(&node)) != 0) { return ret; } /* The current directory may not be a device, so it must have a fs. */ assert(node->in_fs != NULL); *node_store = fsop_get_root(node->in_fs); vop_ref_dec(node); } /* ///... or :/... */ while (*(++ path) == '/'); *subpath = path; return 0; } /* * vfs_lookup - get the inode according to the path filename */ int vfs_lookup(char *path, struct inode **node_store) { int ret; struct inode *node; if ((ret = get_device(path, &path, &node)) != 0) { return ret; } if (*path != '\\0') { ret = vop_lookup(node, path, node_store); vop_ref_dec(node); return ret; } *node_store = node; return 0; } /* * vfs_lookup_parent - Name-to-vnode translation. * (In BSD, both of these are subsumed by namei().) */ int vfs_lookup_parent(char *path, struct inode **node_store, char **endp){ int ret; struct inode *node; if ((ret = get_device(path, &path, &node)) != 0) { return ret; } *endp = path; *node_store = node; return 0; } 我们注意到，这个流程中，有大量以vop开头的函数，它们都通过一些宏和函数的转发，最后变成对inode结构体里的inode_ops结构体的“成员函数”（实际上是函数指针）的调用。对于SFS文件系统的inode来说，会变成对sfs文件系统的具体操作。sfs的这些具体接口的实现较为繁琐，可以在kern/fs/sfs/sfs_inode.c具体查看。我们的练习要求在kern/fs/sfs/sfs_io.c填写一个函数。 // kern/fs/sfs/sfs_inode.c // The sfs specific DIR operations correspond to the abstract operations on a inode. static const struct inode_ops sfs_node_dirops = { .vop_magic = VOP_MAGIC, .vop_open = sfs_opendir, .vop_close = sfs_close, .vop_fstat = sfs_fstat, .vop_fsync = sfs_fsync, .vop_namefile = sfs_namefile, .vop_getdirentry = sfs_getdirentry, .vop_reclaim = sfs_reclaim, .vop_gettype = sfs_gettype, .vop_lookup = sfs_lookup, }; /// The sfs specific FILE operations correspond to the abstract operations on a inode. static const struct inode_ops sfs_node_fileops = { .vop_magic = VOP_MAGIC, .vop_open = sfs_openfile, .vop_close = sfs_close, .vop_read = sfs_read, .vop_write = sfs_write, .vop_fstat = sfs_fstat, .vop_fsync = sfs_fsync, .vop_reclaim = sfs_reclaim, .vop_gettype = sfs_gettype, .vop_tryseek = sfs_tryseek, .vop_truncate = sfs_truncfile, }; Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab8/ying-pan-wen-jian-xi-tong-sfs.html":{"url":"lab8/ying-pan-wen-jian-xi-tong-sfs.html","title":"硬盘文件系统SFS","keywords":"","body":"硬盘文件系统SFS 介绍 通常文件系统中，磁盘的使用是以扇区（Sector）为单位的，但是为了实现简便，SFS 中以 block （4K，与内存 page 大小相等）为基本单位。 SFS 文件系统的布局如下表所示。 superblock root-dir inode freemap inode、File Data、Dir Data Blocks 超级块 根目录索引节点 空闲块映射 目录和文件的数据和索引节点 第 0 个块（4K）是超级块（superblock），它包含了关于文件系统的所有关键参数，当计算机被启动或文件系统被首次接触时，超级块的内容就会被装入内存。其定义如下： struct sfs_super { uint32_t magic; /* magic number, should be SFS_MAGIC */ uint32_t blocks; /* # of blocks in fs */ uint32_t unused_blocks; /* # of unused blocks in fs */ char info[SFS_MAX_INFO_LEN + 1]; /* infomation for sfs */ }; 可以看到，包含一个成员变量魔数 magic，其值为 0x2f8dbe2a，内核通过它来检查磁盘镜像是否是合法的 SFS img；成员变量 blocks 记录了 SFS 中所有 block 的数量，即 img 的大小；成员变量 unused_block 记录了 SFS 中还没有被使用的 block 的数量；成员变量 info 包含了字符串\"simple file system\"。 第 1 个块放了一个 root-dir 的 inode，用来记录根目录的相关信息。有关 inode 还将在后续部分介绍。这里只要理解 root-dir 是 SFS 文件系统的根结点，通过这个 root-dir 的 inode 信息就可以定位并查找到根目录下的所有文件信息。 从第 2 个块开始，根据 SFS 中所有块的数量，用 1 个 bit 来表示一个块的占用和未被占用的情况。这个区域称为 SFS 的 freemap 区域，这将占用若干个块空间。为了更好地记录和管理 freemap 区域，专门提供了两个文件 kern/fs/sfs/bitmap.[ch]来完成根据一个块号查找或设置对应的 bit 位的值。 最后在剩余的磁盘空间中，存放了所有其他目录和文件的 inode 信息和内容数据信息。需要注意的是虽然 inode 的大小小于一个块的大小（4096B），但为了实现简单，每个 inode 都占用一个完整的 block。 在 sfs_fs.c 文件中的 sfs_do_mount 函数中，完成了加载位于硬盘上的 SFS 文件系统的超级块 superblock 和 freemap 的工作。这样，在内存中就有了 SFS 文件系统的全局信息。 [!NOTE|style:flat|label: \"魔数\"是怎样工作的?] 我们经常需要检查某个文件/某块磁盘是否符合我们需要的格式。一般会按照这个文件的完整格式，进行一次全面的分析。 在一个较早的版本，UNIX的可执行文件格式最开头包含一条PDP-11平台上的跳转指令，使得在PDP-11硬件平台上能够正常运行，而在其他平台上，这条指令就是“魔数”（magic number)，只能用作文件类型的标识。 Java类文件（编译到字节码）以十六进制0xCAFEBABE 开头 JPEG图片文件以0xFFD8开头，0xFFD9结尾 PDF文件以“%PDF\"的ASCII码开头，十六进制25 50 44 46 进行这样的约定之后，我们发现，如果文件开头的”魔数“不符合要求，那么这个文件的格式一定不对。这让我们立刻发现文件损坏或者搞错文件类型的情况。由于不同类型的文件有不同的魔数，当你把JPEG文件当作PDF打开的时候，立即就会出现异常。 下面是一个摇滚乐队和巧克力豆的故事，有助于你理解魔数的作用。 美国著名重金属摇滚乐队Van Halen的演出合同中有此一条：演出后台必须提供M&M巧克力豆，但是绝对不许出现棕色豆。如有违反，根据合同，乐队可以取消演出。实际情形中乐队甚至会借此发飙，砸后台，主办方也只好承担所有经济损失。这一条款长期被媒体用来作为摇滚乐队耍大牌的典型例子，有传言指某次由于主唱在后台发现了棕色M&M豆，大发其飙地砸了后台，造成损失高达八万五千美元（当时是八十年代，八万五千还是不少钱）。Van Halen乐队对此从不回应。 多年以后，主唱David Lee Roth 在自传中揭示了这一无厘头条款的来由：Van Halen 乐队在当时是把大型摇滚现场演唱会推向高校及二／三线地区的先锋，由于常常会遇到没有处理过这种大场面的承办者，因此合同里有大量条款来确认演出承办者把场地，器材，工作人员安排等等细节都严格按要求准备好。合同里有成章成章的技术细节，包括场地的承重要求，各类出入口的宽度，电源要求，以至于插座的数量和插座之间的间隔。因此，乐队把棕色豆条款夹带在合同里，以确认承办方是否“仔仔细细阅读了所有条款”。David说：“如果我在后台的M&M里找到棕色豆，我就会立马知道承办方（十有八九）是没好好读完全部技术要求，我们肯定会碰上技术问题。某些技术问题绝对会毁了这场演出，甚至害死人。” 回到上文，八万五千美元的损失是怎么来的？某次在某大学体育场办演唱会，主唱来到后台，发现了棕色M&M豆，当即发飙，砸了后台化妆室，财物损坏大概值一万二。但实际上更糟糕的是，主办方没有细读演出演出场地的承重要求，结果整个舞台压垮（似乎是压穿）了体育场地面，损失高达八万多。 事后媒体的报道是，由于主唱看到棕色M&M豆后发飙砸了后台，造成高达八万五的损失... 索引节点 在 SFS 文件系统中，需要记录文件内容的存储位置以及文件名与文件内容的对应关系。sfs_disk_inode 记录了文件或目录的内容存储的索引信息，该数据结构在硬盘里储存，需要时读入内存（从磁盘读进来的是一段连续的字节，我们将这段连续的字节强制转换成sfs_disk_inode结构体；同样，写入的时候换一个方向强制转换）。sfs_disk_entry 表示一个目录中的一个文件或目录，包含该项所对应 inode 的位置和文件名，同样也在硬盘里储存，需要时读入内存。 磁盘索引节点 SFS 中的磁盘索引节点代表了一个实际位于磁盘上的文件。首先我们看看在硬盘上的索引节点的内容： // kern/fs/sfs/sfs.hc /*inode (on disk)*/ struct sfs_disk_inode { uint32_t size; //如果inode表示常规文件，则size是文件大小 uint16_t type; //inode的文件类型 uint16_t nlinks; //此inode的硬链接数 uint32_t blocks; //此inode的数据块数的个数 uint32_t direct[SFS_NDIRECT]; //此inode的直接数据块索引值（有SFS_NDIRECT个） uint32_t indirect; //此inode的一级间接数据块索引值 }; 通过上表可以看出，如果 inode 表示的是文件，则成员变量 direct[]直接指向了保存文件内容数据的数据块索引值。indirect 间接指向了保存文件内容数据的数据块，indirect 指向的是间接数据块（indirect block），此数据块实际存放的全部是数据块索引，这些数据块索引指向的数据块才被用来存放文件内容数据。 默认的，ucore 里 SFS_NDIRECT 是 12，即直接索引的数据页大小为 12 4k = 48k；当使用一级间接数据块索引时，ucore 支持最大的文件大小为 12 4k + 1024 * 4k = 48k + 4m。数据索引表内，0 表示一个无效的索引，inode 里 blocks 表示该文件或者目录占用的磁盘的 block 的个数。indiret 为 0 时，表示不使用一级索引块。（因为 block 0 用来保存 super block，它不可能被其他任何文件或目录使用，所以这么设计也是合理的）。 对于普通文件，索引值指向的 block 中保存的是文件中的数据。而对于目录，索引值指向的数据保存的是目录下所有的文件名以及对应的索引节点所在的索引块（磁盘块）所形成的数组。数据结构如下： // kern/fs/sfs/sfs.h /* file entry (on disk) */ struct sfs_disk_entry { uint32_t ino; //索引节点所占数据块索引值 char name[SFS_MAX_FNAME_LEN + 1]; //文件名 }; 操作系统中，每个文件系统下的 inode 都应该分配唯一的 inode 编号。SFS 下，为了实现的简便（偷懒），每个 inode 直接用他所在的磁盘 block 的编号作为 inode 编号。比如，root block 的 inode 编号为 1；每个 sfs_disk_entry 数据结构中，name 表示目录下文件或文件夹的名称，ino 表示磁盘 block 编号，通过读取该 block 的数据，能够得到相应的文件或文件夹的 inode。ino 为 0 时，表示一个无效的 entry。 此外，和 inode 相似，每个 sfs_disk_entry也占用一个 block。 内存中的索引节点 // kern/fs/sfs/sfs.h /* inode for sfs */ struct sfs_inode { struct sfs_disk_inode *din; /* on-disk inode */ uint32_t ino; /* inode number */ uint32_t flags; /* inode flags */ bool dirty; /* true if inode modified */ int reclaim_count; /* kill inode if it hits zero */ semaphore_t sem; /* semaphore for din */ list_entry_t inode_link; /* entry for linked-list in sfs_fs */ list_entry_t hash_link; /* entry for hash linked-list in sfs_fs */ }; 可以看到 SFS 中的内存 inode 包含了 SFS 的硬盘 inode 信息，而且还增加了其他一些信息，这属于是便于进行是判断否改写、互斥操作、回收和快速地定位等作用。需要注意，一个内存 inode 是在打开一个文件后才创建的，如果关机则相关信息都会消失。而硬盘 inode 的内容是保存在硬盘中的，只是在进程需要时才被读入到内存中，用于访问文件或目录的具体内容数据 为了方便实现上面提到的多级数据的访问以及目录中 entry 的操作，对 inode SFS 实现了一些辅助的函数： （在kern/fs/sfs/sfs_inode.c实现） sfs_bmap_load_nolock：将对应 sfs_inode 的第 index 个索引指向的 block 的索引值取出存到相应的指针指向的单元（ino_store）。该函数只接受 index blocks 的参数。当 index == inode->blocks 时，该函数理解为需要为 inode 增长一个 block。并标记 inode 为 dirty（所有对 inode 数据的修改都要做这样的操作，这样，当 inode 不再使用的时候，sfs 能够保证 inode 数据能够被写回到磁盘）。sfs_bmap_load_nolock 调用的 sfs_bmap_get_nolock 来完成相应的操作，阅读 sfs_bmap_get_nolock，了解他是如何工作的。（sfs_bmap_get_nolock 只由 sfs_bmap_load_nolock 调用） sfs_bmap_truncate_nolock：将多级数据索引表的最后一个 entry 释放掉。他可以认为是 sfs_bmap_load_nolock 中，index == inode->blocks 的逆操作。当一个文件或目录被删除时，sfs 会循环调用该函数直到 inode->blocks 减为 0，释放所有的数据页。函数通过 sfs_bmap_free_nolock 来实现，他应该是 sfs_bmap_get_nolock 的逆操作。和 sfs_bmap_get_nolock 一样，调用 sfs_bmap_free_nolock 也要格外小心。 sfs_dirent_read_nolock：将目录的第 slot 个 entry 读取到指定的内存空间。他通过上面提到的函数来完成。 sfs_dirent_search_nolock：是常用的查找函数。他在目录下查找 name，并且返回相应的搜索结果（文件或文件夹）的 inode 的编号（也是磁盘编号），和相应的 entry 在该目录的 index 编号以及目录下的数据页是否有空闲的 entry。（SFS 实现里文件的数据页是连续的，不存在任何空洞；而对于目录，数据页不是连续的，当某个 entry 删除的时候，SFS 通过设置 entry->ino 为 0 将该 entry 所在的 block 标记为 free，在需要添加新 entry 的时候，SFS 优先使用这些 free 的 entry，其次才会去在数据页尾追加新的 entry。 注意，这些后缀为 nolock 的函数，只能在已经获得相应 inode 的 semaphore 才能调用。 Inode 的文件操作函数 // kern/fs/sfs/sfs_inode.c static const struct inode_ops sfs_node_fileops = { .vop_magic = VOP_MAGIC, .vop_open = sfs_openfile, .vop_close = sfs_close, .vop_read = sfs_read, .vop_write = sfs_write, …… }; 上述 sfs_openfile、sfs_close、sfs_read 和 sfs_write 分别对应用户进程发出的 open、close、read、write 操作。其中 sfs_openfile 不用做什么事；sfs_close 需要把对文件的修改内容写回到硬盘上，这样确保硬盘上的文件内容数据是最新的；sfs_read 和 sfs_write 函数都调用了一个函数 sfs_io，并最终通过访问硬盘驱动来完成对文件内容数据的读写。 Inode 的目录操作函数 // kern/fs/sfs/sfs_inode.c static const struct inode_ops sfs_node_dirops = { .vop_magic = VOP_MAGIC, .vop_open = sfs_opendir, .vop_close = sfs_close, .vop_getdirentry = sfs_getdirentry, .vop_lookup = sfs_lookup, …… }; 对于目录操作而言，由于目录也是一种文件，所以 sfs_opendir、sys_close 对应户进程发出的 open、close 函数。相对于 sfs_open，sfs_opendir 只是完成一些 open 函数传递的参数判断，没做其他更多的事情。目录的 close 操作与文件的 close 操作完全一致。由于目录的内容数据与文件的内容数据不同，所以读出目录的内容数据的函数是 sfs_getdirentry()，其主要工作是获取目录下的文件 inode 信息。 这里用到的inode_ops结构体，在kern/fs/vfs/inode.h定义，作用是：把关于inode的操作接口，集中在一个结构体里， 通过这个结构体，我们可以把Simple File System的接口（如sfs_openfile())提供给上层的VFS使用。可以想象我们除了Simple File System, 还在另一块磁盘上使用完全不同的文件系统Complex File System，显然vop_open(),vop_read()这些接口的实现都要不一样了。对于同一个文件系统这些接口都是一样的，所以我们可以提供”属于SFS的文件的inode_ops结构体\", “属于CFS的文件的inode_ops结构体\"。 下面的注释里详细解释了每个接口的用途。当然，不必现在就详细了解每一个接口。 // kern/fs/vfs/inode.h struct inode_ops { unsigned long vop_magic; int (*vop_open)(struct inode *node, uint32_t open_flags); int (*vop_close)(struct inode *node); int (*vop_read)(struct inode *node, struct iobuf *iob); int (*vop_write)(struct inode *node, struct iobuf *iob); int (*vop_fstat)(struct inode *node, struct stat *stat); int (*vop_fsync)(struct inode *node); int (*vop_namefile)(struct inode *node, struct iobuf *iob); int (*vop_getdirentry)(struct inode *node, struct iobuf *iob); int (*vop_reclaim)(struct inode *node); int (*vop_gettype)(struct inode *node, uint32_t *type_store); int (*vop_tryseek)(struct inode *node, off_t pos); int (*vop_truncate)(struct inode *node, off_t len); int (*vop_create)(struct inode *node, const char *name, bool excl, struct inode **node_store); int (*vop_lookup)(struct inode *node, char *path, struct inode **node_store); int (*vop_ioctl)(struct inode *node, int op, void *data); }; /* * Abstract operations on a inode. * * These are used in the form VOP_FOO(inode, args), which are macros * that expands to inode->inode_ops->vop_foo(inode, args). The operations * \"foo\" are: * * vop_open - Called on open() of a file. Can be used to * reject illegal or undesired open modes. Note that * various operations can be performed without the * file actually being opened. * The inode need not look at O_CREAT, O_EXCL, or * O_TRUNC, as these are handled in the VFS layer. * * VOP_EACHOPEN should not be called directly from * above the VFS layer - use vfs_open() to open inodes. * This maintains the open count so VOP_LASTCLOSE can * be called at the right time. * * vop_close - To be called on *last* close() of a file. * * VOP_LASTCLOSE should not be called directly from * above the VFS layer - use vfs_close() to close * inodes opened with vfs_open(). * * vop_reclaim - Called when inode is no longer in use. Note that * this may be substantially after vop_lastclose is * called. * ***************************************** * * vop_read - Read data from file to uio, at offset specified * in the uio, updating uio_resid to reflect the * amount read, and updating uio_offset to match. * Not allowed on directories or symlinks. * * vop_getdirentry - Read a single filename from a directory into a * uio, choosing what name based on the offset * field in the uio, and updating that field. * Unlike with I/O on regular files, the value of * the offset field is not interpreted outside * the filesystem and thus need not be a byte * count. However, the uio_resid field should be * handled in the normal fashion. * On non-directory objects, return ENOTDIR. * * vop_write - Write data from uio to file at offset specified * in the uio, updating uio_resid to reflect the * amount written, and updating uio_offset to match. * Not allowed on directories or symlinks. * * vop_ioctl - Perform ioctl operation OP on file using data * DATA. The interpretation of the data is specific * to each ioctl. * * vop_fstat -Return info about a file. The pointer is a * pointer to struct stat; see stat.h. * * vop_gettype - Return type of file. The values for file types * are in sfs.h. * * vop_tryseek - Check if seeking to the specified position within * the file is legal. (For instance, all seeks * are illegal on serial port devices, and seeks * past EOF on files whose sizes are fixed may be * as well.) * * vop_fsync - Force any dirty buffers associated with this file * to stable storage. * * vop_truncate - Forcibly set size of file to the length passed * in, discarding any excess blocks. * * vop_namefile - Compute pathname relative to filesystem root * of the file and copy to the specified io buffer. * Need not work on objects that are not * directories. * ***************************************** * * vop_creat - Create a regular file named NAME in the passed * directory DIR. If boolean EXCL is true, fail if * the file already exists; otherwise, use the * existing file if there is one. Hand back the * inode for the file as per vop_lookup. * ***************************************** * * vop_lookup - Parse PATHNAME relative to the passed directory * DIR, and hand back the inode for the file it * refers to. May destroy PATHNAME. Should increment * refcount on inode handed back. */ Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab8/she-bei-ji-wen-jian.html":{"url":"lab8/she-bei-ji-wen-jian.html","title":"设备即文件","keywords":"","body":"设备即文件 在本实验中，为了统一地访问设备(device)，我们可以把一个设备看成一个文件，通过访问文件的接口来访问设备。目前实现了 stdin 设备文件文件、stdout 设备文件、disk0 设备。stdin 设备就是键盘，stdout 设备就是控制台终端的文本显示，而 disk0 设备是承载 SFS 文件系统的磁盘设备。下面看看 ucore 是如何让用户把设备看成文件来访问。 设备的定义 为了表示一个设备，需要有对应的数据结构，ucore 为此定义了 struct device，如下： 可以认为struct device是一个比较抽象的“设备”的定义。一个具体设备，只要实现了d_open()打开设备， d_close()关闭设备，d_io()(读写该设备，write参数是true/false决定是读还是写)，d_ioctl()(input/output control)四个函数接口，就可以被文件系统使用了。 // kern/fs/devs/dev.h /* * Filesystem-namespace-accessible device. * d_io is for both reads and writes; the iobuf will indicates the direction. */ struct device { size_t d_blocks; size_t d_blocksize; int (*d_open)(struct device *dev, uint32_t open_flags); int (*d_close)(struct device *dev); int (*d_io)(struct device *dev, struct iobuf *iob, bool write); int (*d_ioctl)(struct device *dev, int op, void *data); }; #define dop_open(dev, open_flags) ((dev)->d_open(dev, open_flags)) #define dop_close(dev) ((dev)->d_close(dev)) #define dop_io(dev, iob, write) ((dev)->d_io(dev, iob, write)) #define dop_ioctl(dev, op, data) ((dev)->d_ioctl(dev, op, data)) 这个数据结构能够支持对块设备（比如磁盘）、字符设备（比如键盘）的表示，完成对设备的基本操作。 但这个设备描述没有与文件系统以及表示一个文件的 inode 数据结构建立关系，为此，还需要另外一个数据结构把 device 和 inode 联通起来，这就是 vfs_dev_t 数据结构。 利用 vfs_dev_t 数据结构，就可以让文件系统通过一个链接 vfs_dev_t 结构的双向链表找到 device 对应的 inode 数据结构，一个 inode 节点的成员变量 in_type 的值是 0x1234，则此 inode 的成员变量 in_info 将成为一个 device 结构。这样 inode 就和一个设备建立了联系，这个 inode 就是一个设备文件。 // kern/fs/vfs/vfsdev.c // device info entry in vdev_list typedef struct { const char *devname; struct inode *devnode; struct fs *fs; bool mountable; list_entry_t vdev_link; } vfs_dev_t; #define le2vdev(le, member) \\ to_struct((le), vfs_dev_t, member) //为了使用链表定义的宏, 做到现在应该对它很熟悉了 static list_entry_t vdev_list; // device info list in vfs layer static semaphore_t vdev_list_sem; // 互斥访问的semaphore static void lock_vdev_list(void) { down(&vdev_list_sem); } static void unlock_vdev_list(void) { up(&vdev_list_sem); } ucore 虚拟文件系统为了把这些设备链接在一起，还定义了一个设备链表，即双向链表 vdev_list，这样通过访问此链表，可以找到 ucore 能够访问的所有设备文件。 注意这里的vdev_list对应一个vdev_list_sem。在文件系统中，互斥访问非常重要，所以我们将看到很多的semaphore。 我们使用iobuf结构体传递一个IO请求（要写入设备的数据当前所在内存的位置和长度/从设备读取的数据需要存储到的位置） struct iobuf { void *io_base; // the base addr of buffer (used for Rd/Wr) off_t io_offset; // current Rd/Wr position in buffer, will have been incremented by the amount transferred size_t io_len; // the length of buffer (used for Rd/Wr) size_t io_resid; // current resident length need to Rd/Wr, will have been decremented by the amount transferred. }; 注意设备文件的inode也有一个inode_ops成员, 提供设备文件应具备的接口。 // kern/fs/devs/dev.c /* * Function table for device inodes. */ static const struct inode_ops dev_node_ops = { .vop_magic = VOP_MAGIC, .vop_open = dev_open, .vop_close = dev_close, .vop_read = dev_read, .vop_write = dev_write, .vop_fstat = dev_fstat, .vop_ioctl = dev_ioctl, .vop_gettype = dev_gettype, .vop_tryseek = dev_tryseek, .vop_lookup = dev_lookup, }; stdin设备 trap.c改变了对stdin的处理, 将stdin作为一个设备(也是一个文件), 通过sys_read()接口读取标准输入的数据。 注意，既然我们把stdin, stdout看作文件， 那么也需要先打开文件，才能进行读写。在执行用户程序之前，我们先执行了umain.c建立一个运行时环境，这里主要做的工作，就是让程序能够使用stdin, stdout。 // user/libs/file.c //这是用户态程序可以使用的“系统库函数”，从文件fd读取len个字节到base这个位置。 //当fd = 0的时候，表示从stdin读取 int read(int fd, void *base, size_t len) { return sys_read(fd, base, len); } // user/libs/umain.c int main(int argc, char *argv[]); static int initfd(int fd2, const char *path, uint32_t open_flags) { int fd1, ret; if ((fd1 = open(path, open_flags)) failed: %e.\\n\", fd); }//0用于描述stdin，这里因为第一个被打开，所以stdin会分配到0 if ((fd = initfd(1, \"stdout:\", O_WRONLY)) failed: %e.\\n\", fd); }//1用于描述stdout int ret = main(argc, argv); //真正的“用户程序” exit(ret); } 这里我们需要把命令行的输入转换成一个文件，于是需要一个缓冲区：把已经在命令行输入，但还没有被读取的数据放在缓冲区里。这里遇到一个问题：每当控制台输入一个字符，我们都要及时把它放到stdin的缓冲区里。一般来说，应当有键盘的外设中断来提醒我们。但是我们在QEMU里收不到这个中断，于是采取一个措施：借助时钟中断，每次时钟中断检查是否有新的字符，这效率比较低，不过也还可以接受。 // kern/trap/trap.c void interrupt_handler(struct trapframe *tf) { intptr_t cause = (tf->cause > 1; switch (cause) { /*...*/ case IRQ_S_TIMER: clock_set_next_event(); if (++ticks % TICK_NUM == 0 && current) { // print_ticks(); current->need_resched = 1; } run_timer_list(); //按理说用户程序看到的stdin是“只读”的 //但是，一个文件，只往外读，不往里写，是不是会导致数据\"不守恒\"? //我们在这里就是把控制台输入的数据“写到”stdin里(实际上是写到一个缓冲区里) //这里的cons_getc()并不一定能返回一个字符,可以认为是轮询 //如果cons_getc()返回0, 那么dev_stdin_write()函数什么都不做 dev_stdin_write(cons_getc()); break; } } // kern/driver/console.c #define CONSBUFSIZE 512 static struct { uint8_t buf[CONSBUFSIZE]; uint32_t rpos; uint32_t wpos; //控制台的输入缓冲区是一个队列 } cons; /* * * cons_intr - called by device interrupt routines to feed input * characters into the circular console input buffer. * */ void cons_intr(int (*proc)(void)) { int c; while ((c = (*proc)()) != -1) { if (c != 0) { cons.buf[cons.wpos++] = c; if (cons.wpos == CONSBUFSIZE) { cons.wpos = 0; } } } } /* serial_proc_data - get data from serial port */ int serial_proc_data(void) { int c = sbi_console_getchar(); if (c 我们来看stdin设备的实现: // kern/fs/devs/dev_stdin.c #define STDIN_BUFSIZE 4096 static char stdin_buffer[STDIN_BUFSIZE]; //这里又有一个stdin设备的缓冲区, 能否和之前console的缓冲区合并? static off_t p_rpos, p_wpos; static wait_queue_t __wait_queue, *wait_queue = &__wait_queue; void dev_stdin_write(char c) { //把其他地方的字符写到stdin缓冲区, 准备被读取 bool intr_flag; if (c != '\\0') { local_intr_save(intr_flag);//禁用中断 { stdin_buffer[p_wpos % STDIN_BUFSIZE] = c; if (p_wpos - p_rpos wakeup_flags == WT_KBD) { goto try_again; //再次尝试 } break; } } } local_intr_restore(intr_flag); return ret; } static int stdin_io(struct device *dev, struct iobuf *iob, bool write) { //对应struct device 的d_io() if (!write) { int ret; if ((ret = dev_stdin_read(iob->io_base, iob->io_resid)) > 0) { iob->io_resid -= ret; } return ret; } return -E_INVAL; } disk0设备 封装了一下ramdisk的接口，每次读取或者写入若干个block。 // kern/fs/devs/dev_disk0.c static char *disk0_buffer; static semaphore_t disk0_sem; static void lock_disk0(void) { down(&(disk0_sem)); } static void unlock_disk0(void) { up(&(disk0_sem)); } static void disk0_read_blks_nolock(uint32_t blkno, uint32_t nblks) { int ret; uint32_t sectno = blkno * DISK0_BLK_NSECT, nsecs = nblks * DISK0_BLK_NSECT; if ((ret = ide_read_secs(DISK0_DEV_NO, sectno, disk0_buffer, nsecs)) != 0) { panic(\"disk0: read blkno = %d (sectno = %d), nblks = %d (nsecs = %d): 0x%08x.\\n\", blkno, sectno, nblks, nsecs, ret); } } static void disk0_write_blks_nolock(uint32_t blkno, uint32_t nblks) { int ret; uint32_t sectno = blkno * DISK0_BLK_NSECT, nsecs = nblks * DISK0_BLK_NSECT; if ((ret = ide_write_secs(DISK0_DEV_NO, sectno, disk0_buffer, nsecs)) != 0) { panic(\"disk0: write blkno = %d (sectno = %d), nblks = %d (nsecs = %d): 0x%08x.\\n\", blkno, sectno, nblks, nsecs, ret); } } static int disk0_io(struct device *dev, struct iobuf *iob, bool write) { off_t offset = iob->io_offset; size_t resid = iob->io_resid; uint32_t blkno = offset / DISK0_BLKSIZE; uint32_t nblks = resid / DISK0_BLKSIZE; /* don't allow I/O that isn't block-aligned */ if ((offset % DISK0_BLKSIZE) != 0 || (resid % DISK0_BLKSIZE) != 0) { return -E_INVAL; } /* don't allow I/O past the end of disk0 */ if (blkno + nblks > dev->d_blocks) { return -E_INVAL; } /* read/write nothing ? */ if (nblks == 0) { return 0; } lock_disk0(); while (resid != 0) { size_t copied, alen = DISK0_BUFSIZE; if (write) { iobuf_move(iob, disk0_buffer, alen, 0, &copied); assert(copied != 0 && copied resid) { alen = resid; } nblks = alen / DISK0_BLKSIZE; disk0_read_blks_nolock(blkno, nblks); iobuf_move(iob, disk0_buffer, alen, 1, &copied); assert(copied == alen && copied % DISK0_BLKSIZE == 0); } resid -= copied, blkno += nblks; } unlock_disk0(); return 0; } 这些设备的实现看起来比较复杂，实际上属于比较麻烦的设备驱动的部分。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab8/cong-zhong-duan-dao-zhong-duan.html":{"url":"lab8/cong-zhong-duan-dao-zhong-duan.html","title":"从中断到终端","keywords":"","body":"从中断到终端 可以说，我们的操作系统之旅，从zhong duan(中断)开始， 也在zhong duan(终端）结束。 我们的终端需要实现这样的功能: 根据输入的程序名称, 从文件系统里加载对应的程序并执行。我们采取fork() exec()的方式来加载执行程序，exec()的一系列接口都需要重写来使用文件系统。以do_execve()为例， 以前的函数原型从内存的某个位置加载程序 int do_execve(const char *name, size_t len, unsigned char *binary, size_t size) ; 现在则调用文件系统接口加载程序： // kern/process/proc.c // do_execve - call exit_mmap(mm)&put_pgdir(mm) to reclaim memory space of current process // - call load_icode to setup new memory space accroding binary prog. int do_execve(const char *name, int argc, const char **argv) { static_assert(EXEC_MAX_ARG_LEN >= FS_MAX_FPATH_LEN); struct mm_struct *mm = current->mm; if (!(argc >= 1 && argc %d\", current->pid); } else { if (!copy_string(mm, local_name, name, sizeof(local_name))) { unlock_mm(mm); return ret; } } if ((ret = copy_kargv(mm, argc, kargv, argv)) != 0) { unlock_mm(mm); return ret; } path = argv[0]; unlock_mm(mm); files_closeall(current->filesp); /* sysfile_open will check the first argument path, thus we have to use a user-space pointer, and argv[0] may be incorrect */ int fd; if ((ret = fd = sysfile_open(path, O_RDONLY)) mm = NULL; } ret= -E_NO_MEM;; if ((ret = load_icode(fd, argc, kargv)) != 0) { goto execve_exit; } put_kargv(argc, kargv); set_proc_name(current, local_name); return 0; execve_exit: put_kargv(argc, kargv); do_exit(ret); panic(\"already exit: %e.\\n\", ret); } 我们还要看一下终端程序的实现。可以发现终端程序需要对命令进行词法和语法分析。 // user/sh.c #include #include #include #include #include #include #include #define printf(...) fprintf(1, __VA_ARGS__) #define putc(c) printf(\"%c\", c) #define BUFSIZE 4096 #define WHITESPACE \" \\t\\r\\n\" #define SYMBOLS \"&;\" char shcwd[BUFSIZE]; int gettoken(char **p1, char **p2) { char *s; if ((s = *p1) == NULL) { return 0; } while (strchr(WHITESPACE, *s) != NULL) { *s ++ = '\\0'; } if (*s == '\\0') { return 0; } *p2 = s; int token = 'w'; if (strchr(SYMBOLS, *s) != NULL) { token = *s, *s ++ = '\\0'; } else { bool flag = 0; while (*s != '\\0' && (flag || strchr(WHITESPACE SYMBOLS, *s) == NULL)) { if (*s == '\"') { *s = ' ', flag = !flag; } s ++; } } *p1 = (*s != '\\0' ? s : NULL); return token; } char * readline(const char *prompt) { static char buffer[BUFSIZE]; if (prompt != NULL) { printf(\"%s\", prompt); } int ret, i = 0; while (1) { char c; if ((ret = read(0, &c, sizeof(char))) 0) { buffer[i] = '\\0'; break; } return NULL; } if (c == 3) { return NULL; } else if (c >= ' ' && i 0) { putc(c); i --; } else if (c == '\\n' || c == '\\r') { putc(c); buffer[i] = '\\0'; break; } } return buffer; } void usage(void) { printf(\"usage: sh [command-file]\\n\"); } int reopen(int fd2, const char *filename, uint32_t open_flags) { int ret, fd1; close(fd2); if ((ret = open(filename, open_flags)) >= 0 && ret != fd2) { close(fd2); fd1 = ret, ret = dup2(fd1, fd2); close(fd1); } return ret ': if (gettoken(&cmd, &t) != 'w') { printf(\"sh error: syntax error: > not followed by word\\n\"); return -1; } if ((ret = reopen(1, t, O_RDWR | O_TRUNC | O_CREAT)) != 0) { return ret; } break; case '|': // if ((ret = pipe(p)) != 0) { // return ret; // } if ((ret = fork()) == 0) { close(0); if ((ret = dup2(p[0], 0)) 2) { usage(); return -1; } //shcwd = malloc(BUFSIZE); assert(shcwd != NULL); char *buffer; while ((buffer = readline((interactive) ? \"$ \" : NULL)) != NULL) { shcwd[0] = '\\0'; int pid; if ((pid = fork()) == 0) { ret = runcmd(buffer); exit(ret); } assert(pid >= 0); if (waitpid(pid, &ret) == 0) { if (ret == 0 && shcwd[0] != '\\0') { ret = 0; } if (ret != 0) { printf(\"error: %d - %e\\n\", ret, ret); } } } return 0; } 如果我们能够把终端运行起来，并能输入命令执行用户程序，就说明程序运行正常。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "},"lab8/xiang-mu-zu-cheng-yu-zhi-hang-liu.html":{"url":"lab8/xiang-mu-zu-cheng-yu-zhi-hang-liu.html","title":"项目组成与执行流","keywords":"","body":"项目组成与执行流 项目组成 lab8 ├── Makefile ├── disk0 │ ├── badarg │ ├── badsegment │ ├── divzero │ ├── exit │ ├── faultread │ ├── faultreadkernel │ ├── forktest │ ├── forktree │ ├── hello │ ├── matrix │ ├── pgdir │ ├── priority │ ├── sh │ ├── sleep │ ├── sleepkill │ ├── softint │ ├── spin │ ├── testbss │ ├── waitkill │ └── yield ├── giveitatry.pyq ├── kern │ ├── debug │ │ ├── assert.h │ │ ├── kdebug.c │ │ ├── kdebug.h │ │ ├── kmonitor.c │ │ ├── kmonitor.h │ │ ├── panic.c │ │ └── stab.h │ ├── driver │ │ ├── clock.c │ │ ├── clock.h │ │ ├── console.c │ │ ├── console.h │ │ ├── ide.c │ │ ├── ide.h │ │ ├── intr.c │ │ ├── intr.h │ │ ├── kbdreg.h │ │ ├── picirq.c │ │ ├── picirq.h │ │ ├── ramdisk.c │ │ └── ramdisk.h │ ├── fs │ │ ├── devs │ │ │ ├── dev.c │ │ │ ├── dev.h │ │ │ ├── dev_disk0.c │ │ │ ├── dev_stdin.c │ │ │ └── dev_stdout.c │ │ ├── file.c │ │ ├── file.h │ │ ├── fs.c │ │ ├── fs.h │ │ ├── iobuf.c │ │ ├── iobuf.h │ │ ├── sfs │ │ │ ├── bitmap.c │ │ │ ├── bitmap.h │ │ │ ├── sfs.c │ │ │ ├── sfs.h │ │ │ ├── sfs_fs.c │ │ │ ├── sfs_inode.c │ │ │ ├── sfs_io.c │ │ │ └── sfs_lock.c │ │ ├── swap │ │ │ ├── swapfs.c │ │ │ └── swapfs.h │ │ ├── sysfile.c │ │ ├── sysfile.h │ │ └── vfs │ │ ├── inode.c │ │ ├── inode.h │ │ ├── vfs.c │ │ ├── vfs.h │ │ ├── vfsdev.c │ │ ├── vfsfile.c │ │ ├── vfslookup.c │ │ └── vfspath.c │ ├── init │ │ ├── entry.S │ │ └── init.c │ ├── libs │ │ ├── readline.c │ │ ├── stdio.c │ │ └── string.c │ ├── mm │ │ ├── default_pmm.c │ │ ├── default_pmm.h │ │ ├── kmalloc.c │ │ ├── kmalloc.h │ │ ├── memlayout.h │ │ ├── mmu.h │ │ ├── pmm.c │ │ ├── pmm.h │ │ ├── swap.c │ │ ├── swap.h │ │ ├── swap_fifo.c │ │ ├── swap_fifo.h │ │ ├── vmm.c │ │ └── vmm.h │ ├── process │ │ ├── entry.S │ │ ├── proc.c │ │ ├── proc.h │ │ └── switch.S │ ├── schedule │ │ ├── default_sched.h │ │ ├── default_sched_c │ │ ├── default_sched_stride.c │ │ ├── sched.c │ │ └── sched.h │ ├── sync │ │ ├── check_sync.c │ │ ├── monitor.c │ │ ├── monitor.h │ │ ├── sem.c │ │ ├── sem.h │ │ ├── sync.h │ │ ├── wait.c │ │ └── wait.h │ ├── syscall │ │ ├── syscall.c │ │ └── syscall.h │ └── trap │ ├── trap.c │ ├── trap.h │ └── trapentry.S ├── lab5.md ├── libs │ ├── atomic.h │ ├── defs.h │ ├── dirent.h │ ├── elf.h │ ├── error.h │ ├── hash.c │ ├── list.h │ ├── printfmt.c │ ├── rand.c │ ├── riscv.h │ ├── sbi.h │ ├── skew_heap.h │ ├── stat.h │ ├── stdarg.h │ ├── stdio.h │ ├── stdlib.h │ ├── string.c │ ├── string.h │ └── unistd.h ├── tools │ ├── boot.ld │ ├── function.mk │ ├── gdbinit │ ├── grade.sh │ ├── kernel.ld │ ├── mksfs.c │ ├── sign.c │ ├── user.ld │ └── vector.c └── user ├── badarg.c ├── badsegment.c ├── divzero.c ├── exit.c ├── faultread.c ├── faultreadkernel.c ├── forktest.c ├── forktree.c ├── hello.c ├── libs │ ├── dir.c │ ├── dir.h │ ├── file.c │ ├── file.h │ ├── initcode.S │ ├── lock.h │ ├── panic.c │ ├── stdio.c │ ├── syscall.c │ ├── syscall.h │ ├── ulib.c │ ├── ulib.h │ └── umain.c ├── matrix.c ├── pgdir.c ├── priority.c ├── sh.c ├── sleep.c ├── sleepkill.c ├── softint.c ├── spin.c ├── testbss.c ├── waitkill.c └── yield.c 21 directories, 176 files 通过文件系统接口 user/libs/file.[ch]|dir.[ch]|syscall.c：与文件系统操作相关的用户库实行； kern/syscall.[ch]：文件中包含文件系统相关的内核态系统调用接口； kern/fs/sysfile.[ch]|file.[ch]：通用文件系统接口和实行； 文件系统抽象层-VFS kern/fs/vfs/*.[ch]：虚拟文件系统接口与实现 simple FS文件系统 kern/fs/sfs/*.[ch]：SimpleFS文件系统实现 文件系统的硬盘IO接口 kern/fs/devs/dev.[ch]|dev_disk0.c：disk0硬盘设备提供给文件系统的I/O访问接口和实现 执行流 结合前面所述自行理解、总结。 Copyright © 2020 all right reserved，powered by Gitbook最后修订于： 2020-09-01 10:12:58 "}}